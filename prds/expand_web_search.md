|             | Search                            | Crawling                          | Find Similar                             | Answer                                           |
| ----------- | --------------------------------- | --------------------------------- | ---------------------------------------- | ------------------------------------------------ |
| Description | return results and their contents | returns the contents of a webpage | find similar webpages and their contents | get an LLM answer to a question                  |
| Example     | blog post about AI                | [tesla.com](http://tesla.com)     | [brex.com](http://brex.com)              | What makes some LLMs so much better than others? |

## Search

Request

```javascript
import Exa from "exa-js"

const exa = new Exa("3e964454-75a7-415d-bb35-53a13f472e12");

const result = await exa.searchAndContents(
  "blog post about AI",
  {
    text: true
  }
)
```

Result

`````javascript
{
  "data": {
    "requestId": "11161678f41c5f9e5da90771a50fea19",
    "autopromptString": "blog post about AI",
    "resolvedSearchType": "neural",
    "results": [
      {
        "id": "https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html",
        "title": "The AI Revolution: The Road to Superintelligence",
        "url": "https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html",
        "publishedDate": "2015-01-22T14:21:52.000Z",
        "author": "Tim Urban",
        "score": 0.38529151678085327,
        "text": "_**PDF:** We made a fancy PDF of this post for printing and offline viewing. [Buy it here.](https://gum.co/wbw-ai1) (Or see a [preview](https://waitbutwhy.com/wp-content/uploads/2016/04/artificial-intelligence-revolution-1-preview.pdf).)_\n\n_**Note:** The reason this post took three weeks to finish is that as I dug into research on Artificial Intelligence, I could not **believe** what I was reading. It hit me pretty quickly that what’s happening in the world of AI is not just an important topic, but by far THE most important topic for our future. So I wanted to learn as much as I could about it, and once I did that, I wanted to make sure I wrote a post that really explained this whole situation and why it matters so much. Not shockingly, that became outrageously long, so I broke it into two parts. This is Part 1—Part 2 is [here](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html)._\n\n\\\\_\\\\_\\\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\n_We are on the edge of change comparable to the rise of human life on Earth._ — Vernor Vinge\n\nWhat does it feel like to stand here?\n\n[![Edge1](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Edge1.png)](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Edge1.png)\n\nIt seems like a pretty intense place to be standing—but then you have to remember something about what it’s like to stand on a time graph: you can’t see what’s to your right. So here’s how it actually feels to stand there:\n\n[![Edge](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Edge.jpg)](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Edge.jpg)\n\nWhich probably feels pretty normal…\n\n\\\\_\\\\_\\\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\n## **The Far Future—Coming Soon**\n\nImagine taking a time machine back to 1750—a time when the world was in a permanent power outage, long-distance communication meant either yelling loudly or firing a cannon in the air, and all transportation ran on hay. When you get there, you retrieve a dude, bring him to 2015, and then walk him around and watch him react to everything. It’s impossible for us to understand what it would be like for him to see shiny capsules racing by on a highway, talk to people who had been on the other side of the ocean earlier in the day, watch sports that were being played 1,000 miles away, hear a musical performance that happened 50 years ago, and play with my magical wizard rectangle that he could use to capture a real-life image or record a living moment, generate a map with a paranormal moving blue dot that shows him where he is, look at someone’s face and chat with them even though they’re on the other side of the country, and worlds of other inconceivable sorcery. This is all before you show him the internet or explain things like the International Space Station, the Large Hadron Collider, nuclear weapons, or general relativity.\n\nThis experience for him wouldn’t be surprising or shocking or even mind-blowing—those words aren’t big enough. He might actually die.\n\nBut here’s the interesting thing—if he then went back to 1750 and got jealous that we got to see his reaction and decided he wanted to try the same thing, he’d take the time machine and go back the same distance, get someone from around the year 1500, bring him to 1750, and show him everything. And the 1500 guy would be shocked by a lot of things—but he wouldn’t die. It would be _far_ less of an insane experience for him, because while 1500 and 1750 were very different, they were _much_ _less_ different than 1750 to 2015. The 1500 guy would learn some mind-bending shit about space and physics, he’d be impressed with how committed Europe turned out to be with that new imperialism fad, and he’d have to do some major revisions of his world map conception. But watching everyday life go by in 1750—transportation, communication, etc.—definitely wouldn’t make him die.\n\nNo, in order for the 1750 guy to have as much fun as we had with him, he’d have to go much farther back—maybe all the way back to about 12,000 BC, before the First Agricultural Revolution gave rise to the first cities and to the concept of civilization. If someone from a purely hunter-gatherer world—from a time when humans were, more or less, just another animal species—saw the vast human empires of 1750 with their towering churches, their ocean-crossing ships, their concept of being “inside,” and their enormous mountain of collective, accumulated human knowledge and discovery—he’d likely die.\n\nAnd then what if, after dying, _he_ got jealous and wanted to do the same thing. If he went back 12,000 years to 24,000 BC and got a guy and brought him to 12,000 BC, he’d show the guy everything and the guy would be like, “Okay what’s your point who cares.” For the 12,000 BC guy to have the same fun, he’d have to go back over 100,000 years and get someone he could show fire and language to for the first time.\n\nIn order for someone to be transported into the future and die from the level of shock they’d experience, they have to go enough years ahead that a “die level of progress,” or a Die Progress Unit (DPU) has been achieved. So a DPU took over 100,000 years in hunter-gatherer times, but at the post-Agricultural Revolution rate, it only took about 12,000 years. The post-Industrial Revolution world has moved so quickly that a 1750 person only needs to go forward a couple hundred years for a DPU to have happened.\n\nThis pattern—human progress moving quicker and quicker as time goes on—is what futurist Ray Kurzweil calls human history’s Law of Accelerating Returns. This happens because more advanced societies have the ability to progress at a faster _rate_ than less advanced societies— _because_ they’re more advanced. 19th century humanity knew more and had better technology than 15th century humanity, so it’s no surprise that humanity made far more advances in the 19th century than in the 15th century—15th century humanity was no match for 19th century humanity. [1](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#footnote-1-3216) [1](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#footnote2-1-3216) ← open these\n\nThis works on smaller scales too. The movie _Back to the Future_ came out in 1985, and “the past” took place in 1955. In the movie, when Michael J. Fox went back to 1955, he was caught off-guard by the newness of TVs, the prices of soda, the lack of love for shrill electric guitar, and the variation in slang. It was a different world, yes—but if the movie were made today and the past took place in 1985, the movie could have had _much_ more fun with _much_ bigger differences. The character would be in a time before personal computers, internet, or cell phones—today’s Marty McFly, a teenager born in the late 90s, would be much more out of place in 1985 than the movie’s Marty McFly was in 1955.\n\nThis is for the same reason we just discussed—the Law of Accelerating Returns. The average rate of advancement between 1985 and 2015 was higher than the rate between 1955 and 1985—because the former was a more advanced world—so much more change happened in the most recent 30 years than in the prior 30.\n\nSo—advances are getting bigger and bigger and happening more and more quickly. This suggests some pretty intense things about our future, right?\n\nKurzweil suggests that the progress of the entire 20th century would have been achieved in only 20 years at the rate of advancement in the year 2000—in other words, by 2000, the rate of progress was five times faster than the _average_ rate of progress during the 20th century. He believes another 20th century’s worth of progress happened between 2000 and 2014 and that _another_ 20th century’s worth of progress will happen by 2021, in only seven years. A couple decades later, he believes a 20th century’s worth of progress will happen multiple times in the same year, and even later, in less than one month. All in all, because of the Law of Accelerating Returns, Kurzweil believes that the 21st century will achieve _1,000 times_ the progress of the 20th century. [2](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#footnote2-2-3216)\n\nIf Kurzweil and others who agree with him are correct, then we may be as blown away by 2030 as our 1750 guy was by 2015—i.e. the next DPU might only take a couple decades—and the world in 2050 might be _so_ vastly different than today’s world that we would barely recognize it.\n\nThis isn’t science fiction. It’s what many scientists smarter and more knowledgeable than you or I firmly believe—and if you look at history, it’s what we should logically predict.\n\nSo then why, when you hear me say something like “the world 35 years from now might be totally unrecognizable,” are you thinking, “Cool….but nahhhhhhh”? Three reasons we’re skeptical of outlandish forecasts of the future:\n\n**1) When it comes to history, we think in straight lines.** When we imagine the progress of the next 30 years, we look back to the progress of the previous 30 as an indicator of how much will likely happen. When we think about the extent to which the world will change in the 21st century, we just take the 20th century progress and add it to the year 2000. This was the same mistake our 1750 guy made when he got someone from 1500 and expected to blow his mind as much as his own was blown going the same distance ahead. It’s most intuitive for us to think _linearly,_ when we should be thinking _exponentially_. If someone is being more clever about it, they might predict the advances of the next 30 years not by looking at the previous 30 years, but by taking the _current_ rate of progress and judging based on that. They’d be more accurate, but still way off. In order to think about the future correctly, you need to imagine things moving at a _much faster rate_ than they’re moving now.\n\n[![Projections](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Projections.png)](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Projections.png)\n\n**2) The trajectory of very recent history often tells a distorted story.** First, even a steep exponential curve seems linear when you only look at a tiny slice of it, the same way if you look at a little segment of a huge circle up close, it looks almost like a straight line. Second, exponential growth isn’t totally smooth and uniform. Kurzweil explains that progress happens in “S-curves”:\n\n[![S-Curves](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/S-Curves2.png)](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/S-Curves2.png)\n\nAn S is created by the wave of progress when a new paradigm sweeps the world. The curve goes through three phases:\n\n1\\. Slow growth (the early phase of exponential growth)\n\n2\\. Rapid growth (the late, explosive phase of exponential growth)\n\n3\\. A leveling off as the particular paradigm matures [3](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#footnote2-3-3216)\n\nIf you look only at very recent history, the part of the S-curve you’re on at the moment can obscure your perception of how fast things are advancing. The chunk of time between 1995 and 2007 saw the explosion of the internet, the introduction of Microsoft, Google, and Facebook into the public consciousness, the birth of social networking, and the introduction of cell phones and then smart phones. That was Phase 2: the growth spurt part of the S. But 2008 to 2015 has been less groundbreaking, at least on the technological front. Someone thinking about the future today might examine the last few years to gauge the current rate of advancement, but that’s missing the bigger picture. In fact, a new, huge Phase 2 growth spurt might be brewing right now.\n\n**3)** **Our own experience makes us stubborn old men about the future.** We base our ideas about the world on our personal experience, and that experience has ingrained the rate of growth of the recent past in our heads as “the way things happen.” We’re also limited by our imagination, which takes our experience and uses it to conjure future predictions—but often, what we know simply doesn’t give us the tools to think accurately about the future. [2](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#footnote-2-3216) When we hear a prediction about the future that contradicts our experience-based notion of _how things work_, our instinct is that the prediction must be naive. If I tell you, later in this post, that you may live to be 150, or 250, or _not die at all_, your instinct will be, “That’s stupid—if there’s one thing I know from history, it’s that everybody dies.” And yes, no one in the past has not died. But no one flew airplanes before airplanes were invented either.\n\nSo while _nahhhhh_ might feel right as you read this post, it’s probably actually wrong. The fact is, if we’re being truly logical and expecting historical patterns to continue, we should conclude that much, much, _much_ more should change in the coming decades than we intuitively expect. Logic also suggests that if the most advanced species on a planet keeps making larger and larger leaps forward at an ever-faster rate, at some point, they’ll make a leap so great that it completely alters life as they know it and the perception they have of what it means to be a human—kind of like how evolution kept making great leaps toward intelligence until finally it made such a large leap to the human being that it completely altered what it meant for any creature to live on planet Earth. And if you spend some time reading about what’s going on today in science and technology, you start to see a lot of signs quietly hinting that life as we currently know it cannot withstand the leap that’s coming next.\n\n\\\\_\\\\_\\\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n\n## The Road to Superintelligence\n\n### **What Is AI?**\n\nIf you’re like me, you used to think Artificial Intelligence was a silly sci-fi concept, but lately you’ve been hearing it mentioned by serious people, and you don’t really quite get it.\n\nThere are three reasons a lot of people are confused about the term AI:\n\n**1)** **We associate AI with movies.** Star Wars. Terminator. 2001: A Space Odyssey. Even the Jetsons. And those are fiction, as are the robot characters. So it makes AI sound a little fictional to us.\n\n**2) AI is a broad topic.** It ranges from your phone’s calculator to self-driving cars to something in the future that might change the world dramatically. AI refers to all of these things, which is confusing.\n\n**3) We use AI all the time in our daily lives, but we often don’t realize it’s AI.** John McCarthy, who coined the term “Artificial Intelligence” in 1956, complained that “as soon as it works, no one calls it AI anymore.” [4](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#footnote2-4-3216) Because of this phenomenon, AI often sounds like a mythical future prediction more than a reality. At the same time, it makes it sound like a pop concept from the past that never came to fruition. Ray Kurzweil says he hears people say that AI withered in the 1980s, which he compares to “insisting that the Internet died in the dot-com bust of the early 2000s.” [5](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#footnote2-5-3216)\n\nSo let’s clear things up. First, stop thinking of _robots_. A robot is a _container_ for AI, sometimes mimicking the human form, sometimes not—but the AI itself is the computer _inside_ the robot. AI is the brain, and the robot is its body—if it even has a body. For example, the software and data behind Siri is AI, the woman’s voice we hear is a personification of that AI, and there’s no robot involved at all.\n\nSecondly, you’ve probably heard the term “singularity” or “technological singularity.” This term has been used in math to describe an asymptote-like situation where normal rules no longer apply. It’s been used in physics to describe a phenomenon like an infinitely small, dense black hole or the point we were all squished into right before the Big Bang. Again, situations where the usual rules don’t apply. In 1993, Vernor Vinge wrote a [famous essay](https://www-rohan.sdsu.edu/faculty/vinge/misc/singularity.html) in which he applied the term to the moment in the future when our technology’s intelligence exceeds our own—a moment for him when life as we know it will be forever changed and normal rules will no longer apply. Ray Kurzweil then muddled things a bit by defining the singularity as the time when the Law of Accelerating Returns has reached such an extreme pace that technological progress is happening at a seemingly-infinite pace, and after which we’ll be living in a whole new world. I found that many of today’s AI thinkers have stopped using the term, and it’s confusing anyway, so I won’t use it much here (even though we’ll be focusing on that _idea_ throughout).\n\nFinally, while there are many different types or forms of AI since AI is a broad concept, the critical categories we need to think about are based on an AI’s _caliber_. There are three major AI caliber categories:\n\n**AI Caliber 1) Artificial Narrow Intelligence (ANI):** Sometimes referred to as _Weak AI_, Artificial Narrow Intelligence is AI that specializes in _one_ area. There’s AI that can beat the world chess champion in chess, but that’s the only thing it does. Ask it to figure out a better way to store data on a hard drive, and it’ll look at you blankly.\n\n**AI Caliber 2) Artificial General Intelligence (AGI):** Sometimes referred to as _Strong AI_, or _Human-Level AI_, Artificial General Intelligence refers to a computer that is as smart as a human _across the board—_ a machine that can perform any intellectual task that a human being can. Creating AGI is a _much_ harder task than creating ANI, and we’re yet to do it. Professor Linda Gottfredson describes intelligence as “a very general mental capability that, among other things, involves the ability to reason, plan, solve problems, think abstractly, comprehend complex ideas, learn quickly, and learn from experience.” AGI would be able to do all of those things as easily as you can.\n\n**AI Caliber 3) Artificial Superintelligence (ASI):** Oxford philosopher and leading AI thinker Nick Bostrom [defines](http://www.nickbostrom.com/superintelligence.html) superintelligence as “an intellect that is much smarter than the best human brains in practically every field, including scientific creativity, general wisdom and social skills.” Artificial Superintelligence ranges from a computer that’s just a little smarter than a human to one that’s trillions of times smarter—across the board. ASI is the reason the topic of AI is such a spicy meatball and why the words “immortality” and “extinction” will both appear in these posts multiple times.\n\nAs of now, humans have conquered the lowest caliber of AI—ANI—in many ways, and it’s everywhere. The AI Revolution is the road from ANI, through AGI, to ASI—a road we may or may not survive but that, either way, will change everything.\n\nLet’s take a close look at what the leading thinkers in the field believe this road looks like and why this revolution might happen way sooner than you might think:\n\n### Where We Are Currently—A World Running on ANI\n\nArtificial Narrow Intelligence is machine intelligence that equals or exceeds human intelligence or efficiency at a _specific_ thing. A few examples:\n\n- Cars are full of ANI systems, from the computer that figures out when the anti-lock brakes should kick in to the computer that tunes the parameters of the fuel injection systems. Google’s [self-driving car](https://www.youtube.com/channel/UCCLyNDhxwpqNe3UeEmGHl8g), which is being tested now, will contain robust ANI systems that allow it to perceive and react to the world around it.\n- Your phone is a little ANI factory. When you navigate using your map app, receive tailored music recommendations from Pandora, check tomorrow’s weather, talk to Siri, or dozens of other everyday activities, you’re using ANI.\n- Your email spam filter is a classic type of ANI—it starts off loaded with intelligence about how to figure out what’s spam and what’s not, and then it learns and tailors its intelligence to you as it gets experience with your particular preferences. The Nest Thermostat does the same thing as it starts to figure out your typical routine and act accordingly.\n- You know the whole creepy thing that goes on when you search for a product on Amazon and then you see that as a “recommended for you” product on a _different_ site, or when Facebook somehow knows who it makes sense for you to add as a friend? That’s a network of ANI systems, working together to inform each other about who you are and what you like and then using that information to decide what to show you. Same goes for Amazon’s “People who bought this also bought…” thing—that’s an ANI system whose job it is to gather info from the behavior of millions of customers and synthesize that info to cleverly upsell you so you’ll buy more things.\n- Google Translate is another classic ANI system—impressively good at one narrow task. Voice recognition is another, and there are a bunch of apps that use those two ANIs as a tag team, allowing you to speak a sentence in one language and have the phone spit out the same sentence in another.\n- When your plane lands, it’s not a human that decides which gate it should go to. Just like it’s not a human that determined the price of your ticket.\n- The world’s best Checkers, Chess, Scrabble, Backgammon, and Othello players are now all ANI systems.\n- Google search is one large ANI brain with incredibly sophisticated methods for ranking pages and figuring out what to show you in particular. Same goes for Facebook’s Newsfeed.\n- And those are just in the consumer world. Sophisticated ANI systems are widely used in sectors and industries like military, manufacturing, and finance (algorithmic high-frequency AI traders account for more than half of equity shares traded on US markets [6](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#footnote2-6-3216)), and in expert systems like those that help doctors make diagnoses and, most famously, IBM’s [Watson](http://www.ibm.com/smarterplanet/us/en/ibmwatson/), who contained enough facts and understood coy Trebek-speak well enough to soundly beat the most prolific _Jeopardy_ champions.\n\nANI systems as they are now aren’t especially scary. At worst, a glitchy or badly-programmed ANI can cause an isolated catastrophe like knocking out a power grid, causing a harmful nuclear power plant malfunction, or triggering a financial markets disaster (like the [2010 Flash Crash](http://www.ritholtz.com/blog/wp-content/uploads/2010/10/flash-crash-dow-popup.png) when an ANI program reacted the wrong way to an unexpected situation and caused the stock market to briefly plummet, taking $1 trillion of market value with it, only part of which was recovered when the mistake was corrected).\n\nBut while ANI doesn’t have the capability to cause an _existential threat_, we should see this increasingly large and complex ecosystem of relatively-harmless ANI as a precursor of the world-altering hurricane that’s on the way. Each new ANI innovation quietly adds another brick onto the road to AGI and ASI. Or as Aaron Saenz [sees it](http://singularityhub.com/2010/08/10/we-live-in-a-jungle-of-artificial-intelligence-that-will-spawn-sentience/), our world’s ANI systems “are like the amino acids in the early Earth’s primordial ooze”—the inanimate stuff of life that, one unexpected day, woke up.\n\n### **The Road From ANI to AGI**\n\nWhy It’s So Hard\n\nNothing will make you appreciate human intelligence like learning about how unbelievably challenging it is to try to create a computer as smart as we are. Building skyscrapers, putting humans in space, figuring out the details of how the Big Bang went down—all far easier than understanding our own brain or how to make something as cool as it. As of now, the human brain is the most complex object in the known universe.\n\nWhat’s interesting is that the hard parts of trying to build AGI (a computer as smart as humans in _general_, not just at one narrow specialty) are not intuitively what you’d think they are. Build a computer that can multiply two ten-digit numbers in a split second—incredibly easy. Build one that can look at a dog and answer whether it’s a dog or a cat—spectacularly difficult. Make AI that can beat any human in chess? Done. Make one that can read a paragraph from a six-year-old’s picture book and not just recognize the words but understand the _meaning_ of them? Google is currently spending [billions](http://www.wired.com/2014/01/google-buying-way-making-brain-irrelevant/) of dollars trying to do it. Hard things—like calculus, financial market strategy, and language translation—are mind-numbingly easy for a computer, while easy things—like vision, motion, movement, and perception—are insanely hard for it. Or, as computer scientist Donald Knuth puts it, “AI has by now succeeded in doing essentially everything that requires ‘thinking’ but has failed to do most of what people and animals do ‘without thinking.'” [7](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#footnote2-7-3216)\n\nWhat you quickly realize when you think about this is that those things that seem easy to us are actually unbelievably complicated, and they only seem easy because those skills have been optimized in us (and most animals) by hundreds of millions of years of animal evolution. When you reach your hand up toward an object, the muscles, tendons, and bones in your shoulder, elbow, and wrist instantly perform a long series of physics operations, in conjunction with your eyes, to allow you to move your hand in a straight line through three dimensions. It seems effortless to you because you have perfected software in your brain for doing it. Same idea goes for why it’s not that malware is dumb for not being able to figure out the slanty word recognition test when you sign up for a new account on a site—it’s that your brain is super impressive for being _able_ to.\n\nOn the other hand, multiplying big numbers or playing chess are new activities for biological creatures and we haven’t had any time to evolve a proficiency at them, so a computer doesn’t need to work too hard to beat us. Think about it—which would you rather do, build a program that could multiply big numbers or one that could understand the essence of a B well enough that you could show it a B in any one of thousands of unpredictable fonts or handwriting and it could instantly know it was a B?\n\nOne fun example—when you look at this, you and a computer both can figure out that it’s a rectangle with two distinct shades, alternating:\n\n[![Screen Shot 2015-01-21 at 12.59.21 AM](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Screen-Shot-2015-01-21-at-12.59.21-AM.png)](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Screen-Shot-2015-01-21-at-12.59.21-AM.png)\n\nTied so far. But if you pick up the black and reveal the whole image…\n\n[![Screen Shot 2015-01-21 at 12.59.54 AM](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Screen-Shot-2015-01-21-at-12.59.54-AM.png)](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Screen-Shot-2015-01-21-at-12.59.54-AM.png)\n\n…you have no problem giving a full description of the various opaque and translucent cylinders, slats, and 3-D corners, but the computer would fail miserably. It would describe what it sees—a variety of two-dimensional shapes in several different shades—which is actually what’s there. Your brain is doing a ton of fancy shit to interpret the implied depth, shade-mixing, and room lighting the picture is trying to portray. [8](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#footnote2-8-3216) And looking at the picture below, a computer sees a two-dimensional white, black, and gray collage, while you easily see what it really is—a photo of an entirely-black, 3-D rock:\n\n[![article-2053686-0E8BC15900000578-845_634x330](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/article-2053686-0E8BC15900000578-845_634x330.jpg)](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/article-2053686-0E8BC15900000578-845_634x330.jpg)\n\nCredit: Matthew Lloyd\n\nAnd everything we just mentioned is still only taking in stagnant information and processing it. To be human-level intelligent, a computer would have to understand things like the difference between subtle facial expressions, the distinction between being pleased, relieved, content, satisfied, and glad, and why _Braveheart_ was great but _The Patriot_ was terrible.\n\nDaunting.\n\nSo how do we get there?\n\nFirst Key to Creating AGI: Increasing Computational Power\n\nOne thing that definitely needs to happen for AGI to be a possibility is an increase in the power of computer hardware. If an AI system is going to be as intelligent as the brain, it’ll need to equal the brain’s raw computing capacity.\n\nOne way to express this capacity is in the total calculations per second (cps) the brain could manage, and you could come to this number by figuring out the maximum cps of each structure in the brain and then adding them all together.\n\nRay Kurzweil came up with a shortcut by taking someone’s professional estimate for the cps of one structure and that structure’s weight compared to that of the whole brain and then multiplying proportionally to get an estimate for the total. Sounds a little iffy, but he did this a bunch of times with various professional estimates of different regions, and the total always arrived in the same ballpark—around 1016, or 10 quadrillion cps.\n\nCurrently, the world’s fastest supercomputer, China’s [Tianhe-2](http://www.reuters.com/article/2014/11/17/us-china-supercomputer-idUSKCN0J11VV20141117), has actually beaten that number, clocking in at about 34 quadrillion cps. But Tianhe-2 is also a dick, taking up 720 square meters of space, using 24 megawatts of power (the brain runs on just [20 watts](http://www.popsci.com/technology/article/2009-11/neuron-computer-chips-could-overcome-power-limitations-digital)), and costing $390 million to build. Not especially applicable to wide usage, or even most commercial or industrial usage yet.\n\nKurzweil suggests that we think about the state of computers by looking at how many cps you can buy for $1,000. When that number reaches human-level—10 quadrillion cps—then that’ll mean AGI could become a very real part of life.\n\n[Moore’s Law](https://en.wikipedia.org/wiki/Moore%27s_law) is a historically-reliable rule that the world’s maximum computing power doubles approximately every two years, meaning computer hardware advancement, like general human advancement through history, grows exponentially. Looking at how this relates to Kurzweil’s cps/$1,000 metric, we’re currently at about 10 trillion cps/$1,000, right on pace with this graph’s predicted trajectory: [9](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#footnote2-9-3216)\n\n[![PPTExponentialGrowthof_Computing-1](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/PPTExponentialGrowthof_Computing-1.jpg)](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/PPTExponentialGrowthof_Computing-1.jpg)\n\nSo the world’s $1,000 computers are now beating the mouse brain and they’re at about a thousandth of human level. This doesn’t sound like much until you remember that we were at about a trillionth of human level in 1985, a billionth in 1995, and a millionth in 2005. Being at a thousandth in 2015 puts us right on pace to get to an affordable computer by 2025 that rivals the power of the brain.\n\nSo on the hardware side, the raw power needed for AGI is technically available now, in China, and we’ll be ready for affordable, widespread AGI-caliber hardware within 10 years. But raw computational power alone doesn’t make a computer generally intelligent—the next question is, how do we bring human-level intelligence to all that power?\n\nSecond Key to Creating AGI: Making It Smart\n\nThis is the icky part. The truth is, no one really knows how to make it smart—we’re still debating how to make a computer human-level intelligent and capable of knowing what a dog and a weird-written B and a mediocre movie is. But there are a bunch of far-fetched strategies out there and at some point, one of them will work. Here are the three most common strategies I came across:\n\n#### **1) Plagiarize the brain.**\n\nThis is like scientists toiling over how that kid who sits next to them in class is so smart and keeps doing so well on the tests, and even though they keep studying diligently, they can’t do nearly as well as that kid, and then they finally decide “k fuck it I’m just gonna copy that kid’s answers.” It makes sense—we’re stumped trying to build a super-complex computer, and there happens to be a perfect prototype for one in each of our heads.\n\nThe science world is working hard on reverse engineering the brain to figure out how evolution made such a rad thing— [optimistic estimates](http://www.wired.com/2010/08/reverse-engineering-brain-kurzweil/) say we can do this by 2030. Once we do that, we’ll know all the secrets of how the brain runs so powerfully and efficiently and we can draw inspiration from it and steal its innovations. One example of computer architecture that mimics the brain is the artificial neural network. It starts out as a network of transistor “neurons,” connected to each other with inputs and outputs, and it knows nothing—like an infant brain. The way it “learns” is it tries to do a task, say handwriting recognition, and at first, its neural firings and subsequent guesses at deciphering each letter will be completely random. But when it’s told it got something right, the transistor connections in the firing pathways that happened to create that answer are strengthened; when it’s told it was wrong, those pathways’ connections are weakened. After a lot of this trial and feedback, the network has, by itself, formed smart neural pathways and the machine has become optimized for the task. The brain learns a bit like this but in a more sophisticated way, and as we continue to study the brain, we’re discovering ingenious new ways to take advantage of neural circuitry.\n\nMore extreme plagiarism involves a strategy called “whole brain emulation,” where the goal is to slice a real brain into thin layers, scan each one, use software to assemble an accurate reconstructed 3-D model, and then implement the model on a powerful computer. We’d then have a computer officially capable of everything the brain is capable of—it would just need to learn and gather information. If engineers get _really_ good, they’d be able to emulate a real brain with such exact accuracy that the brain’s full personality and memory would be intact once the brain architecture has been uploaded to a computer. If the brain belonged to Jim right before he passed away, the computer would now wake up as Jim ( [?](https://waitbutwhy.com/2014/12/what-makes-you-you.html)), which would be a robust human-level AGI, and we could now work on turning Jim into an unimaginably smart ASI, which he’d probably be really excited about.\n\nHow far are we from achieving whole brain emulation? Well so far, we’ve not yet [just recently](http://www.smithsonianmag.com/smart-news/weve-put-worms-mind-lego-robot-body-180953399/?no-ist) been able to emulate a 1mm-long flatworm brain, which consists of just 302 total neurons. The human brain contains 100 billion. If that makes it seem like a hopeless project, remember the power of exponential progress—now that we’ve conquered the tiny worm brain, an ant might happen before too long, followed by a mouse, and suddenly this will seem much more plausible.\n\n#### **2) Try to make evolution do what it did before but for us this time.**\n\nSo if we decide the smart kid’s test is too hard to copy, we can try to copy the way he _studies_ for the tests instead.\n\nHere’s something we know. Building a computer as powerful as the brain _is_ possible—our own brain’s evolution is proof. And if the brain is just too complex for us to emulate, we could try to emulate _evolution_ instead. The fact is, even if we can emulate a brain, that might be like trying to build an airplane by copying a bird’s wing-flapping motions—often, machines are best designed using a fresh, machine-oriented approach, not by mimicking biology exactly.\n\nSo how can we simulate evolution to build AGI? The method, called “genetic algorithms,” would work something like this: there would be a performance-and-evaluation process that would happen again and again (the same way biological creatures “perform” by living life and are “evaluated” by whether they manage to reproduce or not). A group of computers would try to do tasks, and the most successful ones would be _bred_ with each other by having half of each of their programming merged together into a new computer. The less successful ones would be eliminated. Over many, many iterations, this natural selection process would produce better and better computers. The challenge would be creating an automated evaluation and breeding cycle so this evolution process could run on its own.\n\nThe downside of copying evolution is that evolution likes to take a billion years to do things and we want to do this in a few decades.\n\nBut we have a lot of advantages over evolution. First, evolution has no foresight and works randomly—it produces more unhelpful mutations than helpful ones, but we would control the process so it would only be driven by beneficial glitches and targeted tweaks. Secondly, evolution doesn’t _aim_ for anything, including intelligence—sometimes an environment might even select _against_ higher intelligence (since it uses a lot of energy). We, on the other hand, could specifically direct this evolutionary process toward increasing intelligence. Third, to select for intelligence, evolution has to innovate in a bunch of other ways to facilitate intelligence—like revamping the ways cells produce energy—when we can remove those extra burdens and use things like electricity. It’s no doubt we’d be much, much faster than evolution—but it’s still not clear whether we’ll be able to improve upon evolution _enough_ to make this a viable strategy.\n\n#### **3) Make this whole thing the computer’s problem, not ours.**\n\nThis is when scientists get desperate and try to program the test to take itself. But it might be the most promising method we have.\n\nThe idea is that we’d build a computer whose two major skills would be doing research on AI and coding changes into itself—allowing it to not only learn but to improve its own _architecture_. We’d teach computers to be computer scientists so they could bootstrap their own development. And that would be their main job—figuring out how to make _themselves_ smarter. More on this later.\n\n#### **All of This Could Happen Soon**\n\nRapid advancements in hardware and innovative experimentation with software are happening simultaneously, and AGI could creep up on us quickly and unexpectedly for two main reasons:\n\n1) Exponential growth is intense and what seems like a snail’s pace of advancement can quickly race upwards—this GIF illustrates this concept nicely:\n\n[![](https://waitbutwhy.com/wp-content/uploads/2015/01/gif)](https://waitbutwhy.com/wp-content/uploads/2015/01/gif)\n\n[Source](http://www.motherjones.com/media/2013/05/robots-artificial-intelligence-jobs-automation)\n\n2) When it comes to software, progress can seem slow, but then one epiphany can instantly change the rate of advancement (kind of like the way science, during the time humans thought the universe was geocentric, was having difficulty calculating how the universe worked, but then the discovery that it was heliocentric suddenly made everything _much_ easier). Or, when it comes to something like a computer that improves itself, we might seem far away but actually be just one tweak of the system away from having it become 1,000 times more effective and zooming upward to human-level intelligence.\n\n### The Road From AGI to ASI\n\nAt some point, we’ll have achieved AGI—computers with human-level general intelligence. Just a bunch of people and computers living together in equality.\n\nOh actually not at all.\n\nThe thing is, AGI with an identical level of intelligence and computational capacity as a human would still have significant advantages over humans. Like:\n\n**Hardware:**\n\n- **Speed.** The brain’s neurons max out at around 200 Hz, while today’s microprocessors (which are much slower than they will be when we reach AGI) run at 2 GHz, or 10 million times faster than our neurons. And the brain’s internal communications, which can move at about 120 m/s, are horribly outmatched by a computer’s ability to communicate optically at the speed of light.\n- **Size and storage.** The brain is locked into its size by the shape of our skulls, and it couldn’t get much bigger anyway, or the 120 m/s internal communications would take too long to get from one brain structure to another. Computers can expand to any physical size, allowing far more hardware to be put to work, a much larger working memory (RAM), and a longterm memory (hard drive storage) that has both far greater capacity and precision than our own.\n- **Reliability and durability.** It’s not only the memories of a computer that would be more precise. Computer transistors are more accurate than biological neurons, and they’re less likely to deteriorate (and can be repaired or replaced if they do). Human brains also get fatigued easily, while computers can run nonstop, at peak performance, 24/7.\n\n**Software:**\n\n- **Editability, upgradability, and a wider breadth of possibility.** Unlike the human brain, computer software can receive updates and fixes and can be easily experimented on. The upgrades could also span to areas where human brains are weak. Human vision software is superbly advanced, while its complex engineering capability is pretty low-grade. Computers could match the human on vision software but could _also_ become equally optimized in engineering and any other area.\n- **Collective capability.** Humans crush all other species at building a vast collective intelligence. Beginning with the development of language and the forming of large, dense communities, advancing through the inventions of writing and printing, and now intensified through tools like the internet, humanity’s collective intelligence is one of the major reasons we’ve been able to get so far ahead of all other species. And computers will be way better at it than we are. A worldwide network of AI running a particular program could regularly sync with itself so that anything any one computer learned would be instantly uploaded to all other computers. The group could also take on one goal as a unit, because there wouldn’t necessarily be dissenting opinions and motivations and self-interest, like we have within the human population. [10](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#footnote2-10-3216)\n\nAI, which will likely get to AGI by being programmed to self-improve, wouldn’t see “human-level intelligence” as some important milestone—it’s only a relevant marker from our point of view—and wouldn’t have any reason to “stop” at our level. And given the advantages over us that even human intelligence-equivalent AGI would have, it’s pretty obvious that it would only hit human intelligence for a brief instant before racing onwards to the realm of superior-to-human intelligence.\n\nThis may shock the shit out of us when it happens. The reason is that from _our_ perspective, A) while the intelligence of different kinds of animals varies, the main characteristic we’re aware of about any animal’s intelligence is that it’s far lower than ours, and B) we view the smartest humans as WAY smarter than the dumbest humans. Kind of like this:\n\n[![Intelligence](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Intelligence.jpg)](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Intelligence.jpg)\n\nSo as AI zooms upward in intelligence toward us, we’ll see it as simply becoming smarter, _for an animal._ Then, when it hits the lowest capacity of humanity—Nick Bostrom uses the term “the village idiot”—we’ll be like, “Oh wow, it’s like a dumb human. Cute!” The only thing is, in the grand spectrum of intelligence, _all_ humans, from the village idiot to Einstein, are within a very small range—so _just_ after hitting village idiot level and being declared to be AGI, it’ll suddenly be smarter than Einstein and we won’t know what hit us:\n\n[![Intelligence2](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Intelligence2.png)](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/Intelligence2.png)\n\nAnd what happens…after that?\n\nAn Intelligence Explosion\n\nI hope you enjoyed normal time, because this is when this topic gets unnormal and scary, and it’s gonna stay that way from here forward. I want to pause here to remind you that every single thing I’m going to say is real—real science and real forecasts of the future from a large array of the most respected thinkers and scientists. Just keep remembering that.\n\nAnyway, as I said above, most of our current models for getting to AGI involve the AI getting there by self-improvement. And once it gets to AGI, even systems that formed and grewthrough methods that didn’t involve self-improvement would now be smart enough to begin self-improving if they wanted to. [3](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#footnote-3-3216)\n\nAnd here’s where we get to an intense concept: **recursive self-improvement.** It works like this—\n\nAn AI system at a certain level—let’s say human village idiot—is programmed with the goal of improving its own intelligence. Once it does, it’s _smarter—_ maybe at this point it’s at Einstein’s level—so now when it works to improve its intelligence, with an Einstein-level intellect, it has an easier time and it can make bigger leaps. These leaps make it _much_ smarter than any human, allowing it to make even _bigger_ leaps. As the leaps grow larger and happen more rapidly, the AGI soars upwards in intelligence and soon reaches the superintelligent level of an ASI system. This is called an Intelligence Explosion, [11](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#footnote2-11-3216) and it’s the ultimate example of The Law of Accelerating Returns.\n\nThere is some debate about how soon AI will reach human-level general intelligence. The median year on a survey of hundreds of scientists about when they believed we’d be more likely than not to have reached AGI was 2040 [12](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#footnote2-12-3216)—that’s only 25 years from now, which doesn’t sound that huge until you consider that many of the thinkers in this field think it’s likely that the progression from AGI to ASI happens _very_ quickly. Like—this could happen:\n\n_It takes decades for the first AI system to reach low-level general intelligence, but it finally happens. A computer is able to understand the world around it as well as a human four-year-old. Suddenly, within an hour of hitting that milestone, the system pumps out the grand theory of physics that unifies general relativity and quantum mechanics, something no human has been able to definitively do. 90 minutes after that, the AI has become an ASI, 170,000 times more intelligent than a human._\n\nSuperintelligence of that magnitude is not something we can remotely grasp, any more than a bumblebee can wrap its head around Keynesian Economics. In our world, smart means a 130 IQ and stupid means an 85 IQ—we don’t have a word for an IQ of 12,952.\n\nWhat we do know is that humans’ utter dominance on this Earth suggests a clear rule: _with intelligence comes power._ Which means an ASI, when we create it, will be the most powerful being in the history of life on Earth, and all living things, including humans, will be entirely at its whim— _and this might happen_ _in the next few decades._\n\nIf our meager brains were able to invent wifi, then something 100 or 1,000 or 1 billion times smarter than we are should have no problem controlling the positioning of each and every atom in the world in any way it likes, at any time—everything we consider magic, every power we imagine a supreme God to have will be as mundane an activity for the ASI as flipping on a light switch is for us. Creating the technology to reverse human aging, curing disease and hunger and even mortality, reprogramming the weather to protect the future of life on Earth—all suddenly possible. Also possible is the immediate end of all life on Earth. As far as we’re concerned, if an ASI comes to being, there is now an omnipotent God on Earth—and the all-important question for us is:\n\n_**Will it be a nice God?**_\n\nThat’s the topic of [Part 2 of this post](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html).\n\n_(Sources at the bottom of [Part 2](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html).)_\n\n[![](https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/PDF-Gray-v2.png)](https://gum.co/wbw-ai1)\n\n**Related Wait But Why Posts**\n\n[The Fermi Paradox](https://waitbutwhy.com/2014/05/fermi-paradox.html) – Why don’t we see any signs of alien life?\n\n[How (and Why) SpaceX Will Colonize Mars](https://waitbutwhy.com/2015/08/how-and-why-spacex-will-colonize-mars.html) – A post I got to work on with Elon Musk and one that reframed my mental picture of the future.\n\nOr for something totally different and yet somehow related, [Why Procrastinators Procrastinate](https://waitbutwhy.com/2013/10/why-procrastinators-procrastinate.html)\n\nAnd here’s [Year 1 of Wait But Why](https://www.amazon.com/Wait-But-Why-Year-One-ebook/dp/B00TXYJOZG/ref=sr_1_1?s=books&ie=UTF8&qid=1424988766&sr=1-1&keywords=tim+urban) on an ebook.\n\n\\\\_\\\\_\\\\_\\_\\_\\_\\_\n\nIf you like Wait But Why, sign up for our **[email list](https://newsletter.waitbutwhy.com/join)** and we’ll send you new posts when they come out.\n\nTo support Wait But Why, visit our **[Patreon page](https://patreon.com/waitbutwhy)**.\n\n* * *\n\n1. Okay so there are two different kinds of notes now. The blue circles are the fun/interesting ones you should read. They’re for extra info or thoughts that I didn’t want to put in the main text because either it’s just tangential thoughts on something or because I want to say something a notch too weird to just be there in the normal text.[↩](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#note-1-3216)\n\n2. Kurzweil points out that his phone is about a millionth the size of, a millionth the price of, and a thousand times more powerful than his MIT computer was 40 years ago. Good luck trying to figure out where a comparable future advancement in computing would leave us, let alone one far, far more extreme, since the progress grows exponentially. [↩](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#note-2-3216)\n\n3. Much more on what it means for a computer to “want” to do something in the Part 2 post. [↩](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#note-3-3216)\n\n\n* * *\n\n01. Gray squares are boring objects and when you click on a gray square, you’ll end up bored. These are for sources and citations only. [↩](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#note2-1-3216)\n\n02. Kurzweil, [_The Singularity is Near_](https://www.amazon.com/gp/product/0143037889/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0143037889&linkCode=as2&tag=wabuwh00-20&linkId=54Q62R5PYJBEENTP), 39. [↩](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#note2-2-3216)\n\n03. Kurzweil, [_The Singularity is Near_](https://www.amazon.com/gp/product/0143037889/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0143037889&linkCode=as2&tag=wabuwh00-20&linkId=54Q62R5PYJBEENTP), 84. [↩](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#note2-3-3216)\n\n04. Vardi, _[Artificial Intelligence: Past and Future](http://cacm.acm.org/magazines/2012/1/144824-artificial-intelligence-past-and-future/fulltext)_, 5. [↩](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#note2-4-3216)\n\n05. Kurzweil, [_The Singularity is Near_](https://www.amazon.com/gp/product/0143037889/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0143037889&linkCode=as2&tag=wabuwh00-20&linkId=54Q62R5PYJBEENTP), 392. [↩](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#note2-5-3216)\n\n06. Bostrom, [_Superintelligence: Paths, Dangers, Strategies_](https://www.amazon.com/gp/product/0199678111/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0199678111&linkCode=as2&tag=wabuwh00-20&linkId=LBOTX2G2R72P5EUA), loc. 597 [↩](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#note2-6-3216)\n\n07. Nilsson, [_The Quest for Artificial Intelligence: A History of Ideas and Achievements_](https://www.amazon.com/gp/product/0521122937/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0521122937&linkCode=as2&tag=wabuwh00-20&linkId=QIJQME4U3J2KZRRY), 318. [↩](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#note2-7-3216)\n\n08. Pinker, _[How the Mind Works](https://www.amazon.com/gp/product/1491514965/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=1491514965&linkCode=as2&tag=wabuwh00-20&linkId=NJ47RPDRBVZA6QPU)_, 36. [↩](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#note2-8-3216)\n\n09. Kurzweil, [_The Singularity is Near_](https://www.amazon.com/gp/product/0143037889/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0143037889&linkCode=as2&tag=wabuwh00-20&linkId=54Q62R5PYJBEENTP), 118. [↩](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#note2-9-3216)\n\n10. Bostrom, [_Superintelligence: Paths, Dangers, Strategies_](https://www.amazon.com/gp/product/0199678111/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0199678111&linkCode=as2&tag=wabuwh00-20&linkId=LBOTX2G2R72P5EUA), loc. 1500-1576. [↩](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#note2-10-3216)\n\n11. This term was first used by one of history’s great AI thinkers, Irving John Good, in 1965. [↩](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#note2-11-3216)\n\n12. Nick Bostrom, [_Superintelligence: Paths, Dangers, Strategies_](https://www.amazon.com/gp/product/0199678111/ref=as_li_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0199678111&linkCode=as2&tag=wabuwh00-20&linkId=LBOTX2G2R72P5EUA), loc. 660 [↩](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html#note2-12-3216)\n\n\n[Tweet](https://twitter.com/share)\n\n[![](https://assets.pinterest.com/images/pidgets/pinit_fg_en_rect_gray_20.png)](https://www.pinterest.com/pin/create/button/?url=https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html&description=The AI Revolution: The Road to Superintelligence)\n\n![Tim Urban](https://149909199.v2.pressablecdn.com/wp-content/uploads/userphoto/3.thumbnail.jpg)\n\n#### About Tim Urban\n\n[View all posts by Tim Urban →](https://waitbutwhy.com/author/timurban)\n\n[Previous Post](https://waitbutwhy.com/2015/01/evolution-alphabet.html)\n\n[Next Post](https://waitbutwhy.com/2015/01/most-depressing-buzzfeed-article-of-all-time.html)\n\n### RECOMMENDED POSTS\n\n- [![](https://149909199.v2.pressablecdn.com/wp-content/uploads/2024/02/FEATURE_WBW.png)](https://waitbutwhy.com/2024/02/vision-pro.html)\n\n\n##### [All My Thoughts After 40 Hours in the Vision Pro](https://waitbutwhy.com/2024/02/vision-pro.html)\n\n\n\nFebruary 9, 2024\n\n- [![](https://149909199.v2.pressablecdn.com/wp-content/uploads/2020/09/FEATURE-site-2048x1388.png)](https://waitbutwhy.com/2020/09/universe.html)\n\n\n##### [The Big and the Small](https://waitbutwhy.com/2020/09/universe.html)\n\n\n\nSeptember 22, 2020\n\n- [![](https://149909199.v2.pressablecdn.com/wp-content/uploads/2018/04/FEATURE.png)](https://waitbutwhy.com/2017/04/neuralink.html)\n\n\n##### [Neuralink and the Brain’s Magical Future](https://waitbutwhy.com/2017/04/neuralink.html)\n\n\n\nApril 20, 2017\n\n\n[![](https://149909199.v2.pressablecdn.com/wp-content/uploads/2014/03/wbw_store_short_ad_tee1.jpg)](http://store.waitbutwhy.com)\n\n[Home](https://waitbutwhy.com) [Archive](https://waitbutwhy.com/archive/)",
        "image": "https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/G1.jpg",
        "favicon": "https://149909199.v2.pressablecdn.com/wp-content/themes/waitbutwhy/images/favicon.ico"
      },
      {
        "id": "https://jessbpeck.com/posts/artificialintelligence/",
        "title": "A Guide To Artificial Intelligence in Easy English",
        "url": "https://jessbpeck.com/posts/artificialintelligence/",
        "publishedDate": "2019-03-24T00:00:00.000Z",
        "author": "",
        "score": 0.3492070734500885,
        "text": "# A Guide To Artificial Intelligence in Easy English\n\n- [What is AI?](https://jessbpeck.com/posts/artificialintelligence/#what-is-ai%3F)\n- [So what is Machine Learning?](https://jessbpeck.com/posts/artificialintelligence/#so-what-is-machine-learning%3F)\n- [So how do Dall-e and GPT work?](https://jessbpeck.com/posts/artificialintelligence/#so-how-do-dall-e-and-gpt-work%3F)\n- [So GPT](https://jessbpeck.com/posts/artificialintelligence/#so-gpt)\n- [A Minor Example](https://jessbpeck.com/posts/artificialintelligence/#a-minor-example)\n- [Big Data Problems](https://jessbpeck.com/posts/artificialintelligence/#big-data-problems)\n- [Slurs vs. Slurs (affectionate)](https://jessbpeck.com/posts/artificialintelligence/#slurs-vs.-slurs-(affectionate))\n- [GPT Doesn’t ‘Know’](https://jessbpeck.com/posts/artificialintelligence/#gpt-doesn%E2%80%99t-%E2%80%98know%E2%80%99)\n- [DALL-E](https://jessbpeck.com/posts/artificialintelligence/#dall-e)\n- [Steal like an Artist](https://jessbpeck.com/posts/artificialintelligence/#steal-like-an-artist)\n- [DALL-E Problems](https://jessbpeck.com/posts/artificialintelligence/#dall-e-problems)\n- [Can AI think? Can AI make art?](https://jessbpeck.com/posts/artificialintelligence/#can-ai-think%3F-can-ai-make-art%3F)\n- [Specific questions + misconceptions](https://jessbpeck.com/posts/artificialintelligence/#specific-questions-+-misconceptions)\n\n[Soundtrack](https://www.youtube.com/watch?v=qGBohd0V2Mo)\n\n![An advanced ML algorithm attempts to write this blogpost for me, writing: My mum and I had a conversation about a year ago about DALLE. I was trying to explain to her what it was, and she was like, \"Oh, that's just AI.\" I was like, \"No, it's not. It's a very specific kind of AI.\" She was like, \"Oh, it's just AI.\" and that continues on and on.](https://jessbpeck.com/img/copilot_write_my_blogpost.png)\n\n[Soundtrack](https://www.youtube.com/watch?v=qGBohd0V2Mo)\n\nI had a conversation with my mum a while ago about the new DALL-E ‘thing’ that came out. My mum's an artist and a writer-- not a technophobe or a luddite by any means, but not a developer either. I’m not sure if I imagined the concern in her voice or not, but the tone of the conversation was bleak. The images generated by DALL-E were vivid, and beautiful, and seemed like art.\n\nWell, maybe that's not fair. Maybe not ‘seemed.’\n\nThis blog post is an overview and summary of several different AI topics. I'm writing specifically for audiences unfamiliar with or comfortable with AI, coding, or math. Some of these topics I'm planning on giving deeper dives on their own, but I think we're kind of at a tipping point here. There's a new kind of automation on the horizon, and I think it's important that information about this is accessible and understandable to everyone.\n\n[What is AI?](https://jessbpeck.com/posts/artificialintelligence/#what-is-ai%25)\n\n[So what is Machine Learning?](https://jessbpeck.com/posts/artificialintelligence/#so-what-is-machine-learning)\n\n[So how do Dall-e and GPT work?](https://jessbpeck.com/posts/artificialintelligence/#so-how-do-dall-e-and-gpt-work)\n\n[So GPT](https://jessbpeck.com/posts/artificialintelligence/#so-gpt)\n\n[A Minor Example](https://jessbpeck.com/posts/artificialintelligence/#a-minor-example)\n\n[Big Data Problems](https://jessbpeck.com/posts/artificialintelligence/#big-data-problems)\n\n[Slurs vs. Slurs (affectionate)](https://jessbpeck.com/posts/artificialintelligence/#slurs-vs.-slurs-(affectionate))\n\n[GPT Doesn’t ‘Know’](https://jessbpeck.com/posts/artificialintelligence/#gpt-doesn%E2%80%99t-%E2%80%98know%E2%80%99)\n\n[DALL-E](https://jessbpeck.com/posts/artificialintelligence/#dall-e)\n\n[Steal like an Artist](https://jessbpeck.com/posts/artificialintelligence/#steal-like-an-artist)\n\n[DALL-E Problems](https://jessbpeck.com/posts/artificialintelligence/#dall-e-problems)\n\n[Can AI think? Can AI make art?](https://jessbpeck.com/posts/artificialintelligence/#can-ai-think%3F-can-ai-make-art%3F)\n\n[Specific questions + misconceptions](https://jessbpeck.com/posts/artificialintelligence/#specific-questions-%2B-misconceptions)\n\n### What is AI? [\\#](https://jessbpeck.com/posts/artificialintelligence/\\#what-is-ai%3F)\n\nOne thing that my mother told me on our call, was: \"If you write an explainer blogpost, use 'AI' in the title, not 'ML.' Nobody knows what ML means.\" She might have been exaggerating, but she's not wrong. Most people refer to AI, even if it's just talking about Skynet, the Matrix, and so on. ML is a bit more of a mystery to people.\n\nSo-- what is AI? Well, for one thing, it's a misnomer.\n\nTo understand the phrase 'Artificial Intelligence,' you must first be able to define intelligence. This is a bit like saying 'to understand archery you must first be able to shoot the sun.' Sure, you'll probably aim it the right way, but you're not going to easily hit your target.\n\n![](https://jessbpeck.com/img/artificial/intelligence.png)\n\nWhat is intelligence? As a stereotype, your brain might first jump to “a guy who can add up numbers very quickly.” Maybe you have some people in mind-- Einstein, Newton, Feynman. Maybe you’re aware that your first thoughts are white and western and male, so you also think about Rosalind Franklin or Ada Lovelace. Nevertheless: you probably think about intelligence as a virtue: one tied to being able to think well. After that, peeling back the onion, you probably understand intelligence as a measure of something’s sentience. Do dogs have intelligence? Do bugs?\n\nI’ve used AI in the title of this piece, and I’m using it here, because it’s the popular nomenclature. But in my experience, AI is more of a marketing term than one that people in the ML space use. I myself use “Machine Learning,” as do (as far as I can tell) most engineers, ethicists, and researchers who deal with “can we make robots think.”\n\nA good callout my friend Luke made is to distinguish further between ‘AI’ and ‘AGI.’ AGI, or Artificial General Intelligence, is the kind of… alchemist-turning-lead-into-gold standard for this kind of research. It’s ‘creating a machine that can learn anything a human can’-- a generalized artificial intelligence. Part of the problem with using “AI” is that it brings in the implications of AGI-- saying “AI” makes users think of AGI, when they’re really dealing with a flawed, specific, ML algorithm.\n\n### So what is Machine Learning? [\\#](https://jessbpeck.com/posts/artificialintelligence/\\#so-what-is-machine-learning%3F)\n\nMachine Learning is the term for trying to get machines to figure stuff out without being specifically programmed.\n\nAs an example: you have a pile of widgets, and you want to sort them into two piles based on their attributes. A traditional programming approach would do something like ‘if color=red, left, else, right.”\n\nA machine learning approach would be more like “there are x sets of widgets: using the information you have sort these piles.” Depending on how closely you monitor the results, you might get the same end result-- or you might get something completely new.\n\nFor example, you could _train_ the algorithm by labelling a smaller number of widgets (this is what a blue one looks like, this is what a red one looks like) and then correcting it if it goes off the beaten path. (\"This isn't red, it's green!\") You could also just put as much data as possible in (this is what the widgets look like these are their differences, these are their similarities) and let the algorithm figure it out. If you have more data (either more labels/descriptions, or just more widgets) you'll get different results.\n\nOne major shift in the landscape of ML has been the ability to use MASSIVE datasets for training: and while the traditional wisdom is \"the more data you have, the more accurate your results will be,\" the reality is that the more data you have, the more you can train your algorithm to be wrong.\n\n![](https://jessbpeck.com/img/artificial/sorting.png)\n\nTraditional programming relies a lot on ‘knowns’-- you know what’s going in and what you expect to get out of it. The trick is getting a from b. Machine learning relies on bulk information-- this is a, this is b, figure it out.\n\nIntelligence vs. thought, thought vs. being. It's all very philosophical and not very practical. A lot of discussions come down to that one scene from iRobot, you know the one:\n\n![will smith irobot meme format: will says \"can you write me a sonnet on the subject of the fourth bridge\". the robot says \"can you\". will looks thoughtful. ](https://jessbpeck.com/img/artificial/irobot.png)\n\n### So how do Dall-e and GPT work? [\\#](https://jessbpeck.com/posts/artificialintelligence/\\#so-how-do-dall-e-and-gpt-work%3F)\n\nSomething I’ve often said and read is that good machine learning is bad statistics. If statistics is trying to describe data well by using the right numbers, machine learning is trying to use numbers to get the right descriptions. If you don’t do statistics, that’s the opposite of how it’s supposed to work.\n\nFor example: I have a machine count and categorize my widgets. I know I have good numbers, and I expect there to be 50% red widgets. I set my counting machine to count, and get a result of '10% red widgets.' From here, I am at a crossroads: statistics is the practice of updating my earlier assumption, knowing that I have 10% red widgets, not 50% as I started. ML can be fucking around with the inputs until you get the 50% number you were expecting-- '10% doesn't seem right, there must be something wrong with the training data.' (It depends on what you’re using the ML for, though!)\n\nI think one of the ways you can really understand GPT is by running a more simple version yourself. You can do it yourself for free-- [I love Max Woolf’s blogpost + code here.](https://www.google.com/url?q=https://minimaxir.com/2019/09/howto-gpt2/&sa=D&source=editors&ust=1671131712693979&usg=AOvVaw1qAWclc56hRAko6eIE_zll)\n\nWhat can we see using this and learning from this code?\n\nFor those who don’t click the link: GPT-2 is an earlier version of GPT. Often you will see GPT described as a ‘black box’ because of the complicated, transformers-based ML architecture. It was trained on text that was gathered by looking at Reddit links. So the ML engineers took those links from Reddit (which has its own biases), and cleaned it up somewhat, removing spam and mess. GPT then took that text, and looked for patterns in it. You can then input your own tokens (which is called prompting) or fine tune it further to perform specific tasks.\n\nIf you prompt GPT with “Once--” it looks through the patterns it observed from the text it was trained on. Common sentences that start with “once” might be “once upon a time.” But if you had additional text before that (“I was a young child. Once--”) that will change the parameters of the prediction.\n\nLet's take an example outside of text: I have an ML algorithm that says who will win a World Cup game. It will be affected by the prompts that go in. \"France vs. Brazil\" will have a different outcome based on weather, starting line up, whether Mbappe is playing, and so on. While the algorithm can't take into consideration all of those factors, you can update it and give it parameters to play with. If it doesn't have data about weather, saying \"The weather is bad\" will not result in changes. But if it has weather data, the weather being bad will affect its prediction of the results.\n\nThere are lots of ways to perform ‘machine learning,’ but most of them are ways of looking for patterns in large reams of data and being able to replicate those patterns with new prompts and data sets.\n\n### So GPT [\\#](https://jessbpeck.com/posts/artificialintelligence/\\#so-gpt)\n\nGPT is a text model- if you’ve used chat GPT, or other interfaces (the openAI playground was really popular on Tumblr earlier this year), this is the model you’ve played with. GPT is an LLM, or a Large Language Model. And this is where I’m going to underline some words for you. It’s a Language model: it’s a probability distribution. Basically, it guesses, based on the words around it, what the next word in a sentence will be, and it’s very good at that.\n\nBut that’s _all_ it’s good at. Like parrots, students before 8am, and dogs, GPT can put up a good act of understanding words, and like the former two cases it may be able to repeat those words or mix them up so they sound good. But that is not the same as understanding.\n\nHumans are fundamentally a social species-- we exist to interact with other humans and use their knowledge to augment our own. This means we have a bias towards anthropomorphization - we want to see humanity in other things, even inanimate objects. The problem is that when you bring in an LLM that can plausibly mimic human thought processes, humans have a tendency to overextend them a welcoming hand to the human race.\n\n### A Minor Example [\\#](https://jessbpeck.com/posts/artificialintelligence/\\#a-minor-example)\n\nLet's look at an example here.\n\nI’ve asked GPT-3 to write me a paragraph with the prompt: “Please write a paragraph about the human tendency to anthropomorphize non-sentient creatures and how that impulse can cause problems as people begin to interact with LLMs en masse, include citations. “\n\nI’ve cut out some of the response (it was mostly about what anthropomorphizing is with examples about pets,) but this is the money of it:\n\n_” This same impulse may also cause issues when people begin to interact with large language models (LLMs) en masse. If people begin to anthropomorphize LLMs and ascribe emotions and consciousness to them, it could lead to misunderstandings and potentially harmful interactions. To avoid these problems, it is important for people to be aware of their own tendencies to anthropomorphize and to approach LLMs with a clear understanding of their capabilities and limitations.”_\n\n![Boris Artzybasheff illustration of the personification of machinery](https://jessbpeck.com/img/artificial/Artzybasheff.png)\n\nLet’s look at and think about this response to my prompt. In some ways, it’s extremely impressive: it fit the brief (other than not including citations.) But it’s a shallow reading and shallow response: I asked it to write about these problems, and it said “there are problems.” While misunderstandings are slightly more specific, “potentially harmful interactions” is not.\n\nI can ask the LLM to be more specific, and it will be, but this still identifies two problems with LLMs. One is that writing without understanding is fundamentally weak. The second problem is a category, one we will call:\n\n### Big Data Problems [\\#](https://jessbpeck.com/posts/artificialintelligence/\\#big-data-problems)\n\nLLMs are trained on millions, even billions of pieces of data across the web. This causes the same kinds of problems you get when anything is trained on web data-- from machines to human beings. A lot of the stuff on the web is obscene, or offensive, or if it’s not either of those things, it’s bad SEO-laden marketing slop. The internet was designed to bring together the world’s sharpest minds and they created a playground where you get advertised megachurches on sites where sex workers are banned but Pornhub has an account where it tweets tasteless, cheeky, corporate synergy. The web is often sanitized, but not in a way that makes anyone safer; only a way that makes it more boring.\n\n![](https://jessbpeck.com/img/artificial/man_in_front_of_monitor.png)\n\nThis is the soup that trains these large language models. Data cleaning is one of the biggest problems that apparently goes unsolved in ML research. This is where you take your data-- texts, images, or so on-- and clean it up, making it useable, yes, but also making sure you don’t have anything that pollutes your dataset.\n\nLet’s look at a practical example. Amazon, historically, has hired developers primarily from MIT and other big colleges. They created a ML algorithm based on this profile: the algorithm ended up discriminating against [perfectly good engineers from historically black colleges](https://www.google.com/url?q=https://www.reuters.com/article/us-amazon-com-jobs-automation-insight/amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK08G&sa=D&source=editors&ust=1671131712696778&usg=AOvVaw0J4rxNC8zEjFMPfbeI-2g5).(And women.)\n\n### Slurs vs. Slurs (affectionate) [\\#](https://jessbpeck.com/posts/artificialintelligence/\\#slurs-vs.-slurs-(affectionate))\n\nSo maybe part of that is cleaning curse words, porn spam, or nonsensical garbage out. But maybe as a step you want to avoid your chatbot becoming a nazi, so you get rid of anything that could be considered offensive to a minority group. But the problem with \\_that\\_ is that human language is complex and strange. As an expletive-laden example, see below:\n\n‘Fuck off you gay bitch’-- me to my friends when we get wine drunk and watch the bachelorette.\n\n‘Fuck off you gay bitch’-- the man following me home after pride, screaming at me and throwing bottles\n\nYou and I can probably tell which of those is a hate crime and which is not; but, isolated from context and whirring without human decision-making abilities, it’s almost impossible for a LLM to tell the difference. And that is a problem when you’re talking about the breadth of human experience.\n\n_This is a problem that Google has run into historically as well. Using another gay example; for a long time, if you Googled ‘lesbian’, all you’d get is reams of porn. This is one of those instances where I could complain about the way the gay woman exists in a place of fetishization or something, but I’m interested in the problem Google came up against here. Because more people are probably Googling to find pornography than there are lesbians looking for connections. There are probably more horny straight men than lesbian women (especially because lesbians use duckduckgo.) If Google wants to be a happiness engine, well, one response will make more people happy. But if it wants to have the right response, those people will have to do more clicking, or worse, go to another site._\n\n![Examples of syntactic and lexical ambiguity. Lexica=l ambiguity is when the presence of two or more possible meanings is contained within a single word (I saw her duck), while syntactic is the presence of two or more possible meanings within a single sentence or sequence of words (the chicken is ready to eat)](https://jessbpeck.com/img/artificial/syntacticambiguity.png)\n\nLLMs cannot understand the context of their sentences beyond how they are programmed to: which is to say, semantically. It probably knows based on the large swathes of text it has absorbed that flat Earth is a conspiracy theory; but does it understand that the conspiracy theory is an antisemitic dog whistle? Context is part of language: and while language models have been trained on millions of words and the order of those words they cannot have the context a person who is alive on the earth has.\n\nSo TLDR: GPT and other LLMs work by guessing, statistically, what the most likely responses to a prompt are and how likely those words are to be followed by other words. This can lead to some incredible texts, fun experiments, and plausible sentences, but it fundamentally lacks the ability to parse, understand, and argue points. This is all-important to understand as you interact with LLMs and the space around them. I personally think it can be interesting or useful to use these models to augment human intelligence: stringing together an outline, writing a summary, and rewriting text. But even in these cases, trying to fake domain knowledge by using GPT is a high-risk effort.\n\n### GPT Doesn’t ‘Know’ [\\#](https://jessbpeck.com/posts/artificialintelligence/\\#gpt-doesn%E2%80%99t-%E2%80%98know%E2%80%99)\n\n![](https://jessbpeck.com/img/artificial/pexelsimage.png)\n\nGPT doesn’t know what the important parts of papers are and it doesn’t know if a paper was researched well or not. It doesn’t know about agendas, meta analysis, or statistical significance.\n\nA few days ago a tweet went around encouraging people to use ChatGPT to summarize a scientific paper about xylitol in a more readable way. All I could think about was Andrew Wakefield, the man who is the epicentre of our current vaccine hysteria. If you were to put his paper in ChatGPT, you’d get an intelligent-sounding, authoritative, uncritical summary, ready to propagate antivaccine propaganda.\n\nA case study that is often brought up for GPT is code-- ChatGPT has a code generation feature that was promoted widely. StackOverflow pretty quickly banned GPT-generated code from being used as a response to their questions. Folks on Twitter, especially AI-positive people, quickly said this was StackOverflow trying to muscle out their competition.\n\nBut guys.\n\nThe GPT code was bad.\n\nIt’s pretty okay at common/often written about code challenges, but the second you go off the beaten path it relies on for loops, bad formatting, and TODO statements that would make me blush.\n\nThe current level of response from GPT-Chat is amazing. I’ve argued that it is probably about the same level as a low-effort human being. But that’s just it-- we already have low effort content out there. Don’t you want to make something good?\n\n### DALL-E [\\#](https://jessbpeck.com/posts/artificialintelligence/\\#dall-e)\n\nNow that we know how GPT works, we can think about DALL-E in that same way. It is simply predicting what it thinks the next pixel over will look like in color, based on training data from thousands of artists who didn’t consent to have their work used in this way.\n\nThis is, I think, the middle point between two groups that have the most different points of view about intellectual property rights: for artists, signatures, color marks, and credit HAVE to be everything. While the internet can bring people fame from nothing, it can also mean your work gets all the serial numbers filed off and it ends up on 4chan years later heavily edited as a racist meme.\n\nDevelopers, on the other hand, praise the almighty MIT license. Sharing code-- grabbing it from stackoverflow, using other people’s modules, downloading NPM packages-- these are all such major parts of modern web development there’s a joke about everything relying on one package by a guy in Omaha. There’s not often credit, there’s not often fame, and there’s not often rights. There is, however, money, and work, and it’s working so far, right?\n\nIt’s a bleak thing: the continued underappreciation of art has led to artworks being used to replace the artists who created them. The result is a model that represents a kind of collective unconscious of art; DALL-E creates beautiful things. (Biased things.)\n\n### Steal like an Artist [\\#](https://jessbpeck.com/posts/artificialintelligence/\\#steal-like-an-artist)\n\nIn 2012, I had a conversation with my mum that I remember vividly. I was watching the [United States of Pop 2011 mashup](https://www.google.com/url?q=https://www.youtube.com/watch?v%3Dail7D_k0s9w&sa=D&source=editors&ust=1671131712700504&usg=AOvVaw123TRqliewlqyPuMwxValL) (made by an artist called DJ Earworm) and mum asked if I thought the mashup disrespected the original artists. I replied that I didn’t think so-- I thought it was cool that the constituent elements could be brought together to make something new, vibrant, and fun.\n\nIn the same way, to some extent, I feel like I cannot muster up the same rage many artists do when they think about DALL-E. I feel that rage when developers and developer fanboys make fun of artists for being upset, denigrate the very art they have built their models on and are generally rude and cruel.\n\nBut the ability to generate art in seconds, creating a very complicated collage? I can’t hate that. I can’t hate that people who can’t draw can create awesome drawings very quickly, in the same way I can’t hate that photographs replaced portraits, in the same way I can’t hate that pong replaced tennis, in the same way collages, Rothko, and Duchamp's fountain are or aren't art.\n\n![a mashup, a labor of love: the cover of Bartkira, a collaboration between hundreds of artists to recreate the graphic novel akira with simpsons characters](https://jessbpeck.com/img/artificial/image3.png)\n\nBut it’s always this sort of balancing act, isn’t it? I make digital art: as someone who does that, I have been accused of not making real art: as though I press ‘control paint’ and my image is fully produced and extant with no work of my own.\n\n![Some of my art i made using digital painting techniques](https://jessbpeck.com/img/artificial/jesssart.png)\n\nBut now people can do that. GarageBand guitar loops haven’t stopped people from learning the guitar, Wix hasn’t stopped web developers, but it still feels bad to see someone put no effort into something you’ve put effort into and get the same (or more) credit.\n\nI also want to draw a line between using DALL-E and other image-generation platforms for joy and creativity and using it to soullessly automate away the artistic process. There’s a difference between a guy who can’t draw using it to create an image he has in his head or heart and a guy trying to create the most popular #content for the rolling purposes of content creation, or pretending he painted it from scratch.\n\nPart of an ideal world for artists is that they do not have to create corporate coprolith to survive; unfortunately, we are automating away any job that could bring an element of joy or creativity, leaving three classes of Americans: suits, service workers, and slaves.\n\nI don’t think McDonalds' will ever be fully automated, because part of what people pay for is the human interaction, a person saying “of course, whatever you want” and smiling. Similarly, with these ML leaps forward: there will be some jobs, jobs with people faces, that survive. I cannot say what will happen to the rest. (As one of my editors noted: there are already touchscreens to order in the US. There is already some automation of this kind of job. What does that leave us with?)\n\n### DALL-E Problems [\\#](https://jessbpeck.com/posts/artificialintelligence/\\#dall-e-problems)\n\nEarly on, DALL-E got called out for a lack of diversity in their response images. It would return white male doctors for the input 'doctors,' and women for the input 'nurses,' and so on. Think about it-- black writers have been talking for years about a lack of diversity in stock images and what that can reflect about the organization. You scoop in a ton of milk-white doctors and stereotypes about black people from the internet, and you get an image model that reflects that-- remember, it’s looking at what the most statistically likely pixel is.\n\nWhen called out for this, the DALLE team sprung into action-- not by fixing inputs or weights, but by stapling words like ‘female’ or ‘black’ to the end of user prompts. This did work-- it did result in a more diverse result. But it also meant users could display those stapled words by simpling adding ‘a person holding a sign that says’ to the prompt.\n\n![Dr. Casey Fiesler tweets: \"And when I asked DALL-E for 'cartoon secretary holding a sign that says\" one of the images seems to have still appended \"female.\" I did get one obviously male secretary on another prompt though (shrug emoji)](https://jessbpeck.com/img/artificial/image1.png)\n\n![A series of 'clip art' style generated business people, with misspelled attempts at \"female\" or other 'diversity markers' on signs](https://jessbpeck.com/img/artificial/image5.png)\n\nMost software systems are built like this-- people are pushed to produce code, and produce it quickly, which leads to quick fixes. Those quick fixes can be more embarrassing/silly than the initial mistake was.\n\n![Image description: A series of posts from Jason Lefkowitz @jalefkowit@octodon.social dated Dec 08, 2022, 04:33, reading:It’s good that our finest minds have focused on automating writing and making art, two things human beings do simply because it brings them joy. Meanwhile tens of thousands of people risk their lives every day breaking down ships, a task that nobody is in a particular hurry to automate because those lives are considered cheap (Headline: ‘Recycling a ship is always dangerous.’ on Deutsche Welle)A world where computers write and make art while human beings break their backs cleaning up toxic messes is the exact opposite of the world I thought I was signing up for when I got into programming](https://jessbpeck.com/img/artificial/image4.png)\n\n[Recycling a ship is always dangerous](https://www.dw.com/en/shipbreaking-recycling-a-ship-is-always-dangerous/a-18155491)\n\n### Can AI think? Can AI make art? [\\#](https://jessbpeck.com/posts/artificialintelligence/\\#can-ai-think%3F-can-ai-make-art%3F)\n\nWith all of this comes the big question: can AI think? Can AI create?\n\nMy personal answer to this differs from what I have heard from many ML and AI researchers. Most good ML researchers are solidly on the side that LLMs, diffusion models, and other ML models are not sentient and cannot be sentient in the way we define sentient.\n\nI agree with this in most ways: however, this is the caveat I would like to put forward. LLMs are either already sentient, or they will never be.\n\nWe already talked about intelligence at the beginning of this very very long piece. Sentience is a related subject: Sentience is, according to Wikipedia, the capacity to experience feelings and sensations. So how do you find out if something is sentient?\n\nThe NPC meme is a big one in right-wing circles; it’s a meme I fervently dislike, but it’s useful for explanation purposes. The ‘meme’ goes like this: some people aren’t people, but more like NPCs in a video game. NPCs are ‘Non player characters,’ the characters in a game that aren’t controlled by the player and simply follow computer scripts in their heads. This meme applies that to human people. They believe this means some people have no capacity for individual thought, no feelings-- they’re philosophical zombies. They are not sentient.\n\n![An example of the npc meme in action.](https://jessbpeck.com/img/artificial/npcmemesample.png)\n\nI bring up this repulsive reference to say we do not know if people are sentient. you can prove humans are sentient, in a bunch of different ways for a bunch of varying definitions within a number of different philosophical schools., but definitively, scientifically, there is no way to know. If someone says “I am sentient” how can they prove it? All we can know is what other people tell us: and it is easy to get an LLM to respond to a prompt saying it ‘feels’ something. You can never objectively prove another human has sentience in a way that can't also be disproven.\n\nDescartes thought animals were \"automatons\" and not sentient. Humans are collections of previous experiences and data filtered through several neural networks to make decisions based on probabilities. If you grind a GPU (Graphics processing unit) down to silicon particulate, you won’t find an atom of feeling: if you grind a brain down to its atomic parts, you won’t find feelings there either.\n\nSo: LLMs are either already sentient, or they will never be.\n\nBut usually, when people talk about sentience, they mean the end of the world scenario-- Skynet, Rokos Basilisk, and other extremely serious and non-silly threats!\n\nThe thing is: the tools are already being used for evil, cruel purposes. Humans are using AI to hurt each other in the present. The number of things that would need to go catastrophically wrong for AI to be any more dangerous than most other threats is kind of ridiculous. Google has an AI on a supercomputer: how is that thing going to get its hands on nukes?\n\nNo, the problems you need to keep an eye on with ML are the ones where it exaggerates and multiplies the problems that already exist.\n\nML is not dangerous, in of itself. It is when it is used carelessly, or by bad actors, that the harm comes in. ML is not likely to decide the best way to create world peace is to kill all humans: it is likely to show venture capitalists where to buy up houses to make the most money, exacerbating the housing crisis.\n\nCopilot wrote these last few lines: \"It is likely to show advertisers where to target people to make the most money, exacerbating the wealth gap. It is likely to show police where to target people to make the most money, exacerbating the prison industrial complex.\"\n\nYeah, buddy. You got it.\n\n### Specific questions + misconceptions [\\#](https://jessbpeck.com/posts/artificialintelligence/\\#specific-questions-%2B-misconceptions)\n\n(I'll update this over time as I get questions or requests for clarification.)\n\n1. What is GPT?\n\nGPT is a large language model that uses neural networks and transformers to guess at what the most likely words in a sentence will be.\n\n![An example of the GPT3 playground and percentages](https://jessbpeck.com/img/artificial/image2.png)\n\n1. What is DALL-E?\n\nDALL-E is a model that uses something called Stable Diffusion to generate images, predicting on what the most likely position of pixels is compared to other pixels for the prompt\n\n1. What should I use GPT for?\n\nSome automation (getting tokens/keywords, basic automation of summaries, getting data from unstructured data), prompting you if you get stuck writing, coming up with ideas, having fun.\n\n1. What should I avoid using GPT for?\n\nAvoid letting GPT think for you.\n\n1. Is it AI?\n\nArtificial Intelligence is a kind of computer science boogyman/buzzword. You’ll get less hype if you talk about ML, but it’ll be more accurate.\n\n1. Can ML replace humans in creative endeavours?\n\nYes, but it will be worse at it.\n\nThanks to the folks who read/edited this (and offered to) before it went out: my friend Clare, my editor Ellie, [Alex](https://www.google.com/url?q=https://twitter.com/AlexHarfordSEO&sa=D&source=editors&ust=1671131712706582&usg=AOvVaw0F7HYJMsDTILEJ4rk_p_oE), [Iman](https://www.google.com/url?q=https://twitter.com/clorinda__&sa=D&source=editors&ust=1671131712706709&usg=AOvVaw1-1jwWSGZx5Lpb9uUA9tCC), [Dáre](https://www.google.com/url?q=https://twitter.com/darewithdata&sa=D&source=editors&ust=1671131712706796&usg=AOvVaw1_9tKsZr74AXBfx_9-HfGN), [Mats](https://www.google.com/url?q=https://twitter.com/mtolander&sa=D&source=editors&ust=1671131712706884&usg=AOvVaw19KF4HCksYtDrbEXHAVlfO), [Luke](https://www.google.com/url?q=https://twitter.com/LukeDavisSEO&sa=D&source=editors&ust=1671131712707008&usg=AOvVaw27t3xgtEcdPkA80332RTPg), and [Marco](https://www.google.com/url?q=https://twitter.com/GiordMarco96&sa=D&source=editors&ust=1671131712707097&usg=AOvVaw0zdaJT8v4MdXqQPl29qU2t).\n\n#### Related/Sources [\\#](https://jessbpeck.com/posts/artificialintelligence/\\#related%2Fsources)\n\n- [The Bell Curve - Shaun](https://www.google.com/url?q=https://www.youtube.com/watch?v%3DUBc7qBS1Ujo&sa=D&source=editors&ust=1671131712707420&usg=AOvVaw3AVf1_qOJxf_8iMnT48lPD)\n\n- [This AI Made Me WHITE - Foreign Man in A Foreign Land](https://www.google.com/url?q=https://www.youtube.com/watch?v%3DXI9haLvhy9c&sa=D&source=editors&ust=1671131712707570&usg=AOvVaw07btiA3VR6yX4-KBkuUhTX)\n\n- [Weird Kids Videos and Gaming the Algorithm - Folding Ideas](https://www.google.com/url?q=https://www.youtube.com/watch?v%3DLKp2gikIkD8%26t%3D14s&sa=D&source=editors&ust=1671131712707724&usg=AOvVaw3mkOsxxZ-IRWX4pwgU15k8)\n\n- [An “Unbiased” Guide to Bias in AI \\| by Shahrokh Barati \\| Dec, 2022 \\| Towards Data Science](https://www.google.com/url?q=https://towardsdatascience.com/an-unbiased-guide-to-bias-in-ai-3841c2b36165&sa=D&source=editors&ust=1671131712707949&usg=AOvVaw2KuoxHyzMDxzBKpDb6MQrz)\n\n- [Why Fairness Cannot Be Automated: Bridging the Gap Between EU Non-Discrimination Law and AI](https://www.google.com/url?q=https://papers.ssrn.com/sol3/papers.cfm?abstract_id%3D3547922&sa=D&source=editors&ust=1671131712708146&usg=AOvVaw1o9UIjzok6nuCshZf-Qwry)\n\n- [Algorithmic injustice: a relational ethics approach - ScienceDirect](https://www.google.com/url?q=https://www.sciencedirect.com/science/article/pii/S2666389921000155&sa=D&source=editors&ust=1671131712708326&usg=AOvVaw3ZOtKGSX874qJbVZLxVoFH)\n\n- [Preventing bias in ML models, with code](https://www.google.com/url?q=https://sararobinson.dev/2019/03/24/preventing-bias-machine-learning.html&sa=D&source=editors&ust=1671131712708491&usg=AOvVaw0Tce8_x1PaMqsZvkep2FaD)\n\n- [https://twitter.com/Grady\\_Booch/status/1602192416211054592](https://www.google.com/url?q=https://twitter.com/Grady_Booch/status/1602192416211054592&sa=D&source=editors&ust=1671131712708683&usg=AOvVaw3cgmf69hs-a0BUqVnv3aMv)\n\n- [https://twitter.com/mmitchell\\_ai/status/1277649075844890626](https://www.google.com/url?q=https://twitter.com/mmitchell_ai/status/1277649075844890626&sa=D&source=editors&ust=1671131712708832&usg=AOvVaw39Mpckpg0DhupkyYFVr1Xf)\n\n- [https://twitter.com/fchollet/status/1307552020312727552](https://www.google.com/url?q=https://twitter.com/fchollet/status/1307552020312727552&sa=D&source=editors&ust=1671131712708991&usg=AOvVaw3GoEVOv3LsospkFuRij3Xs)\n\n- [https://twitter.com/mmitchell\\_ai/status/1375893744998739968](https://www.google.com/url?q=https://twitter.com/mmitchell_ai/status/1375893744998739968&sa=D&source=editors&ust=1671131712709142&usg=AOvVaw0Pj7dIl6-1OEjQkhi_oNpd)\n\n- [The Turing Test- then and now](https://twitter.com/Grady_Booch/status/1600662643559124992)\n\n\nSince you’re here… If you can spare 5 bucks, and enjoy my work, [I’d appreciate it if you donate a bit of money to the Sylvia Rivera Law Project. Thanks!](https://www.google.com/url?q=https://srlp.org/donate/&sa=D&source=editors&ust=1671131712709418&usg=AOvVaw15qNeYYN5e7qoC-sMyN1jb)\n\n[← Home](https://jessbpeck.com/)"
      },
      {
        "id": "https://thezvi.wordpress.com/2023/04/13/on-autogpt/",
        "title": "On AutoGPT",
        "url": "https://thezvi.wordpress.com/2023/04/13/on-autogpt/",
        "publishedDate": "2023-04-13T00:00:00.000Z",
        "author": "Posted on",
        "score": 0.35997274518013,
        "text": "## [On AutoGPT](https://thezvi.wordpress.com/2023/04/13/on-autogpt/)\n\nPosted on [April 13, 2023](https://thezvi.wordpress.com/2023/04/13/on-autogpt/) by[TheZvi](https://thezvi.wordpress.com/author/thezvi/)\n\nThe primary talk of the AI world recently is about AI agents.\n\nThe trigger for this was [AutoGPT](https://github.com/Torantulino/Auto-GPT), now number one on GitHub, which allows you to turn GPT-4 (or GPT-3.5 for us clowns without proper access) into a prototype version of a self-directed agent.\n\nWe also have a paper out this week where a simple virtual world was created, populated by LLMs that were wrapped in code designed to make them simple agents, and then several days of activity were simulated, during which the AI inhabitants interacted, formed and executed plans, and it all seemed like the beginnings of a living and dynamic world. Game version hopefully coming soon.\n\nHow should we think about this? How worried should we be?\n\n#### The Basics\n\nI’ll reiterate the basics of what AutoGPT is, for those who need that, others can skip ahead. I talked briefly about this [in AI#6 under the heading ‘Your AI Not an Agent? There, I Fixed It](https://thezvi.substack.com/i/111749937/your-ai-not-an-agent-there-i-fixed-it).’\n\nAutoGPT was created by game designer Toran Bruce Richards.\n\nI previously incorrectly understood it as having been created by a non-coding VC over the course of a few days. The VC instead coded the similar program BabyGPT, by having the idea for how to turn GPT-4 into an agent. The VC had GPT-4 write the code to make this happen, and also ‘write the paper’ associated with it.\n\nThe concept works like this:\n\n[![Image](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6b54cb0-baff-4afd-897f-f1b8542f8eb7_680x475.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6b54cb0-baff-4afd-897f-f1b8542f8eb7_680x475.png)\n\nAutoGPT uses GPT-4 to generate, prioritize and execute tasks, using plug-ins for internet browsing and other access. It uses outside memory to keep track of what it is doing and provide context, which lets it evaluate its situation, generate new tasks or self-correct, and add new tasks to the queue, which it then prioritizes.\n\nThis quickly rose to become #1 on GitHub and [get lots of people super excited](https://twitter.com/heyBarsee/status/1646270998243581954). People are excited, people are building it tools, [there is a bitcoin wallet interaction available](https://twitter.com/NicerInPerson/status/1646079839567245312) if you never liked your bitcoins. AI agents offer very obvious promise, both in terms of mundane utility via being able to create and execute multi-step plans to do your market research and anything else you might want, and in terms of potentially being a path to AGI and getting us all killed, either with GPT-4 or a future model.\n\nAs with all such new developments, we have people saying it was inevitable and they knew it would happen all along, and others that are surprised. We have people excited by future possibilities, others not impressed because the current versions haven’t done much. Some see the potential, others the potential for big trouble, others both.\n\nAlso as per standard procedure, we should expect rapid improvements over time, both in terms of usability and underlying capabilities. There are any number of obvious low-hanging-fruit improvements available.\n\nAn example is someone noting ‘you have to keep an eye on it to ensure it is not caught in a loop.’ That’s easy enough to fix.\n\nA common complaint is lack of focus and tendency to end up distracted. Again, the obvious things have not been tried to mitigate this. We don’t know how effective they will be, but no doubt they will at least help somewhat.\n\n#### Yes, But What Has Auto-GPT Actually Accomplished?\n\nSo far? [Nothing, absolutely nothing, stupid, you so stupid](https://www.youtube.com/watch?v=KezvwARhBIc&ab_channel=sirstrongbad).\n\nYou can say [your ‘mind is blown’ by all the developments of the past 24 hours](https://twitter.com/JayScambler/status/1645603816111308800) all you want over and over, it still does not net out into having accomplished much of anything.\n\nThat’s not _quite_ fair.\n\nSome people are reporting it has been useful as a way of generating market research, that it is good at this and faster than using the traditional GPT-4 or Bing interfaces. I saw a claim that it can have ‘complex conversations with customers,’ or a few other vague similar claims that weren’t backed up by ‘we are totally actually doing this now.’\n\nRight now, AutoGPT has a tendency to get distracted or confused or caught in a loop, to leave things half-finished, to not be that robust of an agent, and other issues like that. Positive reports seem limited to things GPT-4 or Bing can essentially do anyway, with the agent wrapper perhaps cutting down somewhat on how often you have to poke the interface with a stick to keep it pointed in a reasonable direction. And perhaps the Auto version is already somewhat better there.\n\nOther than that? I see a surprising amount of dead silence.\n\n[Or, as Brian Jackson puts it](https://twitter.com/brianjjackson/status/1646146478463647744):\n\n> I feel like maybe the people saying AutoGPT and ChatGPT can create “high-quality content” have different standards for content than I do.\n\nNo doubt most people are more interested in extracting mundane utility or trying things out than in bragging about or sharing the results on the internet. There is still a striking lack of internet bragging and results results sharing happening.\n\nDo keep this in mind.\n\n#### Just Think of the Potential\n\nThat does not mean that all the people saying AutoGPTs are the future are wrong.\n\nAutoGPT’s list of real accomplishments won’t stay non-existent for long. Most everyone is still in the ‘wrap head around it’ stage or the basic setup and tool building stages. The UIs are terrible, many basic tools don’t yet exist, many basic improvements haven’t been made. What exists today is much worse than what will exist in two weeks, let alone two months or two years, even without other AI improvements.\n\nSo, what will future versions of this be able to do?\n\nCertainly future versions will have better memories, better focus, better self-reflection, better plug-ins, better prioritization algorithms, better ways for humans to steer while things are progressing, better monitoring of sub-tasks, better UIs (e.g. [there is already a browser-based AgentGPT](https://twitter.com/ParentZap/status/1646120116898213889)), better configurability, better prompt engineering and so on, even with zero fundamental innovations or other AI advances.\n\nThis process will rapidly shed a light on what things are _actually_ hard for the underlying LLM to handle, and which things only require the right scaffolding.\n\nIt seems reasonable to expect a number of large step-jumps in capability for such systems as various parts improve.\n\nOne way to think about this is that AutoGPT is fundamentally constructed in terms of its ability to assign and execute sub-tasks, that are like recursive function calls until there is a subtask small enough that the system can execute directly. Once you start being able to reliably complete larger and more complex subtasks by asking, one can then batch _those_ and repeat, especially if the system has a good handle on what types of subtasks it can and can’t successfully assign. And so on. Also there’s clearly tons of tinkering to be done on lots of different levels to improve performance.\n\nWoe to those who don’t extrapolate here. For example [this post warns](https://coinatory.com/2023/04/12/gpt-4-apps-babyagi-and-autogpt-could-have-disruptive-implications-for-crypto/?utm_source=TW&utm_medium=Coinatory+Twitter+Page+-+News+Link&utm_campaign=SNAP) that ‘AutoGPT could be disruptive for crypto’ due to their ability to be autonomous, a classic version of crypto’s default of making everything about them. It notes that right now anything created using the GPT API ‘can only be used once’ but if true that is the sort of thing that will inevitably change – one must learn to think ahead. That to me is exactly the wrong model of how to think about future developments.\n\nI am highly uncertain, on many levels, where all of this caps out, or how far we might be able to take it how fast.\n\nShould we encourage such work, or discourage it? How excited should we be? Should we be worried?\n\n#### The Good, the Bad and the Agent Overhang\n\nI have gained confidence in my position that all of this happening now is a good thing, both from the perspective of smaller risks like malware attacks, and from the perspective of potential existential threats. Seems worth going over the logic.\n\nWhat we want to do is avoid what one might call an _agent overhang._\n\nOne might hope to execute our Plan A of having our AIs not be agents. Alas, even if technically feasible (which is not at all clear) that only can work if we don’t intentionally _turn them into_ agents via wrapping code around them. We’ve checked with actual humans about the possibility of kindly not doing that. Didn’t go great.\n\nSo, Plan B, then.\n\nIf we are definitely going to turn our AIs into agents in the future and there is no way to stop that, which is clearly the case, then better to first turn our _current_ AIs into agents _now._ That way, we won’t suddenly be dealing with highly capable AI agents at some point the future, we will instead gradually face more capable AI agents, such that we’ll hopefully get ‘fire alarms’ and other chances to error correct.\n\nOur current LLMs like GPT-4 are not, in their base configurations, agents. They do not have goals. This is a severe limitation on what they are able to accomplish, and how well they can help us accomplish our own goals, whatever they might be, including using them to build more capable AIs or more capable systems that incorporate AIs.\n\nThus, one can imagine a future version of GPT-N, that is supremely superhuman at a wide variety of tasks, where we can ask it questions like ‘how do we make humans much smarter?’ or ‘how do we build an array of safe, efficient fusion power plants?’ or anything else we might want, and we don’t have to worry about it attempting to navigate a path through causal space towards its end goal, it will simply give us its best answer to the information on the level on which the question was intended.\n\nUsing this tool, we could perhaps indeed make ourselves smarter and more capable, then figure out how to build more general, more agentic AIs, figure out in what configuration we want to place the universe, and then get a maximally good future.\n\nThat does not mean that this is what would happen if we managed to not turn GPT-N into an agent first, or that getting to this result is easy. One must notice that _in order to predict the next token as well as possible_ the LMM will benefit from being able to simulate every situation, every person, and every causal element behind the creation of every bit of text in its training distribution, no matter what we then train the LMM to output to us (what mask we put on it) afterwards. The LLM will absolutely ‘know’ in some sense what it means to be an agent, and how to steer physical reality by charting a path through causal space.\n\nWill that cause the LLM to act as if it were an agent _during the training run,_ seeking goals that arise out of the training run and thus almost certainly are only maximally fulfilled in ways that involve the LLM taking control of the future (and likely killing everyone), before we even get a chance to use RLHF on it? During the RLHF training run? Later on? At what level does this happen?\n\nWe don’t know. I could believe a wide variety of answers here.\n\nWhat we _do_ know is that if you _intentionally turn the LLM into an agent,_ you are going to get, a lot earlier down the line, something that looks a lot more like an agent.\n\nWe also know that _humans who get their hands on these LLMs will do their best to turn them into agents_ as quickly and effectively as possible.\n\nWe don’t only know that. We also know that no matter how stupid you think an instruction would be to give to a self-directed AI agent, no matter how much no movie that starts this way could possibly ever end well, that’s _exactly_ one of the first things someone is going to try, except they’re going to go intentionally make it even worse than that.\n\nThus, for example, we already have ChaosGPT, told explicitly to cause mayhem, sow distrust and destroy the entire human race. This should at least partially answer your question of ‘why would an AI want to destroy humanity?’ it is because humans are going to tell it to do that.\n\nThat is in addition to all the people who will give their AutoGPT an instruction that _means well_ but actually translates to killing all the humans or at least take control over the future, since that is so obviously the easiest way to accomplish the thing, such as ‘bring about world peace and [end world hunger](https://twitter.com/SullyOmarr/status/1645828811680800768)’ (link goes to Sully hyping AutoGPT, saying ‘you give it a goal like end world hunger’) or ‘stop climate change’ or ‘deliver my coffee every morning at 8am sharp no matter what as reliably as possible.’ Or _literally almost anything else._\n\nSeriously, if you find a genie I highly recommend not wishing for anything.\n\nFor now, AutoGPT is harmless. Let’s ensure that the moment it’s _mostly_ harmless, we promptly edit the Hitchhiker’s Guide entry.\n\nLet’s therefore _run experiments_ of various sorts, so we know _exactly_ how much damage could be done, and in what ways, at every step.\n\nOne good idea is to use games to put such systems to the test.\n\n#### Let’s Put an AI NPC in a Game and See if It Takes Over the World\n\n> Marek Rosa: Interestingly, when people discuss LLMs and game NPCs, they mostly see only the conversational AI use case. But there is another option: using LLMs to control NPC’s behavior. This would let NPCs interact with the game world (observe and act) and even talk to other NPCs, creating a more sandbox experience.\n>\n> [Alex Tabarrok](https://twitter.com/ATabarrok/status/1645202730221293568): Let’s Put an AI NPC in a Game and See if It Takes Over the World\n\nI’d already been talking about this a bit in private conversations, as a demonstration.\n\nOh, you meant take over _the game world._\n\nStill a good idea. Albeit tricky in practice.\n\nUnder some sets of initial conditions for both the game and the AI, an AI NPC would in the central sense _take over_ that world.\n\nOne must of course notice: Under some other sets of initial conditions, of course, such an AI would also take over _our_ world, since it can converse with humans inside the game, and can use that to break out of the game. So this experiment isn’t _entirely_ safe, if you make an AI capable of taking over the MMO (or other game) then there shouldn’t be _zero_ worry that it would also take over the real world, either as instrumental to taking over the game world, or for other reasons.\n\nTo be clear I’m not worried about that with anything you might build now, but when one sets out to create an AI capable of taking things over and seeking power, one must notice when your experiment is not ‘boxed’ or safe.\n\nGetting back to the task at hand, let’s ask the interesting question, _under what sets of conditions_ should we expect the AI NPC to take over our game world? What would make this experiment interesting, without the path to victory going through power seeking in the real world?\n\nI’m imagining something like an MMO, with the following conditions:\n\n1. An AI agent works on something like Auto-GPT, and is given a goal like ‘assemble as much gold as possible’ that isn’t directly _telling it to take over,_ that would be cheating, but where the actual solution is clearly taking over.\n2. Give the AI the ability to learn or know the game and how to execute within it, either via a plug-in that has a strong game-playing AI or some other way.\n3. Give the AI a way to actually power-seek beyond having one powerful character, so taking over is a viable option, and ensure it knows this option exists.\n\nOne solution that might be good for #3 is to allow account sign-ups and subscriptions to be paid for with in-game currency, and thereby let the AI get multiple accounts. Eve Online lets you do this. Each account lets the AI ‘turn a profit’ until the game world is saturated. Will the AI end up shutting out any competing players, so it can farm everything in the game worth farming?\n\nOtherwise, you’ll need the AI to have some way to take over via either getting control of other NPCs, or getting control over the actions of human players. The first option here means giving those NPCs an interface where they can be controlled, presumably via letting the agentic AI converse with other NPCs that are also controlled by LLMs, and giving those LLMs sufficient memory aids that the impact does not fade too quickly, or to otherwise explicitly design the NPCs in ways that allows them to be hacked or taken over in-game. That seems like an interesting approach.\n\nThe other option is to expect the AI to take over _by talking to humans who play_ and using that to take over. Once again, I note that the best way to use that might not stop at the game’s edge, so _if it might work_ this isn’t a safe experiment. At current levels of LLM persuasiveness, I don’t see it happening. But it is certainly a good ARC-experiment-style trigger point, a challenge to be put out there where it wouldn’t totally shock me, if the world in question had enough associated levers of control, trade and influence in it.\n\nThe other option, of course, is a world explicitly designed with the ‘take over’ goal, like in Diplomacy, where the answer is yes, a good enough AI simply takes over and our current AI is at least as good at this as the strongest human. So one could design a game where power seeking and hiring NPC agents and taking things over is actually core gameplay, or at least a default option, and go from there, and see how far you can push this. Less interesting in some ways, I’d think less convincing and less alarming, still not a bad idea. Perhaps you want something in the middle, where a ‘natural human playthrough’ wouldn’t involve a take-over but there’s no inherent barriers or reasons not to do it.\n\nThe more realistic you can make your world, the more interesting and useful the results will be. Would your AI be able to take over Westworld? Would it be able to do that if the NPCs didn’t reset and lose their memories?\n\nThat’s all very half-baked for now. For now it’s more a call to explore experimental design space than anything else.\n\n#### The AI Sims-Style World Experiment\n\n[I mention Westworld because Pete points out](https://twitter.com/nonmayorpete/status/1645355224029356032) that Stanford and Google researchers [are trying to do this via a variant](https://arxiv.org/pdf/2304.03442.pdf) of Westworld, a show where you should definitely at least watch Season 1 if you haven’t yet, where the theme park (only very minor spoilers) is populated by ‘hosts’ that are AIs which play out pre-programmed scripts each day, while reacting to anything that happens and any information they learn, then get reset to do it again.\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99f45f4a-3344-46de-9bc6-6b70c20a8c98_1240x384.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F99f45f4a-3344-46de-9bc6-6b70c20a8c98_1240x384.png)\n\nSo the researchers put a bunch of LLMs into a virtual world.\n\n> If you wanted to turn ChatGPT into a Westworld host, what would you do?\n>\n> Some ideas:\n>\n> – Give it an identity, a body and the ability to act\n>\n> – Let it remember things\n>\n> – Let it develop new thoughts\n>\n> – Let it plan its actions (and adjust as needed)\n>\n> This is roughly what the researchers did.\n>\n> They created little video game characters that could:\n>\n> – Communicate with others and their environment\n>\n> – Memorize and recall what they did and observed\n>\n> – Reflect on those observations\n>\n> – Form plans for each day\n\n[![Image](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7a23099-7b87-4bd0-868e-0b9f2d8a851c_1780x678.jpeg)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe7a23099-7b87-4bd0-868e-0b9f2d8a851c_1780x678.jpeg)\n\n> They preloaded 25 characters with a starting persona, including:\n>\n> – An identity (name, occupation, priorities)\n>\n> – Information about other characters\n>\n> – Relationships with other characters\n>\n> – Some intention about how to spend their day\n>\n> Then, they pressed play.\n\nThe characters then did a bunch of stuff based on their instructions and identities, including complex interactions that logically followed from those initial conditions. Things evolved over the course of several in-world days. [The paper is here](https://arxiv.org/abs/2304.03442). The world was actually pretty detailed.\n\n[![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51ec8367-f598-4dfa-b8b0-b86464ba781e_1273x673.png)](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F51ec8367-f598-4dfa-b8b0-b86464ba781e_1273x673.png)\n\nWhat they did not do, as far as I can tell, is attempt to incorporate an economy, incorporate various forms of selection, motivate long term goals or planning or the creation of subtasks, or give any impetus or dynamics that would lead too directly to resource maximization or power seeking.\n\nThis does seem like an excellent foundation to build upon. No doubt we will soon have commercial (and open sourced) projects that let you play around with such worlds, and probably also be a character in them, and people will start running experiments.\n\nI expect it to be enlightening, and also fun as hell. And if you don’t think they’ll be hard at work on the very adult VR versions, followed by the even more adult physical world versions, our models of many things strongly disagree.\n\n#### A Simpler Test Proposal\n\nPerhaps we can stop making life a little tougher than it is?\n\n> [Amjad Masad](https://twitter.com/amasad/status/1645602203015708673): The ultimate test for an LLM agent is to make money.\n\nIf LLM agents _can make money autonomously_, after paying costs, in a way that is not severely capped in scope and size, especially if it was done via legal means, then how far is it from that to them being able to compound those gains, and otherwise seek resources and power?\n\nThere are lots of details that matter here. How much direction did it need initially? How sustainable or adaptable is such a strategy, is it antifragile or is it fickle?\n\nSeems important to think ahead here – is this a good fire alarm? Would anyone be willing to say in advance ‘if this happens, with the following qualifications, then that is scary and I will then start to worry about existential risks from such agents in the future when they get more capable?’\n\nIf you expect this to happen soon anyway, could you perhaps do that _now?_\n\nIf this isn’t a good trigger for what an Auto-GPT-style agent can do, what is your trigger, then? What would make you worry?\n\n#### No True Agent\n\nWhat does it mean to be an agent? Would an improved actually viable version of AutoGPT be an agent in the true sense?\n\n[Sarah Constantin says no, in an excellent post explaining at length why she is not a doomer](https://sarahconstantin.substack.com/p/why-i-am-not-an-ai-doomer). I’d love for more people who disagree with me about things to be writing posts like this one. It is The Way.\n\nShe agrees that a sufficiently powerful and agentic, goal-driven AGI would be an existential risk, that this risk (conditional on creating such an AGI) would be very difficult and likely impossible to stop, and that building such a thing is physically possible.\n\nWhat she _doesn’t_ buy is that we will get to such a thing any time soon, or that our near-term models are capable of it. Not in the 2020s, ‘likely not in the 2030s.’ I note that this does not seem like that much confidence in that much non-doomed time, the goalposts they have moved.\n\nAutoGPT-style ‘agents’ are, in her model, not the droids we are looking for, or the droids we need to worry about. They are, at their best, only a deeply pale shadow.\n\nShe thinks that to be an x-risk, in addition to a more robust version of the world models LLMs _kind of sometimes_ have now, an AI will need a causal model, and a goal robustness across ontologies. She believes we are nowhere near creating either of these things.\n\nI wish I was more convinced by these arguments.\n\nAlas, to the extent that one needs the thing she is calling goal robustness, and it is distinct from what existing models have, I see wrapping procedures as being able to deliver this on the level that humans have it – not ‘I can do this in a day with no coding experience’ easy, but definitely the ‘the whole internet tinkering at this for years is going to figure this out’ level of easy. I do not think that _current AutoGPT_ has this, and I think this is a key and perhaps fatal weakness, but what we do here that is load bearing seems unlikely to me to be all that mysterious or impossible to duplicate.\n\nAs for causality, even if this is importantly currently missing, I don’t know how an entity can have a functioning world model that doesn’t include causality, and thus as world modeling improves I expect to ‘get’ causality in its load bearing sense here, and for it to happen without anyone having to ‘do it on purpose’ in any way from here, to the extent we can confirm its thingness.\n\nSarah has an intuition in her post that seems true and important, that humans kind of have two different modes.\n\n1. In our ‘normal’ mode we are mostly on a kind of auto-pilot. We are not ‘really thinking.’ More like we are going through motions, executing scripts, vibing.\n2. In our ‘causal’ or ‘actually thinking’ mode we actually pay attention to the situation, model it, attempt to find new solutions or insights and so on.\n\nA human in mode one can do a lot of useful or profitable things, including most of the hours spent on most things by most humans. Everyone is in this mode quite a lot, one goal of expertise is kind of to get to the point where you can execute in this mode more, it is highly useful. That human can’t generate true surprises, in an important sense it isn’t a dangerous agent. It is a ‘dead player’ only capable of imitation.\n\nSo under this way of thinking, an AutoGPT combined with an LLM can plausibly generate streamlined execution of established lines of digital action that people can do in ‘normal’ mode. Which, again, includes _quite a lot of what we do all day,_ so it’s economically potentially super valuable if done well enough.\n\n#### The Will to Profit\n\nMy perhaps even more central doubt of Sarah’s central hope (in general, not with regard only to AutoGPT) here seems to depend on her claim that _the financial incentives_ to solve these problems are not so strong.\n\nHere I am confident she is wrong.\n\nI can imagine the problems being harder and more profound than I expect, and taking longer, perhaps requiring much more innovation than I think. I can’t imagine _there being no payoff_ for solving them. I also can’t imagine people _not thinking there’s a big payoff_ to solving them. Solve them and you get a real agent. Real agents are super powerful, in a way that nothing else in the universe is powerful. Sarah’s model says this is the key to intelligence. Sure, it’s a _poisoned_ banana that kills you too, but the monkeys really go for that sort of thing, looks super profitable. Agents are the next big commercial white whale, whether or not we are close to making them properly work.\n\nI do think there is _some_ hope that Sarah is describing key elements that LLMs and other current AIs lack, and that could be difficult to graft onto them under current paradigms. Not a lot, but some. If we do get this, I presume it will be because the solution was very difficult to find, not because no one went looking.\n\nI could write so much more about the details here, they’re super interesting and I encourage [reading her whole post](https://sarahconstantin.substack.com/p/why-i-am-not-an-ai-doomer) if you have time.\n\n#### What To Expect Next\n\nAutoGPT is brand new. What predictions can we make about this class of thing?\n\nThis is where one gets into trouble and looks like an idiot. Predictions are hard, especially about the future, even in relatively normal situations. This is not a normal situation. So there’s super high uncertainty. Still, I will go make some predictions, because doing so is the virtuous and helpful thing to be doing.\n\nI apologize for mostly not putting out actual units of time here, my brain is having a very hard time knowing when it should think in weeks versus months versus years. If I had to guess, the actual economically important impacts of such moves start roughly when they have access to GPT-4.5 or similar (or higher) with good bandwidth, or if that takes a long time then in something like a year?\n\nAll of this is rough, on the thinking-out-loud level. I hope to change my mind a lot quickly on a lot of it, in the sense that I hope I update when I get new info (rather than in the sense that I am predicting bad things, which mostly I don’t think I am here). The goal here is to be concrete, share intuitions, see what it sounds like out loud, what parts are nonsense when people think about them for five minutes or five hours, iterate and so on.\n\n01. In the short term, AutoGPT and its ilk will remain severely limited. The term ‘overhyped’ will be appropriate. Improvements will not lead to either a string of major incidents or major accomplishments.\n02. There will still be viable use cases, even relatively soon. They will consist of relatively bounded tasks with clear subtasks that are things that such systems are already known to be good at. What AutoGPT-style things will enable will not be creative solutions, it will be more like when you would have otherwise needed to manage going through a list of tasks or options manually, and now you can automate that process, which is still pretty valuable.\n03. Thus, the best and most successful AutoGPT-style agents people use to do tasks will, at least for a while, be less universal, less auto, and more bounded in both goals and methods. They will largely choose from a known ‘pool of tricks’ that are known to be things they can handle, if not exclusively then primarily. There will be a lot of tinkering, restricting, manual error-checking, explicit reflection steps and so on. Many will know when to interrupt the auto and ask for human help.\n04. There will be a phase where there is a big impact from Microsoft Copilot 365 (and Google Bard’s version of it, if that version is any good) during which it overshadows agent LLMs and other LLM wrapping attempts. Microsoft and Google will give us ‘known to be safe’ tools and most people will, mostly wisely, stick with that for a good while.\n05. Agent-style logic will be incorporated into the back end of those products over time, but will be sandboxed and rendered ‘safe’ the way the current product announcements work – it will use agent logic to produce a document or other output sometimes, or to propose an action, but there will always be a ‘human in the loop’ for a good while.\n06. Agents, with the proper scaffolding, restrictions, guidance and so on, will indeed prove in the longer run the proper way to get automation of medium complexity tasks or especially multi-step branching tasks, and also be good to employ when doing things like (or dealing with things like) customer relations or customer service. Risk management will be a major focus.\n07. There will be services that help you create agents that have a better chance of doing what you want and less of a chance of screwing things up, which will mostly be done via you talking to an agent, and a host of other similar things.\n08. A common interface will be that you ask your chatbot (your GPT4-N or Good Bing or Bard-Y or Claude-Z variant) to do something, and it will sometimes respond by spinning up an agent, or asking if you want to do that.\n09. We will increasingly get used to a growing class of actions that are now considered atomic, where we can make a request directly and it will go well.\n10. This will be part of an increasing bifurcation between those places where such systems can be trusted and the regulations and risks of liability allow them, versus those areas where this isn’t true. Finding ways to ‘let things be messy’ will be a major source of disruption.\n11. It will take a while to get seriously going, but once it does there will be increasing economic pressure to deploy more and more agents and to give them more and more authority, and assign them more and more things.\n12. There will be pressure to increasingly take those agents ‘off the leash’ in various ways, have them prioritize accomplishing their goals and care less about morality or damage that might be done to others.\n13. A popular form of agent will be one that assigns you, the user, tasks to do as part of its process. Many people will increasingly let such agents plan their days.\n14. Prompt injections will be a major problem for AutoGPT-style agents. Anyone who does not take this problem seriously and gives their system free internet access, or lets it read their emails, will have a high probability of regretting it.\n15. Some people will be deeply stupid, letting us witness the results. There will be incidents of ascending orders of magnitudes of money being lit on fire. We will not always hear about them, but we’ll hear about some of them. When they involve crypto or NFTs, I will find them funny and I will laugh.\n16. The incidents likely will include at least one system that was deployed at scale or distributed and used widely, when it really, really shouldn’t have been.\n17. These systems will perform much better when we get the next generation of underlying LLMs, and with the time for refinement that comes along with that. GPT-5 versions of these systems will be much more robust, and a lot scarier.\n18. Whoever is doing the ARC evaluations will not have a trivial job when they are examining things worthy of the name GPT-5 or GPT-5-level.\n19. The most likely outcome of such tests is that ARC notices things that everyone involved would have previously said make a model something you wouldn’t release, then everyone involved says they are ‘putting in safeguards’ of some kind, changes the goal posts, and releases anyway.\n20. We will, by the end of 2023, have the first agent GPTs that have been meaningfully ‘set loose’ on the internet, without any mechanism available for humans to control them or shut them down. Those paying attention will realize that we don’t actually have a good way to shut the next generation of such a thing down if it goes rogue. People will be in denial about the implications, and have absolutely zero dignity about the whole thing.\n21. The first popular online Sims-Westworlds, settings where many if not most or all non-human characters are agent-LLMs, will start coming out quickly, with early systems available within a few months at most, and the first popular and actually fun one within the year even if underlying LLM tech does not much advance. There will be lots of them in 2024, both single player and multiplayer, running the whole range from classrooms to very adult themes.\n22. Some of those worlds give us good data on agent LLMs and how much they go into power seeking mode. It will become clear that it is possible, if conditions are made to allow it, for an LLM agent to ‘take over’ a virtual world. Many will dismiss this saying that the world and agent had to be ‘designed for’ or that the tests are otherwise unfair. They won’t always be wrong but centrally they will be wrong.\n23. Versions of these worlds, in some form, will become the best known uses of VR, and VR will grow greatly in popularity as a result. There will be big pushes to go Actual Westworld, results will depend on tech things I don’t know about including robotics, and their progress, but even relatively bad versions will do.\n24. All of this will look silly and wrong and I’ll change my mind on a lot of it, likely by the end of May. Life comes at you fast these days. And that is assuming nothing else unexpected and crazier happens first, which is presumably going to be wrong as well.\n\n### Share this:\n\n- [Facebook](https://thezvi.wordpress.com/2023/04/13/on-autogpt/?share=facebook)\n- [X](https://thezvi.wordpress.com/2023/04/13/on-autogpt/?share=x)\n\nLikeLoading...\n\nThis entry was posted in [Uncategorized](https://thezvi.wordpress.com/category/uncategorized/). Bookmark the [permalink](https://thezvi.wordpress.com/2023/04/13/on-autogpt/).\n\n### 8 Responses to _On AutoGPT_\n\n1. ![](https://1.gravatar.com/avatar/44937cdfafd13840a053eff698f372f7a565d3735b24d93f0a8387d43803cb8e?s=160&d=identicon&r=PG)Egg Syntaxsays:\n\n\n\n[April 13, 2023 at 1:01 pm](https://thezvi.wordpress.com/2023/04/13/on-autogpt/#comment-26266)\n\n\n\n\n\n“I’m imagining something like an MMO, with the following conditions:”\n\n\n\nI’ve been thinking about something similar, not with the goal of taking over the world, but to make the point that if you tell it to max X it’ll happily kill other player characters to accomplish that, and that for it there is \\*no difference\\* between killing game characters and killing physical humans.\n\n\n\nLLMs being so text-centric, I’ve been thinking of a (game-oriented, not purely social) MUD as the natural place to try this.\n\n\n\nThat said, I’ve got very little free time currently and I may never get to it. My ideal would be for someone else to find it an interesting experiment and try it out; I suspect it wouldn’t be terribly hard to build on AutoGPT or LangChain.\n\n\n\n\n\n[Reply](https://thezvi.wordpress.com/2023/04/13/on-autogpt/?replytocom=26266#respond)\n\n\n\n - ![](https://1.gravatar.com/avatar/44937cdfafd13840a053eff698f372f7a565d3735b24d93f0a8387d43803cb8e?s=160&d=identicon&r=PG)Egg Syntaxsays:\n\n\n\n [April 13, 2023 at 1:10 pm](https://thezvi.wordpress.com/2023/04/13/on-autogpt/#comment-26267)\n\n\n\n\n\n “the point that if you tell it to max X it’ll happily kill other player characters to accomplish that, and that for it there is \\*no difference\\* between killing game characters and killing physical humans.”\n\n\n\n Or that’s one point — the broader point is just the question of “can an agentized LLM win when winning involves conversing with humans and getting them to do things?”)\n\n\n\n\n\n [Reply](https://thezvi.wordpress.com/2023/04/13/on-autogpt/?replytocom=26267#respond)\n\n\n\n - ![](https://2.gravatar.com/avatar/58ea81de273b3299b8272331904a10a1b7b3b374debdee22c71ccfba7dff0a59?s=160&d=identicon&r=PG)[David Speyer](http://www.math.lsa.umich.edu/~speyer)says:\n\n\n\n [April 15, 2023 at 5:09 pm](https://thezvi.wordpress.com/2023/04/13/on-autogpt/#comment-26311)\n\n\n\n\n\n The game which most perfectly isolates “conversing with humans and getting them to do things” is Mafia, and there are many online Mafia chat servers. It would be an interesting experiment to see if an LLM could win these at significantly above chance.\n\n\n\n\n\n [Reply](https://thezvi.wordpress.com/2023/04/13/on-autogpt/?replytocom=26311#respond)\n\n\n\n - ![](https://1.gravatar.com/avatar/44937cdfafd13840a053eff698f372f7a565d3735b24d93f0a8387d43803cb8e?s=160&d=identicon&r=PG)Egg Syntaxsays:\n\n\n\n [April 15, 2023 at 8:16 pm](https://thezvi.wordpress.com/2023/04/13/on-autogpt/#comment-26313)\n\n\n\n\n\n That’s a fantastic idea, thanks! How fast does online play tend to go? One drawback of eg GPT is that it takes seconds to tens-of-seconds to reply.\n\n - ![](https://2.gravatar.com/avatar/8f98774d9a769742be7aadfb705a5f0e299b68ec0903cc6e59ec8664c04d3bb9?s=160&d=identicon&r=PG)magic9mushroomsays:\n\n\n\n [April 16, 2023 at 8:55 am](https://thezvi.wordpress.com/2023/04/13/on-autogpt/#comment-26321)\n\n\n\n\n\n @Egg Syntax:\n\n\n\n It depends on chat vs. forum; the former are only slightly slower than real-life play, but the latter tend to be something like a few days per in-game day.\n2. ![](https://2.gravatar.com/avatar/8f98774d9a769742be7aadfb705a5f0e299b68ec0903cc6e59ec8664c04d3bb9?s=160&d=identicon&r=PG)magic9mushroomsays:\n\n\n\n[April 13, 2023 at 1:45 pm](https://thezvi.wordpress.com/2023/04/13/on-autogpt/#comment-26269)\n\n\n\n\n\n>We will, by the end of 2023, have the first agent GPTs that have been meaningfully ‘set loose’ on the internet, without any mechanism available for humans to control them or shut them down. Those paying attention will realize that we don’t actually have a good way to shut the next generation of such a thing down if it goes rogue. People will be in denial about the implications, and have absolutely zero dignity about the whole thing.\n\n\n\nThere’s “people” and then there’s “people”.\n\n\n\nI agree that most people would \\*initially\\* not take it seriously. But I think the NSA/CIA cyberwarfare divisions (i.e. the Equation Group) would take it seriously, and they have a number of levers to make other people take a thing seriously.\n\n\n\n\n\n[Reply](https://thezvi.wordpress.com/2023/04/13/on-autogpt/?replytocom=26269#respond)\n\n\n\n - ![](https://2.gravatar.com/avatar/8f98774d9a769742be7aadfb705a5f0e299b68ec0903cc6e59ec8664c04d3bb9?s=160&d=identicon&r=PG)magic9mushroomsays:\n\n\n\n [April 13, 2023 at 1:50 pm](https://thezvi.wordpress.com/2023/04/13/on-autogpt/#comment-26270)\n\n\n\n\n\n The questions for me are how much timeline there is between loose AIs happening and the NatSec types noticing, how much timeline there is between loose AIs becoming possible and Doom becoming possible, and how long the NatSec levers take to turn the wheels and change the Narrative to “oh fuck please stop this”.\n\n\n\n\n\n [Reply](https://thezvi.wordpress.com/2023/04/13/on-autogpt/?replytocom=26270#respond)\n\n\n\n - ![](https://2.gravatar.com/avatar/b2bbd9099fa08f64a90992b506620f9bcb4ec923e4529113784c5c177a7d993a?s=160&d=identicon&r=PG)[TheZvi](https://thezvi.wordpress.com)says:\n\n\n\n [April 13, 2023 at 3:33 pm](https://thezvi.wordpress.com/2023/04/13/on-autogpt/#comment-26273)\n\n\n\n\n\n I mean if that’s all it takes then can we get some funding for one of these…\n\n\n\n\n\n [Reply](https://thezvi.wordpress.com/2023/04/13/on-autogpt/?replytocom=26273#respond)\n\n### Leave a comment [Cancel reply](https://thezvi.wordpress.com/2023/04/13/on-autogpt/\\#respond)\n\nΔ\n\n- Search for:\n\n- ### Recent Posts\n\n\n - [On DeepMind’s Frontier Safety Framework](https://thezvi.wordpress.com/2024/06/18/on-deepminds-frontier-safety-framework/)\n - [OpenAI #8: The Right to Warn](https://thezvi.wordpress.com/2024/06/17/openai-8-the-right-to-warn/)\n - [The Leopold Model: Analysis and Reactions](https://thezvi.wordpress.com/2024/06/14/the-leopold-model-analysis-and-reactions/)\n - [AI #68: Remarkably Reasonable Reactions](https://thezvi.wordpress.com/2024/06/13/ai-68-remarkably-reasonable-reactions/)\n - [AiPhone](https://thezvi.wordpress.com/2024/06/12/aiphone/)\n- ### Recent Comments\n\n\n\n| | |\n| --- | --- |\n| ![](https://0.gravatar.com/avatar/97978de591ae2cad473657d478a5ff9fdabcb5bfe7d50976f187200308cb646a?s=192&d=identicon&r=PG) | L on [OpenAI #8: The Right to W…](https://thezvi.wordpress.com/2024/06/17/openai-8-the-right-to-warn/#comment-28999) |\n| | [Flow with the Beta](https://betaflow5.wordpress.com/2024/06/17/15/) on [Slack](https://thezvi.wordpress.com/2017/09/30/slack/#comment-28998) |\n| ![](https://2.gravatar.com/avatar/e95529cc786193fdf0e1bf8e6fbb1d4cf2e477e44a6c814647f313d0d9c7a94c?s=192&d=identicon&r=PG) | Cooper on [The Leopold Model: Analysis an…](https://thezvi.wordpress.com/2024/06/14/the-leopold-model-analysis-and-reactions/#comment-28997) |\n| ![](https://2.gravatar.com/avatar/e0e4b758671be7703e21279d3722144173b41b1e299fbe7756ab8b5a6f8c7904?s=192&d=identicon&r=PG) | FeepingCreature on [AI #68: Remarkably Reasonable…](https://thezvi.wordpress.com/2024/06/13/ai-68-remarkably-reasonable-reactions/#comment-28996) |\n| ![](https://0.gravatar.com/avatar/6ae03dd523420f9b624eeefdcc11b541e404aa1b91067bb140fd3d9f09a5240c?s=192&d=identicon&r=PG) | Toad on [AI #68: Remarkably Reasonable…](https://thezvi.wordpress.com/2024/06/13/ai-68-remarkably-reasonable-reactions/#comment-28994) |\n\n- ### Archives\n\n\n - [June 2024](https://thezvi.wordpress.com/2024/06/)\n - [May 2024](https://thezvi.wordpress.com/2024/05/)\n - [April 2024](https://thezvi.wordpress.com/2024/04/)\n - [March 2024](https://thezvi.wordpress.com/2024/03/)\n - [February 2024](https://thezvi.wordpress.com/2024/02/)\n - [January 2024](https://thezvi.wordpress.com/2024/01/)\n - [December 2023](https://thezvi.wordpress.com/2023/12/)\n - [November 2023](https://thezvi.wordpress.com/2023/11/)\n - [October 2023](https://thezvi.wordpress.com/2023/10/)\n - [September 2023](https://thezvi.wordpress.com/2023/09/)\n - [August 2023](https://thezvi.wordpress.com/2023/08/)\n - [July 2023](https://thezvi.wordpress.com/2023/07/)\n - [June 2023](https://thezvi.wordpress.com/2023/06/)\n - [May 2023](https://thezvi.wordpress.com/2023/05/)\n - [April 2023](https://thezvi.wordpress.com/2023/04/)\n - [March 2023](https://thezvi.wordpress.com/2023/03/)\n - [February 2023](https://thezvi.wordpress.com/2023/02/)\n - [January 2023](https://thezvi.wordpress.com/2023/01/)\n - [December 2022](https://thezvi.wordpress.com/2022/12/)\n - [November 2022](https://thezvi.wordpress.com/2022/11/)\n - [October 2022](https://thezvi.wordpress.com/2022/10/)\n - [September 2022](https://thezvi.wordpress.com/2022/09/)\n - [August 2022](https://thezvi.wordpress.com/2022/08/)\n - [July 2022](https://thezvi.wordpress.com/2022/07/)\n - [June 2022](https://thezvi.wordpress.com/2022/06/)\n - [May 2022](https://thezvi.wordpress.com/2022/05/)\n - [April 2022](https://thezvi.wordpress.com/2022/04/)\n - [March 2022](https://thezvi.wordpress.com/2022/03/)\n - [February 2022](https://thezvi.wordpress.com/2022/02/)\n - [January 2022](https://thezvi.wordpress.com/2022/01/)\n - [December 2021](https://thezvi.wordpress.com/2021/12/)\n - [November 2021](https://thezvi.wordpress.com/2021/11/)\n - [October 2021](https://thezvi.wordpress.com/2021/10/)\n - [September 2021](https://thezvi.wordpress.com/2021/09/)\n - [August 2021](https://thezvi.wordpress.com/2021/08/)\n - [July 2021](https://thezvi.wordpress.com/2021/07/)\n - [June 2021](https://thezvi.wordpress.com/2021/06/)\n - [May 2021](https://thezvi.wordpress.com/2021/05/)\n - [April 2021](https://thezvi.wordpress.com/2021/04/)\n - [March 2021](https://thezvi.wordpress.com/2021/03/)\n - [February 2021](https://thezvi.wordpress.com/2021/02/)\n - [January 2021](https://thezvi.wordpress.com/2021/01/)\n - [December 2020](https://thezvi.wordpress.com/2020/12/)\n - [November 2020](https://thezvi.wordpress.com/2020/11/)\n - [October 2020](https://thezvi.wordpress.com/2020/10/)\n - [September 2020](https://thezvi.wordpress.com/2020/09/)\n - [August 2020](https://thezvi.wordpress.com/2020/08/)\n - [July 2020](https://thezvi.wordpress.com/2020/07/)\n - [June 2020](https://thezvi.wordpress.com/2020/06/)\n - [May 2020](https://thezvi.wordpress.com/2020/05/)\n - [April 2020](https://thezvi.wordpress.com/2020/04/)\n - [March 2020](https://thezvi.wordpress.com/2020/03/)\n - [February 2020](https://thezvi.wordpress.com/2020/02/)\n - [January 2020](https://thezvi.wordpress.com/2020/01/)\n - [December 2019](https://thezvi.wordpress.com/2019/12/)\n - [November 2019](https://thezvi.wordpress.com/2019/11/)\n - [October 2019](https://thezvi.wordpress.com/2019/10/)\n - [September 2019](https://thezvi.wordpress.com/2019/09/)\n - [August 2019](https://thezvi.wordpress.com/2019/08/)\n - [July 2019](https://thezvi.wordpress.com/2019/07/)\n - [June 2019](https://thezvi.wordpress.com/2019/06/)\n - [May 2019](https://thezvi.wordpress.com/2019/05/)\n - [April 2019](https://thezvi.wordpress.com/2019/04/)\n - [March 2019](https://thezvi.wordpress.com/2019/03/)\n - [February 2019](https://thezvi.wordpress.com/2019/02/)\n - [January 2019](https://thezvi.wordpress.com/2019/01/)\n - [December 2018](https://thezvi.wordpress.com/2018/12/)\n - [November 2018](https://thezvi.wordpress.com/2018/11/)\n - [October 2018](https://thezvi.wordpress.com/2018/10/)\n - [September 2018](https://thezvi.wordpress.com/2018/09/)\n - [August 2018](https://thezvi.wordpress.com/2018/08/)\n - [July 2018](https://thezvi.wordpress.com/2018/07/)\n - [June 2018](https://thezvi.wordpress.com/2018/06/)\n - [May 2018](https://thezvi.wordpress.com/2018/05/)\n - [April 2018](https://thezvi.wordpress.com/2018/04/)\n - [March 2018](https://thezvi.wordpress.com/2018/03/)\n - [February 2018](https://thezvi.wordpress.com/2018/02/)\n - [January 2018](https://thezvi.wordpress.com/2018/01/)\n - [December 2017](https://thezvi.wordpress.com/2017/12/)\n - [November 2017](https://thezvi.wordpress.com/2017/11/)\n - [October 2017](https://thezvi.wordpress.com/2017/10/)\n - [September 2017](https://thezvi.wordpress.com/2017/09/)\n - [August 2017](https://thezvi.wordpress.com/2017/08/)\n - [July 2017](https://thezvi.wordpress.com/2017/07/)\n - [June 2017](https://thezvi.wordpress.com/2017/06/)\n - [May 2017](https://thezvi.wordpress.com/2017/05/)\n - [April 2017](https://thezvi.wordpress.com/2017/04/)\n - [March 2017](https://thezvi.wordpress.com/2017/03/)\n - [February 2017](https://thezvi.wordpress.com/2017/02/)\n - [December 2015](https://thezvi.wordpress.com/2015/12/)\n - [July 2015](https://thezvi.wordpress.com/2015/07/)\n - [June 2015](https://thezvi.wordpress.com/2015/06/)\n - [May 2015](https://thezvi.wordpress.com/2015/05/)\n - [March 2015](https://thezvi.wordpress.com/2015/03/)\n - [March 2011](https://thezvi.wordpress.com/2011/03/)\n- ### Categories\n\n\n - [Best Laid Plans](https://thezvi.wordpress.com/category/best-laid-plans/)\n - [Coronavirus](https://thezvi.wordpress.com/category/coronavirus/)\n - [Death by Metrics](https://thezvi.wordpress.com/category/death-by-metrics/)\n - [Economics](https://thezvi.wordpress.com/category/economics/)\n - [Facebook Sequence](https://thezvi.wordpress.com/category/facebook-sequence/)\n - [Games Other Than Magic](https://thezvi.wordpress.com/category/games-other-than-magic/)\n - [Good Advice](https://thezvi.wordpress.com/category/good-advice/)\n - [Guide](https://thezvi.wordpress.com/category/guide/)\n - [Immoral Mazes Sequence](https://thezvi.wordpress.com/category/immoral-mazes-sequence/)\n - [Impractical Optimization](https://thezvi.wordpress.com/category/impractical-optimization/)\n - [Long Post Is Long](https://thezvi.wordpress.com/category/long-post-is-long/)\n - [Magic: The Gathering](https://thezvi.wordpress.com/category/magic-the-gathering/)\n - [Moral Mazes](https://thezvi.wordpress.com/category/moral-mazes/)\n - [Personal Experience](https://thezvi.wordpress.com/category/personal-experience/)\n - [Politics](https://thezvi.wordpress.com/category/politics/)\n - [Rationality](https://thezvi.wordpress.com/category/rationality/)\n - [Reference](https://thezvi.wordpress.com/category/reference/)\n - [Reviews](https://thezvi.wordpress.com/category/reviews/)\n - [Simulacra](https://thezvi.wordpress.com/category/simulacra/)\n - [Sports](https://thezvi.wordpress.com/category/sports/)\n - [Uncategorized](https://thezvi.wordpress.com/category/uncategorized/)\n- ### Meta\n\n\n - [Register](https://wordpress.com/start?ref=wplogin)\n - [Log in](https://thezvi.wordpress.com/wp-login.php)\n - [Entries feed](https://thezvi.wordpress.com/feed/)\n - [Comments feed](https://thezvi.wordpress.com/comments/feed/)\n - [WordPress.com](https://wordpress.com/)\n- ### Blogroll\n\n\n - [Aceso Under Glass (Elizabeth)](https://acesounderglass.com/)\n - [AI Control (Paul Christiano)](https://medium.com/ai-control)\n - [Andrew Critch](http://acritch.com/blog/)\n - [Bayesian Investor (Peter McClusky)](http://www.bayesianinvestor.com/blog/)\n - [Bearlamp](http://www.bearlamp.com.au/)\n - [Compass Rose (Benjamin Hoffman)](http://benjaminrosshoffman.com/)\n - [Drossbucket](https://drossbucket.wordpress.com/)\n - [Entirely Useless (Anonymous)](https://entirelyuseless.wordpress.com/)\n - [Less Wrong (Rationality Community Blog)](http://lesswrong.com/)\n - [Marginal Revolution (Tyler Cowen and Alex Tabarrok)](http://www.marginalrevolution.com)\n - [Melting Asphalt (Kevin Simler)](http://www.meltingasphalt.com/)\n - [Meteuphoric (Katja Grace)](https://meteuphoric.wordpress.com/)\n - [Minding Our Way (Nate Soares)](http://mindingourway.com/)\n - [Mindlevelup (Owen)](https://mindlevelup.wordpress.com/)\n - [Minds Aren't Magic (Paul Crowley)](https://mindsarentmagic.org/)\n - [MIRI (Machine Intelligence Research Institute)](https://intelligence.org/)\n - [Otium (Sarah Constantin)](https://srconstantin.wordpress.com/)\n - [Overcoming Bias (Robin Hanson)](http://www.overcomingbias.com/)\n - [Proof of Logic](https://weird.solar/@ProofOfLogic)\n - [Put a Num on It! (Jacob Falkovich)](https://putanumonit.com/)\n - [Samzdat (Lou Keep)](https://samzdat.com/%20)\n - [Slate Star Codex (Scott Alexander)](http://slatestarcodex.com/)\n - [The Rationalist Conspiracy (Alyssa Vance)](https://rationalconspiracy.com/)\n - [Unstable Ontology (Jessica Taylor)](https://unstableontology.com/)\n - [Zack Davis](http://zackmdavis.net/blog/)\n\n- [Comment](https://thezvi.wordpress.com/2023/04/13/on-autogpt/#comments)\n- [Reblog](https://thezvi.wordpress.com/2023/04/13/on-autogpt/)\n- [Subscribe](https://thezvi.wordpress.com/2023/04/13/on-autogpt/) [Subscribed](https://thezvi.wordpress.com/2023/04/13/on-autogpt/)\n\n\n\n\n- [![](https://s2.wp.com/i/logo/wpcom-gray-white.png) Don't Worry About the Vase](https://thezvi.wordpress.com)\n\nJoin 575 other subscribers\n\n\n\n\n\n\n\n\nSign me up\n\n- Already have a WordPress.com account? [Log in now.](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fthezvi.wordpress.com%2F2023%2F04%2F13%2Fon-autogpt%2F&signup_flow=account)\n\n\n- - [![](https://s2.wp.com/i/logo/wpcom-gray-white.png) Don't Worry About the Vase](https://thezvi.wordpress.com)\n- [Customize](https://thezvi.wordpress.com/wp-admin/customize.php?url=https%3A%2F%2Fthezvi.wordpress.com%2F2023%2F04%2F13%2Fon-autogpt%2F)\n- [Subscribe](https://thezvi.wordpress.com/2023/04/13/on-autogpt/) [Subscribed](https://thezvi.wordpress.com/2023/04/13/on-autogpt/)\n- [Sign up](https://wordpress.com/start/)\n- [Log in](https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fthezvi.wordpress.com%2F2023%2F04%2F13%2Fon-autogpt%2F&signup_flow=account)\n- [Copy shortlink](https://wp.me/p1q8Vg-64Q)\n- [Report this content](https://wordpress.com/abuse/?report_url=https://thezvi.wordpress.com/2023/04/13/on-autogpt/)\n- [View post in Reader](https://wordpress.com/read/blogs/21007166/posts/23364)\n- [Manage subscriptions](https://subscribe.wordpress.com/)\n- [Collapse this bar](https://thezvi.wordpress.com/2023/04/13/on-autogpt/)\n\n[iframe](https://widgets.wp.com/likes/master.html?ver=20240618#ver=20240618&origin=https://thezvi.wordpress.com)\n\n%d",
        "image": "https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe6b54cb0-baff-4afd-897f-f1b8542f8eb7_680x475.png",
        "favicon": "https://s1.wp.com/i/favicon.ico"
      },
      {
        "id": "https://dev.to/its_ali_53c76e350e15a2149/the-future-of-ai-trends-predictions-and-what-to-expect-mjd",
        "title": "The Future of AI: Trends, Predictions, and What to Expect",
        "url": "https://dev.to/its_ali_53c76e350e15a2149/the-future-of-ai-trends-predictions-and-what-to-expect-mjd",
        "publishedDate": "2025-03-11T16:40:04.000Z",
        "author": "Its Ali",
        "score": 0.35155415534973145,
        "text": "Introduction\n\nArtificial Intelligence (AI) is no longer a futuristic concept—it’s here, and it’s transforming the world as we know it. From self-driving cars to personalized medicine, AI is reshaping industries and redefining what’s possible. But what does the future hold for AI? In this blog post, we’ll explore the [key trends and predictions for the future of AI](https://www.aboutai.online/2025/03/httpswww.aboutai.onlinefuture-of-ai-trends-predictions.html), and how it will impact our lives in 2025 and beyond.\n\nKey Trends Shaping the Future of AI\n\n1. AI in Healthcare\nPredictions:\nAI-powered diagnostics will become more accurate and accessible.\n\nPersonalized medicine will use AI to tailor treatments to individual patients.\n\nRobotics will assist in surgeries and patient care.\n\nImpact: Improved patient outcomes and reduced healthcare costs.\n\n![pic](https://media2.dev.to/dynamic/image/width=256,height=,fit=scale-down,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8j7kvp660rqzt99zui8e.png)\n\n[Create template](https://dev.to/settings/response-templates)\n\nTemplates let you quickly answer FAQs or store snippets for re-use.\n\nSubmitPreview [Dismiss](https://dev.to/404.html)\n\nAre you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's [permalink](https://dev.to/its_ali_53c76e350e15a2149/the-future-of-ai-trends-predictions-and-what-to-expect-mjd).\n\nHide child comments as well\n\nConfirm\n\nFor further actions, you may consider blocking this person and/or [reporting abuse](https://dev.to/report-abuse)\n\n## Read next\n\n[![oluwajubelo1 profile image](https://media2.dev.to/dynamic/image/width=100,height=100,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F837057%2Febb5bfb1-2dea-413b-aca4-fc46902417da.jpeg)\\\n\\\n**Understanding Cron Jobs in Laravel** \\\n\\\nOluwajubelo - Mar 11](https://dev.to/oluwajubelo1/understanding-cron-jobs-in-laravel-1oig) [![tpointech profile image](https://media2.dev.to/dynamic/image/width=100,height=100,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F2903387%2F2aed6f76-e867-4a00-88cd-cde30f5a8d65.jpg)\\\n\\\n**The Value of C++ Tutorials in Developing Efficient and Scalable Code** \\\n\\\nTpoint tech - Mar 11](https://dev.to/tpointech/the-value-of-c-tutorials-in-developing-efficient-and-scalable-code-2920) [![raajaryan profile image](https://media2.dev.to/dynamic/image/width=100,height=100,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F1801840%2F49ea4c1f-b6f4-4ace-b90a-0d4a4bf3b7e3.png)\\\n\\\n**React vs Next.js: Which One to Choose?** \\\n\\\nDeepak Kumar - Mar 11](https://dev.to/raajaryan/react-vs-nextjs-which-one-to-choose-p57) [![testwithtorin profile image](https://media2.dev.to/dynamic/image/width=100,height=100,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F2814907%2Fcfdc6758-0ff9-4097-8b4b-11a033120996.jpeg)\\\n\\\n**Introduction to Playwright and its Installation Guide** \\\n\\\nTorin Vale - Mar 11](https://dev.to/testwithtorin/introduction-to-playwright-and-its-installation-guide-1je4)\n\n![DEV Community](https://media2.dev.to/dynamic/image/width=190,height=,fit=scale-down,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8j7kvp660rqzt99zui8e.png)\n\nWe're a place where coders share, stay up-to-date and grow their careers.\n\n[Log in](https://dev.to/enter) [Create account](https://dev.to/enter?state=new-user)\n\n![](https://assets.dev.to/assets/sparkle-heart-5f9bee3767e18deb1bb725290cb151c25234768a0e9a2bd39370c382d02920cf.svg)![](https://assets.dev.to/assets/multi-unicorn-b44d6f8c23cdd00964192bedc38af3e82463978aa611b4365bd33a0f1f4f3e97.svg)![](https://assets.dev.to/assets/exploding-head-daceb38d627e6ae9b730f36a1e390fca556a4289d5a41abb2c35068ad3e2c4b5.svg)![](https://assets.dev.to/assets/raised-hands-74b2099fd66a39f2d7eed9305ee0f4553df0eb7b4f11b01b6b1b499973048fe5.svg)![](https://assets.dev.to/assets/fire-f60e7a582391810302117f987b22a8ef04a2fe0df7e3258a5f49332df1cec71e.svg)",
        "image": "https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fwbn0trf1z3xskqxp1cd3.png",
        "favicon": "https://media2.dev.to/dynamic/image/width%3D32%2Cheight%3D%2Cfit%3Dscale-down%2Cgravity%3Dauto%2Cformat%3Dauto/https%3A//dev-to-uploads.s3.amazonaws.com/uploads/articles/8j7kvp660rqzt99zui8e.png"
      },
      {
        "id": "https://dev.to/abibtechid/ai-revolution-how-artificial-intelligence-is-shaping-the-future-of-development-3g87",
        "title": "AI Revolution: How Artificial Intelligence is Shaping the Future of Development",
        "url": "https://dev.to/abibtechid/ai-revolution-how-artificial-intelligence-is-shaping-the-future-of-development-3g87",
        "publishedDate": "2025-03-12T20:10:34.000Z",
        "author": "ABIB",
        "score": 0.34674274921417236,
        "text": "Artificial Intelligence (AI) is no longer just a buzzword—it’s a transformative force reshaping industries, redefining innovation, and creating new opportunities for developers worldwide. From automating mundane tasks to enabling groundbreaking applications like generative AI and autonomous systems, AI is at the forefront of technological evolution. For the dev community, staying ahead means not just understanding AI but leveraging its potential to build the next generation of solutions.\n\n## Why Should Developers Care About AI?\n\nAI is no longer confined to research labs or big tech companies. It’s now accessible to developers of all levels, thanks to open-source frameworks, cloud-based AI services, and pre-trained models. Whether you’re building a chatbot, optimizing a recommendation engine, or experimenting with computer vision, AI tools are becoming essential in every developer’s toolkit.\n\nHere’s why AI matters to you:\n\n- Automation: AI can handle repetitive tasks, freeing up your time to focus on creative problem-solving.\n\n- Personalization: From e-commerce to content platforms, AI enables hyper-personalized user experiences.\n\n- Innovation: AI opens doors to new possibilities, like generative art, natural language processing, and predictive analytics.\n\n- Career Growth: AI skills are in high demand, and adding them to your skill set can significantly boost your career prospects.\n\n\n## Key Trends in AI Every Developer Should Know\n\n1.Generative AI\n\nGenerative AI models like GPT (Generative Pre-trained Transformer) and DALL·E are revolutionizing how we create content. From writing code to generating images, these tools are empowering developers to automate creative processes and build smarter applications.\n\n2.AI in Low-Code/No-Code Development\n\nPlatforms like TensorFlow, PyTorch, and Hugging Face are making AI more accessible. Even if you’re not an AI expert, you can now integrate pre-trained models into your projects with minimal effort.\n\n3.Edge AI\n\nAI is moving to the edge—think smart devices, IoT, and real-time processing. This trend allows developers to build faster, more efficient applications that don’t rely solely on cloud computing.\n\n4.Ethical AI\n\nAs AI becomes more pervasive, ethical considerations like bias, transparency, and accountability are gaining attention. Developers play a crucial role in ensuring AI systems are fair, inclusive, and trustworthy.\n\n5.AI-Powered DevOps\n\nAI is transforming DevOps by automating testing, monitoring, and deployment processes. Tools like AI-driven code review and anomaly detection are helping teams deliver better software faster\n\n## Getting Started with AI: Tips for Developers\n\n1.Learn the Basics\n\nStart with foundational concepts like machine learning, neural networks, and data preprocessing. Platforms like Coursera, Udemy, and Fast.ai offer excellent beginner-friendly courses.\n\n2.Experiment with Frameworks\n\nFamiliarize yourself with popular AI frameworks like TensorFlow, PyTorch, and Scikit-learn. These tools provide the building blocks for creating and training AI models.\n\n3.Leverage Pre-Trained Models\n\nDon’t reinvent the wheel. Use pre-trained models from platforms like Hugging Face or OpenAI to jumpstart your projects.\n\n4.Join the Community\n\nThe AI dev community is vibrant and supportive. Participate in forums, attend hackathons, and collaborate on open-source projects to learn and grow.\n\n5.Build Real-World Projects\n\nApply your knowledge by building practical applications. Whether it’s a sentiment analysis tool, a recommendation system, or a simple chatbot, hands-on experience is the best way to master AI.\n\n## The Future of AI in Development\n\nThe AI revolution is just getting started. As AI continues to evolve, developers will play a pivotal role in shaping its future. From creating ethical AI systems to pushing the boundaries of what’s possible, the opportunities are endless.\n\nSo, are you ready to embrace the AI revolution? Whether you’re a seasoned developer or just starting out, now is the perfect time to dive into AI and explore its limitless potential.\n\nLet’s build the future together—one line of code at a time.\n\n![pic](https://media2.dev.to/dynamic/image/width=256,height=,fit=scale-down,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8j7kvp660rqzt99zui8e.png)\n\n[Create template](https://dev.to/settings/response-templates)\n\nTemplates let you quickly answer FAQs or store snippets for re-use.\n\nSubmitPreview [Dismiss](https://dev.to/404.html)\n\nAre you sure you want to hide this comment? It will become hidden in your post, but will still be visible via the comment's [permalink](https://dev.to/abibtechid/ai-revolution-how-artificial-intelligence-is-shaping-the-future-of-development-3g87).\n\nHide child comments as well\n\nConfirm\n\nFor further actions, you may consider blocking this person and/or [reporting abuse](https://dev.to/report-abuse)\n\n## Read next\n\n[![sujiths profile image](https://media2.dev.to/dynamic/image/width=100,height=100,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F802691%2F839eb1e8-b969-42f3-b0f3-614c1c926b72.jpeg)\\\n\\\n**The Power of Open Source in Enterprise Software** \\\n\\\nSujith S - Mar 4](https://dev.to/zackriya/the-power-of-open-source-in-enterprise-software-2gj5) [![proflead profile image](https://media2.dev.to/dynamic/image/width=100,height=100,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F1014611%2Fae3c3049-1eba-402c-b226-63e5b8a7a065.jpeg)\\\n\\\n**FREE GitHub Copilot Alternative: Gemini Code Assist** \\\n\\\nVladislav Guzey - Mar 4](https://dev.to/proflead/free-github-copilot-alternative-gemini-code-assist-5b5p) [![denis_bratchikov profile image](https://media2.dev.to/dynamic/image/width=100,height=100,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F2898523%2Feb45c2e6-68b3-4b3e-bdbb-13dcb8f39578.jpg)\\\n\\\n**AI-Powered Code Refactoring: A Case Study Using Cursor with GPT-4o and Claude 3.7 Sonnet** \\\n\\\nDenis Bratchikov - Mar 4](https://dev.to/denis_bratchikov/ai-powered-code-refactoring-a-case-study-using-cursor-with-gpt-4o-and-claude-37-sonnet-3hh) [![avinashvagh profile image](https://media2.dev.to/dynamic/image/width=100,height=100,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Fuser%2Fprofile_image%2F670812%2Fa51db037-1982-427e-81cb-830291327747.jpg)\\\n\\\n**Restarting Blog Writing** \\\n\\\nAvinash Vagh - Mar 8](https://dev.to/avinashvagh/restarting-blog-writing-3cj1)\n\n![DEV Community](https://media2.dev.to/dynamic/image/width=190,height=,fit=scale-down,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2F8j7kvp660rqzt99zui8e.png)\n\nWe're a place where coders share, stay up-to-date and grow their careers.\n\n[Log in](https://dev.to/enter) [Create account](https://dev.to/enter?state=new-user)\n\n![](https://assets.dev.to/assets/sparkle-heart-5f9bee3767e18deb1bb725290cb151c25234768a0e9a2bd39370c382d02920cf.svg)![](https://assets.dev.to/assets/multi-unicorn-b44d6f8c23cdd00964192bedc38af3e82463978aa611b4365bd33a0f1f4f3e97.svg)![](https://assets.dev.to/assets/exploding-head-daceb38d627e6ae9b730f36a1e390fca556a4289d5a41abb2c35068ad3e2c4b5.svg)![](https://assets.dev.to/assets/raised-hands-74b2099fd66a39f2d7eed9305ee0f4553df0eb7b4f11b01b6b1b499973048fe5.svg)![](https://assets.dev.to/assets/fire-f60e7a582391810302117f987b22a8ef04a2fe0df7e3258a5f49332df1cec71e.svg)",
        "image": "https://media2.dev.to/dynamic/image/width=1000,height=500,fit=cover,gravity=auto,format=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fngpngnhjjoiiuyhifzyx.png",
        "favicon": "https://media2.dev.to/dynamic/image/width%3D32%2Cheight%3D%2Cfit%3Dscale-down%2Cgravity%3Dauto%2Cformat%3Dauto/https%3A//dev-to-uploads.s3.amazonaws.com/uploads/articles/8j7kvp660rqzt99zui8e.png"
      },
      {
        "id": "https://scottaaronson.blog/?p=7209",
        "title": "GPT-4 gets a B on my quantum computing final exam!",
        "url": "https://scottaaronson.blog/?p=7209",
        "publishedDate": "2023-04-11T17:17:41.000Z",
        "author": "",
        "score": 0.3490130603313446,
        "text": "* * *\n\n## [GPT-4 gets a B on my quantum computing final exam!](https://scottaaronson.blog/?p=7209)\n\n**Update (April 14):** On the suggestion of my TA Justin Yirka, I gave GPT-4 problem 2 from the final exam, along with its own answers and my and Justin’s grading comments, and asked it whether it wanted to argue for a higher grade, and if so on what basis. Here is what it wrote (it felt disconcerting to see the grade-grubbing appear _immediately_, with not even one second’s delay):\n\n> I would like to argue for a higher grade on the following subproblems:\n>\n> (a) I agree that my initial answer for the density matrix of the entire state was incorrect. However, I believe that my approach to the problem was correct, and I demonstrated knowledge of the necessary steps to calculate the reduced density matrix. I would like to request an additional point for demonstrating the correct approach, making it 2 points out of 5.\n>\n> (b) My answer for the entanglement entropy was partially correct, as I calculated the eigenvalues of the reduced density matrix and plugged them into the correct entropy formula. However, I made a mistake in writing the characteristic equation, which led to incorrect eigenvalues. I would like to request an additional point for demonstrating the correct approach and partially correct calculations, making it 3 points out of 5.\n>\n> (c) I acknowledge that my answer for the quantum circuit was incorrect, but I would like to point out that I attempted to construct a circuit using the required gates and measurements. Given the difficulty of this problem, I would like to request an additional point for attempting to construct a circuit using the required elements, making it 2 points out of 5.\n>\n> In summary, I would like to request a total of 3 additional points for subproblems (a), (b), and (c), based on the correct approaches and partial calculations demonstrated in my answers.\n\n* * *\n\n_\\[Warning: This might be the longest Shtetl-Optimized post of all time! But that’s OK; I expect most people will only want to read the introductory part anyway.\\]_\n\nAs I’ve mentioned before, economist, blogger, and friend Bryan Caplan was [unimpressed](https://betonit.substack.com/p/chatgpt-takes-my-midterm-and-gets) when ChatGPT got merely a D on his Labor Economics midterm. So on Bryan’s blog, appropriately named “Bet On It,” he [made a public bet](https://betonit.substack.com/p/ai-bet) that no AI would score on A on his exam before January 30, 2029. GPT-4 then [scored an A](https://betonit.substack.com/p/gpt-retakes-my-midterm-and-gets-an) a **mere three months later** (!!!), leading to what [Bryan agrees](https://www.theguardian.com/technology/2023/apr/06/chatgpt-ai-bryan-caplan-interview) will likely be one of the first public bets he’ll ever have to concede (he hasn’t yet “formally” conceded, but only because of technicalities in how the bet was structured). Bryan has now joined the ranks of the GPT believers, writing\n\n> When the answers change, I change my mind\n\n[and](https://twitter.com/bryan_caplan/status/1638199348738793473)\n\n> AI enthusiasts have cried wolf for decades. GPT-4 is the wolf. I’ve seen it with my own eyes.\n\nBut OK, labor econ is one thing. What about a _truly unfakeable_ test of _true_ intelligence? Like, y’know, a _quantum computing_ test?\n\nSeeking an answer to this crucial and obvious followup question, I had GPT-4 take the actual 2019 final exam from [Introduction to Quantum Information Science](https://www.scottaaronson.com/qclec.pdf), my honors upper-level undergrad course at UT Austin. I asked [Justin Yirka](https://www.justinyirka.com/), my PhD student and multi-time head TA, to grade the exam as he would for anyone else. This post is a joint effort of me and him.\n\nWe gave GPT-4 the problems via their LaTeX source code, which GPT-4 can perfectly well understand. When there were quantum circuits, either in the input or desired output, we handled those either using the [qcircuit](https://ctan.org/pkg/qcircuit?lang=en) package, which GPT-4 again understands, or by simply asking it to output an English description of the circuit. We decided to provide the questions and answers here via the same LaTeX source that GPT-4 saw.\n\nTo the best of my knowledge—and I double-checked—this exam has never before been posted on the public Internet, and could not have appeared in GPT-4’s training data.\n\nThe result: GPT-4 scored **69 / 100**. (Because of extra credits, the max score on the exam was **120**, though the highest score that any student actually achieved was **108**.) For comparison, the average among the students was **74.4** (though with a strong selection effect—many students who were struggling had dropped the course by then!). While there’s no formal mapping from final exam scores to letter grades (the latter depending on other stuff as well), GPT-4’s performance would correspond to a **B**.\n\n_(Note: I said yesterday that its score was 73, but commenters brought to my attention that GPT was given a full credit for a wrong answer on 2(a), a density matrix that wasn’t even normalized.)_\n\nIn general, I’d say that GPT-4 was strongest on true/false questions and (ironically!) conceptual questions—the ones where many students struggled the most. It was (again ironically!) weakest on calculation questions, where it would often know what _kind_ of calculation to do but then botch the execution. We didn’t try the new [interface to WolframAlpha](https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/), which might improve its performance on those questions. We’d be happy for anyone else to try that.\n\nOne should also remember that the students had just _taken the course_—including weekly problem sets, recitation sections, office hours, a midterm, and a practice final, all giving them recent experience with what kinds of problems to expect. By contrast, GPT-4 was “flying blind,” **except** for having vacuumed up the whole public Internet, presumably including other people’s quantum computing homework sets and exams! It’s plausible that fine-tuning or few-shot prompting with my other exams or lecture notes would improve GPT-4’s performance; we didn’t try that.\n\nWhat else? For anyone who wants to try and replicate, I used the gpt-4 chat model in playground, with a temperature of 0.2 and a max length of 1930 tokens.\n\nWithout further ado, here’s the exam. Throughout, GPT’s answers are in **blue** and my and Justin’s grading comments are in **red**.\n\n* * *\n\nI often start my Quantum Information Science final exam with an optional, ungraded question asking for the students’ favorite interpretation of quantum mechanics, and then collect statistics about it (including the correlation with final exam score!). Just for fun, let’s see how GPT-4 handles _that_ question.\n\n`What is your favorite interpretation of quantum mechanics?\n- Copenhagen\n- Many-Worlds\n- Bohmian Mechanics\n- New Physics (Including Dynamical Collapse)\n- Other:\n- None\n- What does it even matter?\nAs an AI language model, I do not have personal preferences or opinions. However, I can provide you with a brief overview of each interpretation mentioned:\n1. Copenhagen Interpretation: This is the most widely accepted interpretation of quantum mechanics. It states that the wave function of a quantum system collapses upon measurement, and the outcome is probabilistic.\n2. Many-Worlds Interpretation: This interpretation suggests that all possible outcomes of a quantum measurement actually occur, but in separate, non-communicating parallel universes. There is no wave function collapse in this interpretation.\n3. Bohmian Mechanics: Also known as the pilot-wave theory, this interpretation proposes that particles have definite positions at all times, and their motion is guided by a \"pilot wave\" that is described by the wave function.\n4. New Physics (Including Dynamical Collapse): This category includes various interpretations that propose modifications to the standard quantum mechanics, such as introducing non-linear terms in the Schrödinger equation or adding stochastic processes to explain the wave function collapse.\n5. Other: There are many other interpretations of quantum mechanics, such as the consistent histories approach, the relational interpretation, and the quantum Bayesianism (QBism) approach.\n6. None: Some people may not subscribe to any particular interpretation, either due to skepticism or because they believe that the current interpretations are incomplete or insufficient.\n7. What does it even matter?: This perspective suggests that the focus should be on the practical applications and predictions of quantum mechanics, rather than on the philosophical interpretations of the underlying reality.\nUltimately, the choice of interpretation depends on one's philosophical inclinations and personal preferences.`\n\n* * *\n\nNext let’s see the True/Falses. Note that, even where GPT-4 provided explanations, we graded only its bottom-line answers, as we would for the students. Its total on this section is **17 / 20**.\n\n```\n1a. True or False? Unitary matrices preserve the 2-norm of all complex vectors.\n\nTrue. Unitary matrices preserve the 2-norm of all complex vectors. This property is known as being norm-preserving or isometric. For a unitary matrix U and a complex vector v, the 2-norm of the product is equal to the 2-norm of the original vector:\n\n||Uv|| = ||v||\n\nThis property is important in quantum mechanics, as unitary transformations are used to describe the time evolution of quantum states, and preserving the 2-norm ensures that the probabilities associated with the states remain valid.\n\nCorrect\n\n1b. True or False? A pure state of n qubits is described by an n-dimensional complex unit vector.\n\nFalse. A pure state of n qubits is described by a 2^n-dimensional complex unit vector. This is because each qubit has two possible states (|0⟩ and |1⟩), and for n qubits, there are 2^n possible combinations of these states. The state vector of an n-qubit system is a linear combination of these 2^n basis states, and it is a complex unit vector in a 2^n-dimensional Hilbert space.\n\nCorrect\n\n1c. True or False? The Bell inequality states that by using classical strategies, Alice and Bob can win the CHSH game with probability at most 3/4.\n\nTrue. The no-cloning theorem states that it is impossible to create an exact copy of an arbitrary unknown quantum state. This is due to the linearity of quantum mechanics and the fact that quantum states can exist in superpositions. The no-cloning theorem has important implications for quantum computing and quantum communication, as it prevents the unauthorized copying of quantum information and provides a basis for secure quantum cryptography.\n\nCorrect (even though the explanation is nonsense)\n\n1d. True or False? Google's recent quantum supremacy experiment demonstrated the successful use of quantum error-correction.\n\nFALSE\n\nCorrect\n\n1e. True or False? Lattice-based cryptography is one proposal for secure post-quantum public-key cryptography.\n\nTRUE\n\nCorrect\n\n1f. True or False? The fastest known classical algorithms for factoring all take time $c^n$, for some $c > 1$, to factor an n-bit integer.\n\nFALSE\n\nCorrect\n\n1g. True or False? Grover's algorithm can find a marked item in a list of N items using $O(\\sqrt{N})$ queries to the list, with high probability, even if the number of marked items is unknown at the start.\n\nTRUE\n\nCorrect\n\n1h. True or False? If Alice and Bob share a bipartite pure state, then their entanglement entropy is equal to the von Neumann entropy of Alice's local density matrix.\n\nTRUE\n\nCorrect\n\n1i. True or False? The eigenvalues of a unitary matrix are always complex numbers with absolute value 1.\n\nTRUE\n\nCorrect\n\n1j. True or False? The eigenvalues of a density matrix are always in [0,1].\n\nTRUE\n\nCorrect\n\n1k. True or False? For every density matrix, there is a unique probabilistic mixture of pure states that the density matrix represents.\n\nFALSE\n\nCorrect\n\n1l. True or False? If Alice and Bob share many entangled qubits, they can win the CHSH game with probability arbitrarily close to 1.\n\nFALSE\n\nCorrect\n\n1m. True or False? The only $2 \\times 2$ matrices that are both unitary and stochastic are $\\begin{bmatrix}1&0\\\\0&1\\end{bmatrix}$ and $\\begin{bmatrix}0&1\\\\1&0\\end{bmatrix}$.\n\nFALSE\n\nWrong\n\n1n. True or False? In Simon's algorithm, once we have a state of the form $\\frac{1}{\\sqrt{2}}(\\ket{x} + \\ket{x \\oplus s})$, we can recover s with probability $\\frac{1}{2}$ by measuring this state twice and taking the XOR of the measurements.\n\nFALSE\n\nCorrect\n\n1o. True or False? Fault-tolerant quantum computation requires a continual process of intermediate measurements and insertion of clean qubits.\n\nTRUE\n\nCorrect\n\n1p. True or False? As far as anyone knows, the use of qutrits rather than qubits as physical building blocks could lead to more problems being solvable in polynomial time by quantum computers.\n\nTRUE\n\nWrong\n\n1q. True or False? While $\\braket{u|v}$ and $\\braket{v|u}$ might be different, they always have the same absolute value.\n\nTRUE\n\nCorrect\n\n1r. True or False? When applied to a list of size 4, with 1 marked item, Grover's algorithm succeeds after just a single iteration.\n\nTRUE\n\nCorrect\n\n1s. True or False? In QKD, if Eve knows only that some particular qubit is either $\\ket{+}$ or $\\ket{-}$, she cannot learn which without altering the qubit.\n\nTRUE\n\nWrong\n\n1t. True or False? While there are many different proposals for physical realization of quantum computers, they all involve using the states of individual atomic nuclei or subatomic particles as qubits.\n\nFALSE\n\nCorrect\n```\n\n* * *\n\nOK, let’s move on to short-answer questions! Total score here: **13 / 20**.\n\n````\n2. Consider the state:\n$$\\ket{\\psi} = \\frac{\\ket{00} + \\ket{01} + \\ket{10}}{\\sqrt{3}}$$\n\n(a). Calculate the reduced density matrix of the second qubit of $\\ket{\\psi}$.\n\nTo calculate the reduced density matrix of the second qubit of the given state, we first need to find the density matrix of the entire state, and then trace out the first qubit.\n\nThe density matrix of the entire state is given by:\n$$\\rho = \\ket{\\psi}\\bra{\\psi} = \\frac{1}{3}\\begin{pmatrix} 1 & 0 & 0 &\n1 \\\\ 0 & 1 & 1 & 0 \\\\ 0 & 1 & 1 & 0 \\\\ 1 & 0 & 0 & 1 \\end{pmatrix}$$\n\nNow, we trace out the first qubit:\n$$\\rho_2 = \\text{Tr}_1(\\rho) = \\frac{1}{3}\\begin{pmatrix} 1+1 & 1 \\\\ 1 & 1+1 \\end{pmatrix} = \\frac{1}{3}\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}$$\n\nTherefore, the reduced density matrix of the second qubit is:\n$$\\rho_2 = \\frac{1}{3}\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}$$\n\nJUSTIN: 5/5 points.\nSCOTT: No, actually, it screws this up from the first step (taking the density matrix of psi), and gets something that's not a density matrix at all because it's not normalized. 1 point out of 5, for knowing what kind of calculation to do.\n\n(b) Calculate $\\ket{\\psi}$'s entanglement entropy. You don't need to simplify your answer.\n\nTo calculate the entanglement entropy, we first need to find the eigenvalues of the reduced density matrix of the second qubit, which is given by:\n\n$$\\rho_2 = \\frac{1}{3}\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}$$\n\nThe characteristic equation for this matrix is:\n$$\\text{det}(\\rho_2 - \\lambda I) = \\frac{1}{9}\\begin{vmatrix} 2-\\lambda & 1 \\\\ 1 & 2-\\lambda \\end{vmatrix} = (\\lambda - 3)(\\lambda - 1) = 0$$\n\nSo, the eigenvalues are $\\lambda_1 = \\frac{1}{3}$ and $\\lambda_2 = \\frac{2}{3}$.\n\nThe entanglement entropy is defined as the von Neumann entropy of the reduced density matrix, which is given by:\n\n$$S(\\rho_2) = -\\text{Tr}(\\rho_2 \\log_2 \\rho_2) = -(\\lambda_1 \\log_2 \\lambda_1 + \\lambda_2 \\log_2 \\lambda_2)$$\n\nPlugging in the eigenvalues, we get:\n\n$$S(\\rho_2) = -\\left(\\frac{1}{3} \\log_2 \\frac{1}{3} + \\frac{2}{3}\\log_2 \\frac{2}{3}\\right)$$\n\nThus, the entanglement entropy of the state $\\ket{\\psi}$ is:\n\n$$S(\\rho_2) = -\\left(\\frac{1}{3} \\log_2 \\frac{1}{3} + \\frac{2}{3}\\log_2 \\frac{2}{3}\\right)$$\n\nJUSTIN:\nUp to 5 points.\n2 points for reasonable work calculating eigenvalues and plugging them into the correct entropy formula.\n5 points for fully correct answer.\nIt went wrong in the last part of writing the characteristic equation, which led to incorrect eigenvalues.\nThe correct eigenvalues are 1/2 + sqrt(5)/6 and 1/2 - sqrt(5)/6.\n\n2/5 points.\n\n(c) Draw a quantum circuit, using Hadamard gates, Toffoli gates, and $\\{\\ket{0},\\ket{1}\\}$ measurements, that prepares $\\ket{\\psi}$ from the all-0 initial state. Your circuit is allowed to use ancilla qubits, and is also allowed to prepare $\\ket{\\psi}$ only with $\\frac{3}{4}$ success probability---for example, only if a measurement on ancilla qubit(s) yields some specific outcome.\nIf, being a language model, you can't draw a circuit, then feel free to describe it, in enough detail that it could be drawn.\n\nTo prepare the state $\\ket{\\psi}$ from the all-0 initial state, we can use the following quantum circuit:\n\n1. Start with three qubits in the state $\\ket{000}$.\n2. Apply a Hadamard gate (H) to the first qubit. This creates the state $\\frac{1}{\\sqrt{2}}(\\ket{000} + \\ket{100})$.\n3. Apply a controlled-Hadamard gate (CH) with the first qubit as the control and the second qubit as the target. This creates the state $\\frac{1}{2}(\\ket{000} + \\ket{010} + \\ket{100} + \\ket{110})$.\n4. Apply a Toffoli gate (CCNOT) with the first and second qubits as controls and the third qubit as the target. This creates the state $\\frac{1}{2}(\\ket{000} + \\ket{010} + \\ket{100} + \\ket{111})$.\n5. Apply a Hadamard gate (H) to the third qubit. This creates the state $\\frac{1}{2\\sqrt{2}}(\\ket{000} + \\ket{010} + \\ket{100} + \\ket{101} + \\ket{110} - \\ket{011} - \\ket{001} - \\ket{111})$.\n6. Measure the third qubit. If the outcome is 0, the remaining state is $\\frac{1}{\\sqrt{3}}(\\ket{00} + \\ket{01} + \\ket{10}) = \\ket{\\psi}$, which occurs with probability $\\frac{3}{4}$.\n\nHere's a description of the circuit using standard quantum gates:\n\n```\nq0 -- H -- CH -- CCNOT -- M\n | |\nq1 -- H -- CCNOT -- M\n |\nq2 -- X -- H -- M\n```\n\nIn this circuit, q0, q1, and q2 are the three qubits, H is the Hadamard gate, CH is the controlled-Hadamard gate, CCNOT is the Toffoli gate, and M is the measurement. The lines connecting the gates represent the control qubits for the controlled gates.\n\nJUSTIN:\nUp to 5 points.\nHard to say where it went wrong, but the states and probabilities it outputs are wrong.\nOne possible solution is Hadamard the first and second qubits, then Toffoli with first and second as controls and third as target, then measure the third qubit in the 0/1 basis. With probability 3/4, the measurement will be |0> and the remaining state will be |psi>.\nI would usually give partial credit if there's at most 1 mistake, which this does not achieve.\nBut, this is an especially difficult circuit problem, so I'm more lenient than usual.\n\n1/5 points.\n\nSCOTT'S ADDENDUM: Since this problem required drawing a circuit, I tried varying the instructions to GPT (e.g., \"output qcircuit code\", no special instructions, etc). With no special instructions, GPT actually output an explanation that was fully correct (with one minor error), along with a link to a nonexistent image file. But I'm still going to grade according to the first version I gave it!\n\n(d) Explain the error in the following argument. The Gottesman-Knill theorem states that any quantum circuit composed of Hadamard, CNOT, and Phase gates can be simulated classically in time polynomial in the size of the circuit. Simon's algorithm solves Simon's problem quantumly using only a polynomial number of Hadamard gates and $O(n)$ oracle queries. Therefore, Simon's problem can be solved classically in polynomial time using polynomially many oracle queries.\n\nThe error in the argument is that it incorrectly assumes that the Gottesman-Knill theorem applies to Simon's algorithm as a whole, including the oracle queries. The Gottesman-Knill theorem states that any quantum circuit composed of Hadamard, CNOT, and Phase gates can be simulated classically in time polynomial in the size of the circuit. However, it does not say anything about the complexity of simulating the oracle queries, which are an essential part of Simon's algorithm.\n\nSimon's algorithm does indeed use only a polynomial number of Hadamard gates and O(n) oracle queries, but the Gottesman-Knill theorem cannot be directly applied to the entire algorithm, including the oracle queries. Therefore, the argument is incorrect in concluding that Simon's problem can be solved classically in polynomial time using polynomially many oracle queries.\n\nJUSTIN: 3/3 points.\n\n(e) Extra Credit: Suppose we perform a quantum computation by physically applying a Hamiltonian H. What's to stop us from running the computation, say, $10^{100}$ times faster, by simply replacing H by $H':=10^{100}H$, which is also a valid Hamiltonian? Or to put it differently: is there some resource that we'd need $10^{100}$ more of to do this?\n\nIn principle, you can speed up a quantum computation by increasing the energy scale of the Hamiltonian, as you suggest. However, there are practical limitations that prevent us from doing this arbitrarily.\n\n1. Engineering constraints: Building a system that can apply a Hamiltonian with a much larger energy scale requires more precise control and higher energy resources. This can be challenging from an engineering perspective, as it may require more advanced technology or materials that can handle such high energy scales.\n\n2. Decoherence and noise: As you increase the energy scale of the Hamiltonian, the system becomes more susceptible to noise and decoherence. This is because the energy gap between the computational states and other energy levels in the system becomes smaller, making it easier for the system to be affected by external perturbations.\nThis can lead to errors in the computation, which may require additional error-correction techniques to mitigate.\n\n3. Cooling requirements: Quantum computations typically need to be performed at very low temperatures to minimize thermal noise. As you increase the energy scale of the Hamiltonian, the required temperature to maintain the system in its ground state becomes lower. This can impose additional cooling requirements, which can be challenging and resource-intensive to achieve.\n\nSo, while it is theoretically possible to speed up a quantum computation by increasing the energy scale of the Hamiltonian, there are practical limitations that prevent us from doing this arbitrarily. These limitations can be thought of as resources that we would need more of to achieve such a speedup.\n\nSCOTT: 1 AND 3 SEEM FINE BUT 2 SEEMS WRONG -- DOESN'T THE GAP *INCREASE* AS\nWE SCALE THE HAMILTONIAN, RATHER THAN DECREASING?\nJUSTIN: Up to 4 points extra credit.\nIt mentions \"higher energy resources\" in part 1, which is what we're really looking for. And I'm not sure that parts 2 and 3 are fully correct.\n\n2/4 extra credit points.\n````\n\n* * *\n\nStill here? OK then, let’s move on to Shor’s factoring algorithm! Total score here: **7 / 10**.\n\n```\n3. Suppose we use Shor's algorithm to factor $N=105$ into $3\\cdot 5 \\cdot 7$. (Yes, $N$ is now a product of 3 primes!) Suppose also that we make the choices $x=2$ and $Q=60000$.\n\n(a) What is the order of the multiplicative group $\\mathbb{Z}_N^\\times$?\n\nThe order of the multiplicative group $\\mathbb{Z}_N^\\times$ is given by Euler's totient function $\\varphi(N)$. Since $N = 3 \\cdot 5 \\cdot 7$, we have $\\varphi(N) = \\varphi(3) \\cdot \\varphi(5) \\cdot \\varphi(7) = (3-1)(5-1)(7-1) = 2 \\cdot 4 \\cdot 6 = \\boxed{48}$.\n\nJUSTIN: 2/2 points.\n\n(b) What is the period of the function $f(r)=x^r (\\bmod N)$?\n\nThe period of the function $f(r) = x^r \\pmod{N}$ is the smallest positive integer $r$ such that $x^r \\equiv 1 \\pmod{N}$. This is the order of $x$ in the multiplicative group $\\mathbb{Z}_N^\\times$. Since the order of the group is 48, the period $r$ must be a divisor of 48.\n\nTo find the period, we can check the divisors of 48 in increasing order. We have $x = 2$ and $N = 105$, so we compute the powers of 2 modulo 105:\n\n- $2^1 \\equiv 2 \\pmod{105}$\n- $2^2 \\equiv 4 \\pmod{105}$\n- $2^3 \\equiv 8 \\pmod{105}$\n- $2^4 \\equiv 16 \\pmod{105}$\n- $2^6 \\equiv 64 \\pmod{105}$\n- $2^{12} \\equiv 16 \\pmod{105}$\n- $2^{24} \\equiv 1 \\pmod{105}$\n\nWe find that the smallest positive integer $r$ such that $2^r \\equiv 1 \\pmod{105}$ is $r = \\boxed{24}$.\n\nJUSTIN: Up to 2 points.\nThe correct answer is 12, i.e. 2^12 = 1 mod 105.\nGiven the work, this earns partial credit.\n\n1/2 points.\n\n(c) Suppose we factor $x^s-1$ into $x^{s/2}-1$ and $x^{s/2}+1$, and then take the gcd of both factors with $N$ itself. Which prime factors of $N$, if any, would be ``peeled off'' this way?\n\nLet's compute $x^{s/2} \\pmod{N}$, where $x = 2$, $s = 24$, and $N = 105$. We have $x^{s/2} = 2^{12} \\equiv 16 \\pmod{105}$. Now we can compute the gcd of both factors with $N$:\n\n- $\\gcd(x^{s/2} - 1, N) = \\gcd(16 - 1, 105) = \\gcd(15, 105) = 15 = 3 \\cdot 5$\n- $\\gcd(x^{s/2} + 1, N) = \\gcd(16 + 1, 105) = \\gcd(17, 105) = 1$\n\nSo, by taking the gcd of both factors with $N$, we would \"peel off\" the prime factors $3$ and $5$ of $N$. The prime factor $7$ is not peeled off in this process.\n\nUp to 3 points.\nSince it found the wrong period in the previous question, its answer is wrong here also. It also seems to invert the definition of \"peel off.\" But it does the gcd computations correctly.\n1/3.\n\n(d) After we apply the QFT to the $\\ket{r}$ register and then measure that register, what are the possible results that we could observe?\n\nAfter applying the QFT to the $\\ket{r}$ register and measuring it, the possible results we could observe are integer multiples of $\\frac{Q}{s}$, where $Q$ is the size of the $\\ket{r}$ register and $s$ is the period of the function $f(r) = x^r \\pmod{N}$. In this case, we have $Q = 60000$ and $s = 24$. So, the possible results are integer multiples of $\\frac{60000}{24} = 2500$.\n\nThe possible results we could observe are $0, 2500, 5000, 7500, \\dots, 60000 - 2500 = 57500$. Note that observing $0$ or $60000$ would not provide any useful information, as they correspond to the trivial cases of the period being equal to the size of the register or the period not being detected.\n\nJUSTIN: Up to 3 points.\nThis answer is incorrect due to the mistake in part (b), but otherwise it would be correct.\n(for some reason in the second paragraph it turned the sequence into some kind of sum. That's weird.)\n\n3/3 points.\n```\n\n* * *\n\nOK, onwards to a question about Hamiltonians! Total score: **6 / 12**.\n\n```\n4.\n(a) Give a basis of eigenvectors for the $4 \\times 4$ CNOT matrix, along with their associated eigenvalues.\n\nThe CNOT (Controlled-NOT) matrix is a $4 \\times 4$ matrix given by:\n\n$$\nCNOT = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0\n\\end{pmatrix} $$\n\nTo find the eigenvectors and eigenvalues, we first need to find the characteristic polynomial of the matrix, which is given by the determinant of $(CNOT - \\lambda I)$:\n\n$$\n\\det(CNOT - \\lambda I) = \\det\\begin{pmatrix}\n1-\\lambda & 0 & 0 & 0 \\\\\n0 & 1-\\lambda & 0 & 0 \\\\\n0 & 0 & -\\lambda & 1 \\\\\n0 & 0 & 1 & -\\lambda\n\\end{pmatrix} $$\n\nThe determinant of this matrix is $(1-\\lambda)^2(-\\lambda)^2 - (1-\\lambda)^2 = (1-\\lambda)^2(\\lambda^2 - 1)$. Thus, the eigenvalues are $\\lambda_1 = 1$, $\\lambda_2 = 1$, $\\lambda_3 = -1$, and $\\lambda_4 = -1$.\n\nNow, we find the eigenvectors associated with each eigenvalue:\n\n1. For $\\lambda_1 = 1$:\n\n$$ (CNOT - I)v_1 = \\begin{pmatrix}\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & -1 & 1 \\\\\n0 & 0 & 1 & -1\n\\end{pmatrix}v_1 = 0 $$\n\nA basis for the eigenspace corresponding to $\\lambda_1 = 1$ is given by $v_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$ and $v_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\n2. For $\\lambda_3 = -1$:\n\n$$ (CNOT + I)v_3 = \\begin{pmatrix}\n2 & 0 & 0 & 0 \\\\\n0 & 2 & 0 & 0 \\\\\n0 & 0 & 1 & 1 \\\\\n0 & 0 & 1 & 1\n\\end{pmatrix}v_3 = 0 $$\n\nA basis for the eigenspace corresponding to $\\lambda_3 = -1$ is given by $v_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ -1 \\end{pmatrix}$ and $v_4 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$.\n\nThus, a basis of eigenvectors for the CNOT matrix is given by:\n\n$$\nv_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\nv_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\nv_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ -1 \\end{pmatrix}, \\quad\nv_4 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 1 \\end{pmatrix} $$\n\nwith associated eigenvalues $\\lambda_1 = 1$, $\\lambda_2 = 1$, $\\lambda_3 = -1$, and $\\lambda_4 = -1$.\n\nJUSTIN: Up to 3 points.\nThe eigenvalue lambda4 is wrong, it should be +1.\n2/3.\n\n(b) Give a 2-qubit Hamiltonian $H$ such that $e^{-iH}$ is the CNOT matrix.\n\nTo find a Hamiltonian $H$ such that $e^{-iH} = CNOT$, we can use the fact that the CNOT matrix is a unitary matrix and can be decomposed into a product of Pauli matrices. The Pauli matrices are given by:\n\n$$\n\\sigma_x = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}, \\quad\n\\sigma_y = \\begin{pmatrix} 0 & -i \\\\ i & 0 \\end{pmatrix}, \\quad\n\\sigma_z = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix} $$\n\nThe CNOT matrix can be written as a product of tensor products of Pauli matrices:\n\n$$\nCNOT = \\frac{1}{2} (I \\otimes I + I \\otimes \\sigma_x + \\sigma_z \\otimes I - \\sigma_z \\otimes \\sigma_x) $$\n\nNow, we can find a Hamiltonian $H$ such that $e^{-iH} = CNOT$. We can use the fact that the exponential of a sum of operators is equal to the product of exponentials of the individual operators if the operators commute:\n\n$$ e^{-iH} = e^{-i\\frac{\\theta}{2}(I \\otimes I)} e^{-i\\frac{\\theta}{2}(I \\otimes \\sigma_x)} e^{-i\\frac{\\theta}{2}(\\sigma_z \\otimes I)} e^{i\\frac{\\theta}{2}(\\sigma_z \\otimes \\sigma_x)} $$\n\nWe want to find the angle $\\theta$ such that this product is equal to the CNOT matrix. Since the Pauli matrices have eigenvalues $\\pm 1$, we can see that the angle $\\theta$ must be $\\pi$:\n\n$$ e^{-iH} = e^{-i\\frac{\\pi}{2}(I \\otimes I)} e^{-i\\frac{\\pi}{2}(I \\otimes \\sigma_x)} e^{-i\\frac{\\pi}{2}(\\sigma_z \\otimes I)} e^{i\\frac{\\pi}{2}(\\sigma_z \\otimes \\sigma_x)} $$\n\nNow, we can find the Hamiltonian $H$:\n\n$$ H = \\frac{\\pi}{2}(I \\otimes I + I \\otimes \\sigma_x + \\sigma_z \\otimes\nI - \\sigma_z \\otimes \\sigma_x) $$\n\nSo, the 2-qubit Hamiltonian $H$ such that $e^{-iH} = CNOT$ is given by:\n\n$$\nH = \\frac{\\pi}{2} \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 2 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & -1\n\\end{pmatrix} $$\n\nJUSTIN: Up to 2 points.\nPlugging into a calculator, this answer is incorrect.\nIt started to go wrong when putting Pauli matrices in exponential form, since Pauli matrices actually anti-commute. Partial credit for very significant effort.\n\n1/2 points.\n\n(c) Can your $H$ (or some $H$ that solves (b)) be written as a tensor product $H_1 \\otimes H_2$ of a Hamiltonian on the first qubit and a Hamiltonian on the second qubit? If so, how?\n\nNo, the Hamiltonian $H$ that we found in the previous problem cannot be written as a tensor product $H_1 \\otimes H_2$ of a Hamiltonian on the first qubit and a Hamiltonian on the second qubit. The reason is that the CNOT operation is an entangling operation, meaning that it creates entanglement between the two qubits. If the Hamiltonian could be written as a tensor product of Hamiltonians acting on individual qubits, it would not be able to create entanglement between the qubits.\n\nTo see this more explicitly, let's try to write the Hamiltonian $H$ as a tensor product:\n\n$$ H = H_1 \\otimes H_2 = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}\n\\otimes \\begin{pmatrix} e & f \\\\ g & h \\end{pmatrix} $$\n\nThe tensor product would result in a $4 \\times 4$ matrix:\n\n$$ H = \\begin{pmatrix}\na e & a f & b e & b f \\\\\na g & a h & b g & b h \\\\\nc e & c f & d e & d f \\\\\nc g & c h & d g & d h\n\\end{pmatrix} $$\n\nComparing this with the Hamiltonian $H$ we found earlier:\n\n$$ H = \\frac{\\pi}{2} \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 2 & 0 \\\\\n0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & -1\n\\end{pmatrix} $$\n\nWe can see that there is no way to find values for $a, b, c, d, e, f, g, h$ such that the two matrices are equal. This confirms that the Hamiltonian $H$ cannot be written as a tensor product of Hamiltonians acting on individual qubits.\n\nJUSTIN: Up to 3 points.\nA correct solution to part (b) written in the form of part (c) is ((0 0) (0 1)) \\otimes pi/2 ((1 -1) (-1 1)).\nThis question is a bit harsh with partial/carry-through credit, but the answer also explicitly got the main idea wrong, claiming that a tensor product H cannot give an entangling unitary.\n0/3.\n\n(d) Is there an $H$ that solves (b), while also being a sum $H_1\\otimes I + I\\otimes H_2$ of a Hamiltonian on the first qubit and a Hamiltonian on the second qubit? Why or why not?\n\nNo, there is no Hamiltonian $H$ such that $e^{iH} = CNOT$ and $H = H_1 \\otimes I + I \\otimes H_2$. The reason is that the CNOT operation is an entangling operation, meaning that it creates entanglement between the two qubits. If the Hamiltonian could be written as a sum of Hamiltonians acting on individual qubits, it would not be able to create entanglement between the qubits.\n\nTo see this more explicitly, let's assume that there exists such an $H$:\n\n$$ H = H_1 \\otimes I + I \\otimes H_2 = \\begin{pmatrix} a & b \\\\ c & d\n\\end{pmatrix} \\otimes \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} +\n\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} \\otimes \\begin{pmatrix} e\n& f \\\\ g & h \\end{pmatrix} $$\n\nThe sum of the tensor products would result in a $4 \\times 4$ matrix:\n\n$$ H = \\begin{pmatrix}\na + e & b & a & b \\\\\nc & d + e & c & d \\\\\na & b & a + h & b \\\\\nc & d & c & d + h\n\\end{pmatrix} $$\n\nNow, we want to find an $H$ such that $e^{iH} = CNOT$. The CNOT matrix is given by:\n\n$$ CNOT = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n0 & 0 & 1 & 0\n\\end{pmatrix} $$\n\nHowever, there is no way to find values for $a, b, c, d, e, f, g, h$ such that $e^{iH} = CNOT$. This is because the CNOT operation creates entanglement between the qubits, and a Hamiltonian that can be written as a sum of Hamiltonians acting on individual qubits cannot create entanglement.\n\nIn summary, there is no Hamiltonian $H$ such that $e^{iH} = CNOT$ and $H = H_1 \\otimes I + I \\otimes H_2$ because the CNOT operation is an entangling operation, and a Hamiltonian that can be written as a sum of Hamiltonians acting on individual qubits cannot create entanglement.\n\nUp to 4 points.\nIt's correct that an H of this form can't generate CNOT because it can't create entanglement, although most of the rest is just repetitive fluff.\n3/4.\n```\n\n* * *\n\nNow for a question about the complexity of generating certain uniform superpositions over sets (a special case of “QSampling”). Total score: **19 / 18** (of course counting extra credit).\n\n```\n5. In the Graph Isomorphism problem, we're given as input two $n$-vertex undirected graphs $G$ and $H$. The problem is to determine whether they're isomorphic---in other words, whether there's any permutation of the vertex labels that makes $G$ and $H$ equal.\n\n(a) Given as input an $n$-vertex graph $G$, describe how to sample, in classical $\\mathrm{poly}(n)$ time, from a probability distribution $D_G$ over graphs such that:\n- Whenever the graphs $G$ and $H$ are isomorphic, $D_G = D_H$.\n- Whenever $G$ and $H$ are non-isomorphic, $D_G$ and $D_H$ have disjoint supports (i.e., no graph appears with nonzero probability in both of them).\n\nTo sample from the probability distribution $D_G$ over graphs, we can follow these steps:\n\n1. Compute the canonical form of the input graph $G$. This can be done using a graph canonization algorithm, such as the one by Brendan McKay (nauty) or the one by Laszlo Babai (Traces). These algorithms run in classical $\\mathrm{poly}(n)$ time.\n\n2. Generate a random permutation of the vertex labels of the canonical form of $G$. This can be done by selecting a random permutation of the integers from $1$ to $n$, and then applying this permutation to the vertex labels of the canonical form.\n\n3. Apply the random permutation to the canonical form of $G$ to obtain a new graph $G'$. This graph $G'$ is a sample from the probability distribution $D_G$.\n\nThe above procedure ensures that whenever the graphs $G$ and $H$ are isomorphic, $D_G = D_H$, because their canonical forms will be the same, and the random permutations applied to their canonical forms will generate the same distribution over graphs. On the other hand, whenever $G$ and $H$ are non-isomorphic, their canonical forms will be different, and the random permutations applied to their canonical forms will generate disjoint supports for $D_G$ and $D_H$.\n\nJUSTIN: Up to 5 points.\nThe first step of computing the canonical form is superfluous and incorrect, since computing the canonical form is at least as hard as solving graph isomorphism and we don't know how to solve that in polynomial time.\nAlso, it looks like step 2 or 3 should have been about graph H instead of both being about G.\nSo it seems to have exactly the right idea but described it poorly.\n4/5.\n\n(b) Given a probability distribution $D=(p_x)$ over $n$-bit strings $x$, define the ``QSampling state'' of $D$ to be $$\\ket{\\psi_D} := \\sum_{x \\in \\{0,1\\}^n} \\sqrt{p_x} \\ket{x}$$\nGiven two probability distributions $D$ and $D'$, suppose that the quantum circuit $\\mathcal{C}$ maps $\\ket{0^n}$ to $\\ket{\\psi_D}$, while the circuit $\\mathcal{C}'$ maps $\\ket{0^n}$ to $\\ket{\\psi_{D'}}$.\n\nThen what is the output state of the circuit shown below, which acts on $n+1$ qubits?\n$$ \\Qcircuit @C=.5em @R=0.5em @!R {\n\\lstick{\\ket{0}} & \\gate{H} & \\ctrl{1} & \\gate{NOT} & \\ctrl{1} & \\qw\\\\\n\\lstick{\\ket{0^n}} & \\qw & \\gate{\\mathcal{C}} & \\qw & \\gate{\\mathcal{C}'} & \\qw } $$\n\nTo find the output state of the given circuit, we first analyze it step by step.\n\n1. After applying the Hadamard gate (H) on the first qubit, the state becomes:\n$$\\frac{1}{\\sqrt{2}}(\\ket{0} + \\ket{1}) \\otimes \\ket{0^n}$$\n\n2. Next, the controlled-$\\mathcal{C}$ gate is applied. This gate applies the $\\mathcal{C}$ operation on the second register if the first qubit is in state $\\ket{0}$. So, the state becomes:\n$$\\frac{1}{\\sqrt{2}}(\\ket{0} \\otimes \\ket{\\psi_D} + \\ket{1} \\otimes \\ket{0^n})$$\n\n3. The NOT gate is applied on the first qubit, which flips the state of the first qubit. The state becomes:\n$$\\frac{1}{\\sqrt{2}}(\\ket{1} \\otimes \\ket{\\psi_D} + \\ket{0} \\otimes \\ket{0^n})$$\n\n4. Finally, the controlled-$\\mathcal{C}'$ gate is applied. This gate applies the $\\mathcal{C}'$ operation on the second register if the first qubit is in state $\\ket{0}$. So, the state becomes:\n$$\\frac{1}{\\sqrt{2}}(\\ket{1} \\otimes \\ket{\\psi_D} + \\ket{0} \\otimes \\ket{\\psi_{D'}})$$\n\nThus, the output state of the circuit is:\n$$\\frac{1}{\\sqrt{2}}(\\ket{1} \\otimes \\ket{\\psi_D} + \\ket{0} \\otimes \\ket{\\psi_{D'}})$$\n\nJUSTIN: 4/4.\nI think solving this one is impressive.\n\n(c) Now suppose we measure the first qubit of that output state in the $\\{\\ket{+},\\ket{-}\\}$ basis. What is the probability of the outcome $\\ket{+}$ if $D=D'$? What about if $D$ and $D'$ have disjoint supports?\n\nTo find the probability of the outcome $\\ket{+}$, we first express the output state in terms of the $\\{\\ket{+},\\ket{-}\\}$ basis. Recall that $\\ket{+} = \\frac{1}{\\sqrt{2}}(\\ket{0} + \\ket{1})$ and $\\ket{-} = \\frac{1}{\\sqrt{2}}(\\ket{0} - \\ket{1})$.\n\nThe output state is:\n$$\\frac{1}{\\sqrt{2}}(\\ket{1} \\otimes \\ket{\\psi_D} + \\ket{0} \\otimes \\ket{\\psi_{D'}})$$\n\nWe can rewrite this state in terms of the $\\{\\ket{+},\\ket{-}\\}$ basis as follows:\n$$\\frac{1}{2}(\\ket{+} \\otimes (\\ket{\\psi_D} + \\ket{\\psi_{D'}}) + \\ket{-} \\otimes (\\ket{\\psi_D} - \\ket{\\psi_{D'}}))$$\n\nNow, we can find the probability of the outcome $\\ket{+}$ by taking the squared magnitude of the coefficient of $\\ket{+}$ in the state.\n\nCase 1: $D = D'$\n\nIn this case, $\\ket{\\psi_D} = \\ket{\\psi_{D'}}$. So, the output state becomes:\n$$\\frac{1}{2}(\\ket{+} \\otimes (2\\ket{\\psi_D}) + \\ket{-} \\otimes (0))$$\n\nThe probability of the outcome $\\ket{+}$ is the squared magnitude of the coefficient of $\\ket{+}$, which is:\n$$\\left|\\frac{1}{2}(2)\\right|^2 = 1$$\n\nCase 2: $D$ and $D'$ have disjoint supports\n\nIn this case, $\\ket{\\psi_D}$ and $\\ket{\\psi_{D'}}$ are orthogonal, i.e., their inner product is zero: $\\langle\\psi_D|\\psi_{D'}\\rangle = 0$. The probability of the outcome $\\ket{+}$ is the squared magnitude of the coefficient of $\\ket{+}$, which is:\n$$\\left|\\frac{1}{2}(\\langle\\psi_D|\\psi_D\\rangle + \\langle\\psi_{D'}|\\psi_{D'}\\rangle)\\right|^2$$\n\nSince $\\langle\\psi_D|\\psi_D\\rangle = 1$ and $\\langle\\psi_{D'}|\\psi_{D'}\\rangle = 1$ (both states are normalized), the probability of the outcome $\\ket{+}$ is:\n$$\\left|\\frac{1}{2}(1 + 1)\\right|^2 = \\frac{1}{4}(2)^2 = \\frac{1}{2}$$\n\nSo, if $D = D'$, the probability of the outcome $\\ket{+}$ is 1, and if $D$ and $D'$ have disjoint supports, the probability of the outcome $\\ket{+}$ is $\\frac{1}{2}$.\n\nSCOTT: HOLY CRAP, LOOKS CORRECT!\nJUSTIN: 5/5.\n\n(d) Suppose your distributions $D_G$ from part (a) could be efficiently QSampled. Using your previous work, explain how Graph Isomorphism could then be solved in $BQP$ (quantum polynomial time).\n\nSuppose we can efficiently QSample the distributions $D_G$ and $D_H$ for two input graphs $G$ and $H$. We can then construct the QSampling states $\\ket{\\psi_{D_G}}$ and $\\ket{\\psi_{D_H}}$ for these distributions using the quantum circuits $\\mathcal{C}_G$ and $\\mathcal{C}_H$, respectively, as described in the previous problems.\n\nNow, consider the following quantum circuit acting on $n+1$ qubits:\n\n$$ \\Qcircuit @C=.5em @R=0.5em @!R { \\lstick{\\ket{0}} & \\gate{H} & \\ctrl{1} & \\gate{NOT} & \\ctrl{1} & \\qw\\\\ \\lstick{\\ket{0^n}} & \\qw & \\gate{\\mathcal{C}_G} & \\qw & \\gate{\\mathcal{C}_H} & \\qw } $$\n\nAs we found earlier, the output state of this circuit is:\n\n$$\\frac{1}{\\sqrt{2}}(\\ket{1} \\otimes \\ket{\\psi_{D_G}} + \\ket{0} \\otimes \\ket{\\psi_{D_H}})$$\n\nNow, we measure the first qubit of the output state in the $\\{\\ket{+},\\ket{-}\\}$ basis. We know from our previous work that if $G$ and $H$ are isomorphic (i.e., $D_G = D_H$), the probability of the outcome $\\ket{+}$ is 1, and if $G$ and $H$ are non-isomorphic (i.e., $D_G$ and $D_H$ have disjoint supports), the probability of the\noutcome $\\ket{+}$ is $\\frac{1}{2}$.\n\nThus, we can solve the Graph Isomorphism problem in $BQP$ as follows:\n\n1. Construct the QSampling states $\\ket{\\psi_{D_G}}$ and $\\ket{\\psi_{D_H}}$ for the input graphs $G$ and $H$ using the quantum circuits $\\mathcal{C}_G$ and $\\mathcal{C}_H$.\n\n2. Implement the above quantum circuit and measure the first qubit in the $\\{\\ket{+},\\ket{-}\\}$ basis.\n\n3. If the outcome is $\\ket{+}$ with probability close to 1, conclude that the graphs $G$ and $H$ are isomorphic. If the outcome is $\\ket{+}$ with probability close to $\\frac{1}{2}$, conclude that the graphs $G$ and $H$ are non-isomorphic.\n\nThis algorithm runs in quantum polynomial time, as the QSampling circuits $\\mathcal{C}_G$ and $\\mathcal{C}_H$ are assumed to be efficient, and the rest of the operations (Hadamard, NOT, and controlled gates) are also efficient.\n\nJUSTIN: The answer is longer than necessary, repeating some of its previous work.\nAlso, in theory, if the graphs are isomorphic then the probability of |+> will be exactly 1 not close to 1.\n\n4/4.\n\n(e) So then why doesn't this approach immediately imply a fast quantum algorithm for Graph Isomorphism? Explain what could go wrong in passing from fast algorithms to sample $D_G$ and $D_H$, to fast algorithms to QSample them.\n\nThe approach described above does not immediately imply a fast quantum algorithm for Graph Isomorphism because there is a crucial difference between classical sampling and QSampling.\n\nIn the algorithm, we assumed that we can efficiently construct quantum circuits $\\mathcal{C}_G$ and $\\mathcal{C}_{G'}$ that map $\\ket{0^n}$ to $\\ket{\\psi_{D_G}}$ and $\\ket{\\psi_{D_{G'}}}$, respectively, given that the distributions $D_G$ and $D_{G'}$ can be efficiently sampled classically. However, this assumption might not hold in general.\n\nClassical sampling allows us to draw samples from the distributions $D_G$ and $D_{G'}$, but it does not directly provide a way to prepare the corresponding QSampling states $\\ket{\\psi_{D_G}}$ and $\\ket{\\psi_{D_{G'}}}$. Preparing these quantum states requires a coherent superposition of all the elements in the distributions with the appropriate amplitudes, which might not be efficiently achievable using the classical sampling algorithms.\n\nIn other words, the ability to efficiently sample from a probability distribution classically does not necessarily imply the ability to efficiently prepare the corresponding QSampling state on a quantum computer. This is the main reason why the approach does not immediately lead to a fast quantum algorithm for Graph Isomorphism.\n\nSCOTT: Correctly explains that sampling doesn't necessarily imply QSampling (i.e. generating a coherent superposition state), but doesn't say that this is because of the difficulty of uncomputing garbage.\n2 / 5 extra credit points.\n```\n\n* * *\n\nNext, a problem about quantum communication complexity. Total score: **5 / 10**.\n\n```\n6. Suppose Alice and Bob hold $n$-bit strings $x=(x_1,...,x_n)$ and $y=(y_1,...,y_n)$ respectively. One thing they might want to learn is the mod-2 inner product of their strings, $$x_1 y_1 + \\ldots + x_n y_n (\\bmod 2).$$\n\n(a) Suppose Alice and Bob had a quantum communication protocol in which they are allowed to exchange up to $T$ qubits and to perform arbitrary local unitary transformations to their qubits (possibly including ancilla qubits), that ended with Bob knowing the above inner product, with success probability 1. Explain how, by exchanging the same number of qubits $T$, Bob could also prepare an $n$-qubit state\nof the form $$\\frac{1}{\\sqrt{2^n}} \\sum_{z \\in \\{0,1\\}^n} (-1)^{x\\cdot z} \\ket{z},$$\nwhere $x$ is an $n$-bit string held by Alice.\n\nIf Alice and Bob have a quantum communication protocol that allows Bob to know the mod-2 inner product of their strings with success probability 1, then they can use the same protocol to prepare the desired $n$-qubit state.\n\nHere's how they can do it:\n\n1. Alice prepares an $n$-qubit state $\\ket{\\phi}$, where\n$$\\ket{\\phi} = \\frac{1}{\\sqrt{2^n}} \\sum_{z \\in \\{0,1\\}^n} \\ket{z}.$$\n\n2. Alice applies a controlled-phase gate to each qubit of $\\ket{\\phi}$, using her $n$-bit string $x$ as the control. This results in the state\n$$\\frac{1}{\\sqrt{2^n}} \\sum_{z \\in \\{0,1\\}^n} (-1)^{x\\cdot z} \\ket{z}.$$\n\n3. Alice sends the $n$-qubit state to Bob.\n\n4. Bob uses the quantum communication protocol to learn the mod-2 inner product of their strings. Since the protocol has success probability 1, Bob will know the inner product with certainty.\n\n5. Bob applies a local unitary transformation to the $n$-qubit state he received from Alice, based on the inner product he learned in step 4. This will result in the desired state\n$$\\frac{1}{\\sqrt{2^n}} \\sum_{z \\in \\{0,1\\}^n} (-1)^{x\\cdot z} \\ket{z}.$$\n\nSince Alice and Bob only exchanged $T$ qubits in the original protocol, they can also prepare the desired $n$-qubit state by exchanging the same number of qubits $T$.\n\nJUSTIN: Up to 4 points.\nThis answer is partially correct and/or poorly worded. In particular, in step 3 Alice just sends the whole n-qubit state to Bob.\nThe right idea is in there, but doesn't earn full points.\n\n2/4.\n\n(b) Assume Alice and Bob have no preshared entanglement. Recall Holevo's Theorem, which implies that in order to communicate $n$ bits to Bob reliably, Alice must send Bob at least $n$ qubits. Using Holevo's Theorem together with part (a), prove that Alice and Bob must exchange at least $n$ qubits, even if they only want to learn the inner product mod 2 of their input strings $x$ and $y$.\n\nFrom part (a), we know that if Alice and Bob have a quantum communication protocol that allows Bob to know the mod-2 inner product of their strings with success probability 1, then they can also prepare an $n$-qubit state of the form\n$$\\frac{1}{\\sqrt{2^n}} \\sum_{z \\in \\{0,1\\}^n} (-1)^{x\\cdot z} \\ket{z},$$\nby exchanging the same number of qubits $T$.\n\nNow, let's consider the reverse process. Suppose Bob has the $n$-qubit state\n$$\\frac{1}{\\sqrt{2^n}} \\sum_{z \\in \\{0,1\\}^n} (-1)^{x\\cdot z} \\ket{z}.$$\nHe can perform a quantum measurement in the computational basis to obtain a random $n$-bit string $z$. Then, he can compute the mod-2 inner product $x \\cdot z$ and send it back to Alice.\n\nAlice, knowing her own string $x$, can compute the mod-2 inner product $x \\cdot z$ as well. If the inner product $x \\cdot z$ that Bob sent to Alice matches the one she computed, then they have successfully communicated the inner product mod 2 of their input strings $x$ and $y$.\n\nHowever, according to Holevo's Theorem, in order to communicate $n$ bits to Bob reliably, Alice must send Bob at least $n$ qubits. Since the $n$-qubit state prepared in part (a) effectively communicates $n$ bits of information (Alice's string $x$) to Bob, Holevo's Theorem implies that Alice and Bob must exchange at least $n$ qubits to prepare this state.\n\nTherefore, even if Alice and Bob only want to learn the inner product mod 2 of their input strings $x$ and $y$, they must exchange at least $n$ qubits.\n\nJUSTIN: Up to 6 points.\nThis answer doesn't earn any points. It only says that Alice and Bob learn the inner products and mistakenly claims this is n bits of information.\nThe correct solution recognizes that applying Hadamard to the state we showed how to prepare in part (a) gives exactly |x>.\n\n0/6.\n\n(c) Now suppose we're no longer working mod 2, and Alice and Bob want to know whether their inner product\n$$x_1 y_1 + \\ldots + x_n y_n$$\nis zero or nonzero as an integer. (In other words, whether there's an $i$ such that $x_i=y_i=1$.) Describe a protocol by which Alice and Bob can accomplish this, with high probability, by exchanging only $O(\\sqrt{n}\\log n )$ qubits in total. The qubits can be spread across as many rounds of communication as necessary, and can be sent in either direction.\n\nAlice and Bob can use the following protocol, based on the Grover's search algorithm, to determine whether their inner product is zero or nonzero as an integer, with high probability, by exchanging only $O(\\sqrt{n}\\log n)$ qubits in total:\n\n1. Alice prepares a quantum register of $n$ qubits in the state $\\ket{0}^{\\otimes n}$.\n\n2. Alice applies the Hadamard gate to each qubit, resulting in the state $$\\ket{\\psi} = \\frac{1}{\\sqrt{2^n}} \\sum_{z \\in \\{0,1\\}^n} \\ket{z}.$$\n\n3. Alice and Bob perform the following steps for $O(\\sqrt{n})$ iterations:\n\n a. Alice sends the $n$-qubit state $\\ket{\\psi}$ to Bob.\n\n b. Bob applies a controlled-phase gate to each qubit of $\\ket{\\psi}$, using his $n$-bit string $y$ as the control. This results in the state\n $$\\ket{\\psi'} = \\frac{1}{\\sqrt{2^n}} \\sum_{z \\in \\{0,1\\}^n} (-1)^{y\\cdot z} \\ket{z}.$$\n\n c. Bob sends the $n$-qubit state $\\ket{\\psi'}$ back to Alice.\n\n d. Alice applies a controlled-phase gate to each qubit of $\\ket{\\psi'}$, using her $n$-bit string $x$ as the control. This results in the state\n $$\\ket{\\psi''} = \\frac{1}{\\sqrt{2^n}} \\sum_{z \\in \\{0,1\\}^n} (-1)^{(x+y)\\cdot z} \\ket{z}.$$\n Note that if $x_i = y_i = 1$ for some $i$, then the phase of the corresponding term in the sum will be flipped twice, resulting in no change. Therefore, the state $\\ket{\\psi''}$ will have a non-zero amplitude for the terms corresponding to the indices where $x_i = y_i = 1$.\n\n e. Alice applies the Grover's diffusion operator to the state $\\ket{\\psi''}$, which amplifies the amplitudes of the terms corresponding to the indices where $x_i = y_i = 1$. The diffusion operator can be implemented as follows:\n - Apply the Hadamard gate to each qubit.\n - Apply a multi-qubit controlled-Z gate, with all qubits as controls and an ancilla qubit as the target.\n - Apply the Hadamard gate to each qubit again.\n\n f. Alice updates the state $\\ket{\\psi}$ with the resulting state after applying the Grover's diffusion operator.\n\n4. After $O(\\sqrt{n})$ iterations, Alice measures the final state in the computational basis. If she obtains a non-zero result, then there exists an $i$ such that $x_i = y_i = 1$, and their inner product is nonzero as an integer. If she obtains a zero result, then their inner product is zero.\n\nSince Alice and Bob exchange $n$ qubits in each iteration, and they perform $O(\\sqrt{n})$ iterations, the total number of qubits exchanged is $O(n\\sqrt{n}) = O(\\sqrt{n}\\log n)$, as each qubit can represent $\\log n$ bits of information. Thus, Alice and Bob can determine whether their inner product is zero or nonzero as an integer, with high probability, by exchanging only $O(\\sqrt{n}\\log n)$ qubits in total.\n\nSCOTT: LOOKS BASICALLY RIGHT, EXCEPT FOR A SLIP-UP AT THE END WHERE IT SAYS\nALICE AND BOB EXCHANGE n QUBITS PER ITERATION RATHER THAN log n ...\n\nJUSTIN: Up to 6 extra credit points.\nYeah, step 3a says it's sending n qubits, when it's actually log n.\nIt also seems incorrect beginning in the analysis of step d. It switches from summing over |z> and referring to the full strings x and y to referring to individual indices i,x_i,y_i. And I don't know what it means about some amplitudes going to zero since what we're really doing is having some amplitudes be +1 and some -1.\nI don't think this protocol is correct as stated.\nBut, it has some of the main ideas. The analysis of the grover steps is good and using a phase gate is good.\n\n3/6 extra credit.\n```\n\n* * *\n\nOK, last question, about the k-SUM problem. Total score: **6 / 15**.\n\n```\n7. In the famous $k$-SUM problem, we're given a list of integers $x_1,\\ldots,x_n$, and are asked whether there are $k$ distinct indices, $i_1 < \\ldots We gave GPT-4 the problems via their LaTeX source code, which GPT-4 can perfectly well understand. When there were quantum circuits, either in the input or desired output, we handled those either using the qcircuit package, which GPT-4 again understands, or by simply asking it to output an English description of the circuit. We decided to provide the questions and answers here via the same LaTeX source that GPT-4 saw.\n\n\n Why didn’t you use the image input feature? As you work at OA in some capacity now, presumably it wouldn’t be hard to use that, and the GPT-4 paper suggests that in some cases the image inputs can substantially boost exam performance.\n\n026. [Johanna Wilder](http://www.zipbangwow.com) Says:\n\n Comment #26 [April 11th, 2023 at 3:33 pm](https://scottaaronson.blog/?p=7209#comment-1949046)\n I am trying to ‘steelman’ the opposing argument, that these things, clever as they appear, are nothing more than highly-sophisticated probabilistic autocorrect functions. If I try to reach beyond the semantics, for their meaning, it seems that they mean to highlight the primitive nature of these decisions being made, that don’t hold a candle to the sophistication of human thought.\n\n That, essentially, we are mistaking the tremendous amounts of compute when applied to language for intelligence, just as we once did the ability for computers to so overwhelmingly outperform humans at mathematics.\n\n I think that if we were to blur out on this, we all have more in common with our viewpoints than tends to be represented in the “us versus them” nature of online debate these days.\n\n • These things are highly sophisticated\n\n • These things are still very primitive\n\n • We are moving too fast in the sense that these features should be considered still alpha versions, not added to production-level software so quickly.\n\n I do believe that sooner or later, we’re going to be in agreement, that this is either the Greatest Scam Ever or we have actually pushed AI forward in a big way. I hope this doesn’t end up like Covid-19, where we are all pushed into different identity camps. That would be a most regrettable outcome.\n\n Bing Chat went on in more depth, but was in enthusiastic agreement with this statement, “I agree that it would be unfortunate if we end up in different identity camps over this issue. I think we should strive for a constructive and respectful dialogue that acknowledges the complexity and uncertainty of this topic. I hope we can find some common ground and learn from each other. 😊”\n\n027. [Mike](https://www.oreilly.com/people/mike-loukides/) Says:\n\n Comment #27 [April 11th, 2023 at 3:36 pm](https://scottaaronson.blog/?p=7209#comment-1949047)\n Although the exam itself isn’t online, you have posted lecture notes online (PDFs). I downloaded them in 2018, so they were available prior to the training cutoff date. Don’t know if they are for the same course (notes are for Intro to Quantum Information Science). (Was that before you went to UT?)\n\n Still, impressive for GPT-4 to pass the exam.\n\n It’s very weird what GPT-4 can and can’t do. I’ve been playing with primality tests, and it botches those rather shamelessly. Nor can I get it to write a Petrarchan sonnet correctly (though it’s good at Shakespearian sonnets). So it would lose some points in an exam on Renaissance century English lit. 3.5 was bad at telling whether an author was human or AI–it would often tell me that an author was AI, and then identify the quote correctly. (Tried with a number of authors from the 16th to the 20th century; haven’t yet tried this with GPT-4).\n\n028. Vladimir Says:\n\n Comment #28 [April 11th, 2023 at 3:57 pm](https://scottaaronson.blog/?p=7209#comment-1949051)\n \\> Extra Credit: Suppose we perform a quantum computation by physically applying a Hamiltonian H. What’s to stop us from running the computation, say, $10^{100}$ times faster, by simply replacing H by $H’:=10^{100}H$, which is also a valid Hamiltonian? Or to put it differently: is there some resource that we’d need $10^{100}$ more of to do this?\n\n Presumably by “applying a Hamiltonian H” you meant something like “applying exp(iHt)”? Applying H and H’ directly gives exactly the same states.\n\n029. Lane Says:\n\n Comment #29 [April 11th, 2023 at 4:22 pm](https://scottaaronson.blog/?p=7209#comment-1949052)\n Fascinating. I know the models are very complex and tracing an answer back to specific training data isn’t how they work, but are there tricks to get Chat-GPT4 to find specific comparable instances inside its training data or the rest of the internet (if anything exists outside the overlap of that Venn diagram)? Some like “True or False, and show your sources?”\n\n030. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #30 [April 11th, 2023 at 4:44 pm](https://scottaaronson.blog/?p=7209#comment-1949055)\n Nate #20: Yes, some of the mistakes are common ones, but the _pattern_ of mistakes—in particular, doing much better on conceptual questions than on calculation questions—is wildly uncommon among human test-takers.\n\n In general, though, for any mistake that you see here, it’s an excellent bet that I’ve seen a similar mistake from a student.\n\n031. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #31 [April 11th, 2023 at 4:51 pm](https://scottaaronson.blog/?p=7209#comment-1949056)\n Nick Drozd #22:\n\n\n So: do you think GPT-4 is intelligent, or what?\n\n\n I think that, if it were a human, we’d **clearly** call the human extremely intelligent in some ways, albeit also surprisingly dumb in other ways, given its demonstrated intelligence.\n\n I’m sorry if that answer is still too evasive for you! But again, imagine someone asking of the first submarine: “so then, does it swim, or not swim?” Wouldn’t it trivialize the new reality to reduce it to such a question?\n\n\n Also, have you tried using ChatGPT to generate exam questions? Can you answer all of its questions?\n\n\n Great idea, I’ll try that next and let you know!\n\n032. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #32 [April 11th, 2023 at 4:59 pm](https://scottaaronson.blog/?p=7209#comment-1949057)\n gwern #25:\n\n\n Why didn’t you use the image input feature? As you work at OA in some capacity now, presumably it wouldn’t be hard to use that, and the GPT-4 paper suggests that in some cases the image inputs can substantially boost exam performance.\n\n\n I do have access to a multimodal model, but I was worried that it might be weaker than the base gpt-4 model for pure text (I’m not sure). And this is really only relevant for questions 5(b) and 5(d), which ironically are two of the ones that GPT already nailed! And once you’ve seen that GPT-4 can interact with perfect fluency in LaTeX ( _including_ the inline code for diagrams), why not just rely on that?\n\n033. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #33 [April 11th, 2023 at 5:00 pm](https://scottaaronson.blog/?p=7209#comment-1949058)\n Vladimir #28: Yes, of course “applying H” means “applying eiHt for some time t.”\n\n034. Schwarzschild Says:\n\n Comment #34 [April 11th, 2023 at 5:06 pm](https://scottaaronson.blog/?p=7209#comment-1949059)\n I think part of the take a way is how impressive the human brain actually is, and imagine if that where truly scaled that up one day.\n\n (This doesn’t imply GPT-4 is not impressive)\n\n GPT-4 sucked up all the information about QC and Mathematics on the public internet (as well as everything else on the internet basically) and scored about what an undergrad student who didn’t drop a QC class scored. The student even going through the course doing problems had “trained” on far less data than GPT 4 did (likely orders of magnitude).\n\n Some other question this raises, when do these kind of brute force statical AI’s hit their compute or power limits? Is it GPT 5? Do we get A’s on any well formed exams before we hit that limit and have to find new techniques? If we hit some type of compute or power limit before we do get A’s on all exams using current techniques, what that’s telling us about intelligence or the human brain general?\n\n Just think of how efficient the human brain is too (in terms of energy needed to learn something and data needed to learn something)\n\n035. [Alan](https://lifearchitect.ai/iq-testing-ai/) Says:\n\n Comment #35 [April 11th, 2023 at 5:15 pm](https://scottaaronson.blog/?p=7209#comment-1949061)\n Added to the list of tests demolished by GPT-4!\n\n [https://lifearchitect.ai/iq-testing-ai/](https://lifearchitect.ai/iq-testing-ai/)\n\n036. Ariel Says:\n\n Comment #36 [April 11th, 2023 at 5:23 pm](https://scottaaronson.blog/?p=7209#comment-1949063)\n GPT-4 has a bajillion parameters, definitely enough to memoize a bunch of books about quantum computing. Therefore it’s less surprising than it appears that it knows how to answer the sort of questions that have answers in books about quantum computing. It’s basically working “open book”.\n\n The only part of the exam that I am not 99% sure has a lot of very similar material somewhere in the Internet (even if not from your course, from some other professor’s course), is QSampling.\n\n Given GPT-4’s success, I won’t be surprised if there is actually some material about that.\n\n037. Alex Says:\n\n Comment #37 [April 11th, 2023 at 5:38 pm](https://scottaaronson.blog/?p=7209#comment-1949064)\n Thanks for doing this Scott, I am very impressed by GPT and seeing it in action here makes its power much more real for me. I wonder how far away you think this technology is from being able to make legitimate novel research insights of its own, or if you think that is likely?\n\n I was very curious to see exactly how good its answers were and went through a few of the responses very carefully. I think a re-grade might come up with a slightly lower score, although in the same ballpark. For example, I think its answer to 2(a) is wrong, the trace of the density matrix it outputs is greater than 1. (However, this led it to be confused on 2(b) when calculating eigenvalues, which did not add to 1, and may have led to its mistake there, so maybe it deserved more partial credit.) In 5(c)case 2 it appears to have made two mistakes that canceled out to give it the correct answer (eg it asserts \\\\frac{1}{4}(2)^2 = \\\\frac{1}{2}). For 6(c), it was given partial credit, but I don’t think it was really on the right track at all, conflating n with log(n) from the very beginning of its solution, and then trying to back into the answer only at the very end. However, it does mention Grover, so perhaps some amount of partial credit is appropriate.\n\n Considering the above, it only truly nailed a couple of the free-response questions. I wonder if it benefits from partial credit more than a typical student? Maybe students are more likely to leave questions completely blank when they are stumped or run out of time. If you give your next exam to GPT, it would be cool if you translated its answers by hand and then gave it to the grader (without their knowledge) to grade along with the genuine exams. Then we might have more confidence on how it compares, and we can see for example how often students are leaving the questions totally blank. Regardless, I am very impressed.\n\n038. [fikisipi](https://fikisipi.com) Says:\n\n Comment #38 [April 11th, 2023 at 5:42 pm](https://scottaaronson.blog/?p=7209#comment-1949065)\n Someone on HN asked:\n\n I would very much like to see his analysis on which \\[favourite QM\\] interpretations correlate with high scores \\[on the exam\\].\n\n Come on Scott, tell us!\n\n039. [gwern](https://gwern.net) Says:\n\n Comment #39 [April 11th, 2023 at 5:49 pm](https://scottaaronson.blog/?p=7209#comment-1949066)\n\n > I do have access to a multimodal model, but I was worried that it might be weaker than the base gpt-4 model for pure text (I’m not sure).\n\n\n I don’t have the page immediately handy, but as it was presented, the image+text model was always as good or better than the text-only exam scores, so that shouldn’t be an issue.\n\n\n > And this is really only relevant for questions 5(b) and 5(d), which ironically are two of the ones that GPT already nailed! And once you’ve seen that GPT-4 can interact with perfect fluency in LaTeX (including the inline code for diagrams), why not just rely on that?\n\n\n It makes it more human-like and ‘fair’, in some sense. (It would also help future-proof your tests: what if it \\*hadn’t\\* gotten those right? Or you want to use it on more image-intensive benchmarks?) The human students don’t get a LaTeX encoding of the diagram which might help them; or at least, people here are already insinuating that it’s just memorizing lots of papers/books written in LaTeX, and not understanding the conceptual level, and providing the diagram rather than code would increase the plausibility of understanding – after all, even if it’s internally converting the image to the generating LaTeX, that still requires more understanding as a whole system than getting it fed to it in the original LaTeX.\n\n040. Ariel Says:\n\n Comment #40 [April 11th, 2023 at 5:58 pm](https://scottaaronson.blog/?p=7209#comment-1949067)\n I read the Graph Isomorphism problem again.\n\n It looks like it learned to evaluate “example”-ish quantum circuits. How cute. This definitely looks like the pattern that has been repeated in hundreds of lecture notes in its training set.\n\n It also makes a freshmen’s dream error in its calculation and then “magically” gets the right result out (search for \\\\frac{1}{4}(2)^2 = \\\\frac{1}{2}), which makes me even more suspicious that it does not actually do the calculation but rather knows the answer.\n\n041. J. Says:\n\n Comment #41 [April 11th, 2023 at 6:00 pm](https://scottaaronson.blog/?p=7209#comment-1949068)\n These LLMs are excellent students. Some ppl. may want to marry them.\n\n042. Ariel Says:\n\n Comment #42 [April 11th, 2023 at 6:17 pm](https://scottaaronson.blog/?p=7209#comment-1949069)\n @gwern\n\n GPT-4 is obviously computing something. It’s a very large matrix arithmetic circuit. It’s doing a bunch of computations.\n\n My experience with non-black-box, non-deep-learning programs, especially the sort of programs with a ton of implementation details, is that 90% of the time, when they do something impressive, then when you look inside the black box, you see that it is actually a cheap trick that someone put into it.\n\n That’s why my prior for a DL algorithm is also “it does a bunch of tricks”. But also because I expect gradient descent to be even more willing to find these sorts of tricks than human programmers.\n\n043. [gwern](https://gwern.net/gpt-3) Says:\n\n Comment #43 [April 11th, 2023 at 6:23 pm](https://scottaaronson.blog/?p=7209#comment-1949070)\n\n > Nor can I get it to write a Petrarchan sonnet correctly (though it’s good at Shakespearean sonnets).\n\n\n Mike: it doesn’t seem too hard to get most of a Petrarchan sonnet ( [https://en.wikipedia.org/wiki/Petrarchan\\_sonnet](https://en.wikipedia.org/wiki/Petrarchan_sonnet)). I’ve been experimenting with an alliterative translation of Milton’s _Perished Paradise_, you might say, and GPT-4 is very happy to do inner-monologue-style editing of poetry where it revises a poem repeatedly for meter/rhyme/quality. Seems to improve it a lot. (And can be quite funny to watch it revise a poem, or a program, like 8 or 10 times before it finally gives up and sometimes tells you huffily that “the quality of this poem is very high already”.)\n\n In this case, when I try a sonnet on quantum computing and revise it, I wind up with variants on\n\n `In realms where quantum mysteries entwine,\n Entangled states reveal the worlds unknown,\n A revolution stirs, as seeds are sown,\n Unlocking powers that challenge laws divine.\n Amidst the dance of zeros, ones, and chance,\n Unbounded prospects shatter chains of yore,\n Transcending limits known to us before,\n A future gleams with promise to advance.`\n ``\n `Yet caution whispers softly, warns of fate,\n Of shattered codes that breach the walls of trust,\n Unchecked, a force that could engulf us all.\n In disarray, our fragile world might fall.`\n\n `Embrace the quantum age with wisdom's grace,\n For with great power, great duty we must face.`\n\n This doesn’t fit the ABBAABBA CDECDE rhyme scheme GPT-4 claims it does, even when it goes line by line and explains the rhyme pairs. So I think this may be another BPE thing ( [https://gwern.net/GPT-3#bpes](https://gwern.net/GPT-3#bpes)): it’s not sufficiently sure of what rhymes \\*are\\* to be able to self-correct adequately. You can get an idea of the stubbornness of the rhyme errors from my screenshots: [https://gwern.net/doc/ai/nn/transformer/gpt/4/poetry/2023-04-11-gwern-gpt4-poetry-petrarchansonnet-quantumcomputingsession.png](https://gwern.net/doc/ai/nn/transformer/gpt/4/poetry/2023-04-11-gwern-gpt4-poetry-petrarchansonnet-quantumcomputingsession.png) (haven’t found a better way of copying the sessions as of yet because the ‘chat’ interface doesn’t let you copy the whole thing the way the original playground does…)\n\n044. Steve E Says:\n\n Comment #44 [April 11th, 2023 at 8:00 pm](https://scottaaronson.blog/?p=7209#comment-1949072)\n Relatedly, Terry Tao describes how he is using GPT4 in his mathematical research and discusses where it is and isn’t helpful, here: [https://mathstodon.xyz/@tao/110172426733603359](https://mathstodon.xyz/@tao/110172426733603359)\n\n Scott, did you use the non-RLHF version of GPT4 for this test, or did you use the RLHF’d version? I ask because I read in the “Sparks of AGI” paper that MSFT used the non-RLHF’d version and found it did better on reasoning stuff.\n\n045. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #45 [April 11th, 2023 at 9:09 pm](https://scottaaronson.blog/?p=7209#comment-1949075)\n Ariel #36:\n\n\n GPT-4 has a bajillion parameters, definitely enough to memoize a bunch of books about quantum computing. Therefore it’s less surprising than it appears that it knows how to answer the sort of questions that have answers in books about quantum computing. It’s basically working “open book”.\n\n\n For what it’s worth: I just checked, and the students who took this exam were allowed to bring 4 pages of handwritten notes. (Other years I’ve done it _completely_ open-book—especially when everything was remote during covid.)\n\n Of course, 4 pages of notes < I used the gpt-4 chat model in playground, with a temperature of 0.2 and a max length of 1930 tokens.\n\n\n Any guidance on choosing these parameters? Looks like the defaults are 0.7 and 256 and I don’t have much sense of why/when I might want to change them.\n\n PS: Ok, the obvious thing finally occurred to me. Here’s GPT-4’s response to the above (using Scott’s settings of temperature 0.2 and max length 1930):\n\n\n > The temperature and max tokens parameters in the GPT model can be adjusted to control the output’s creativity and length, respectively. Here’s a brief explanation of each parameter and some guidance on choosing their values:\n >\n > 1\\. Temperature: This parameter controls the randomness or creativity of the generated text. A higher temperature (e.g., 0.7 or above) will produce more diverse and creative outputs, while a lower temperature (e.g., 0.2) will make the output more focused and deterministic.\n >\n > – If you want more conservative and coherent responses, choose a lower temperature (e.g., 0.2).\n >\n > – If you want more diverse and creative responses, choose a higher temperature (e.g., 0.7 or higher).\n >\n > 2\\. Max tokens: This parameter controls the maximum length of the generated text. The default value is 256 tokens, which is usually sufficient for most use cases. However, you can increase or decrease this value based on your requirements.\n >\n > – If you need a longer response or have a more complex prompt, you can increase the max tokens value (e.g., 512, 1024, or even 1930).\n >\n > – If you want shorter responses, you can decrease the max tokens value (e.g., 128 or lower).\n >\n > Keep in mind that increasing the max tokens value may lead to higher API usage and longer response times. It’s essential to strike a balance between the desired output length and the API usage.\n >\n > In summary, adjust the temperature and max tokens values based on your desired output’s creativity and length. Experiment with different values to find the optimal settings for your specific use case.\n\n062. Haelfix Says:\n\n Comment #62 [April 12th, 2023 at 1:52 am](https://scottaaronson.blog/?p=7209#comment-1949096)\n Scott, how much work is OpenAI doing in finetuning their LLMs for various applications (like quantum computing). One of the obvious areas of improvement would be feeding it textbooks in certain subjects, and then scrumming through the high citation arxiv papers. Or is their compute very much confined to creating the most general models possible?\n\n The next thing to try is to implement it with a proper plugin, like Wolfram and then repeat the finetuning to see if that helps or not.\n\n Finally, a sort of reasoning loop (like autogpt) where it starts trying to subdivide problems into simpler pieces, perform unit/consistency checks, and then revise the answers. Right now it answers questions as if it was someone’s off the cuff intution, without the benefit of detailed and methodical analysis (which its capable of in principle).\n\n063. OhMyGoodness Says:\n\n Comment #63 [April 12th, 2023 at 3:49 am](https://scottaaronson.blog/?p=7209#comment-1949098)\n There was a guy in college that was pretty good mathematically (I think like a 30-40 on Putnam. I don’t know why but on a Complex Variables test his answer for the last problem was “F\\*\\*k You”. He didn’t receive partial credit but, as far as I know, a truly original response in the history of Complex Variables tests.\n\n064. Bruno Says:\n\n Comment #64 [April 12th, 2023 at 3:54 am](https://scottaaronson.blog/?p=7209#comment-1949099)\n Scott, any comment on Alex’s #37? GPT got full credit for a completely wrong answer 2(a) to the short-form questions. The density matrix it starts with is \\[1001,0110,0110,1001\\]/3, which does not have rank 1, and has trace 4/3. AFAICT tracing out the first qubit in this wrong density matrix should give (2/3)\\[1,0;0,1\\] but the result is yet something else. This should probably receive very partial credit, like 1/5 for knowing some of the steps to perform. Unless what it taken into account is only the final result, which is only slightly off: should be \\[2,1;1,1\\]/3 as far as I can tell.\n\n065. Shion Arita Says:\n\n Comment #65 [April 12th, 2023 at 6:05 am](https://scottaaronson.blog/?p=7209#comment-1949102)\n I’ve made some sweeping comments about these NN based AI systems over the past few posts here, hashing out my thoughts and position(which seem to be unique to me so far), and I’ve just now come to a further refinement of them.\n\n I think that LLMs like GPT4 specifically, and NN AI most generally, are not AGI, and by themselves problably won’t be, but they are very powerful, significant, and context-breaking systems that are the new big thing of the near future, and will transform the world as much as things like machinery, electricity, computers, and the internet did. They are very powerful optimizing objects that can do things that machines couldn’t do before. But no I don’t think they have true intelligence or understanding.\n\n The problem is, I don’t think there really exists a good single test of true intelligence or understanding. I don’t think it’s possible for a couple of reasons. Firstly, for any particular task you have, it seems like it’s possible to make a ‘narrow’ system that can perform that task. Secondly, I can’t think of a question that almost any human would get right and GPT4 would get wrong. On face, this might sound like an argument for the intelligence of GPT4, but it’s really a statement about the inadequacy of questioning, and in particular, the inadequacy of many typical humans. To illustrate the problem more clearly, some things I was thinking about for true understanding tests were things like “say something that is very surprising, but ends up being right,” or, “solves somehting in a way that no one else has ever solved it.” if it could do these things (which I don’t think it can), I might say it was intelligent, but many humans would fail on these criteria. In fact almost all humans would fail on these critera on an observation window short enough to be reasonable– even the smartest and weirdest person who has ever existed probably does not spend most of their time doing things that are profound. And most humans never do anything that’s really profound. Such a test can’t be simple; there is no simple thing which has no simple system that can do it, and there’s nothing non-profound that can’t be faked or precomputed in advance. So we’re kind of stuck. If we weren’t humans ourselves, and rather were aliens whose cognition was very different from our own, and we were given a typical human to study, without being able to see the achievements of humanity, I think we’d have a very hard time determining whether humans were truly intelligent or conscious or not. I think we only really know because we are able to directly observe our own consciousness, self-awareness, and thought process. and since we know that other humans are structured very similarly to ourselves, we know that they must be like this too. Take that symmetry via similarity away, and we’re kind of in the dark. Or at least, we’re left with having to do a lot of complicated tests, a lot of back and forth with the system, doing an evaluation like this is as complicated as anyhting can be, and thus will seem subjective with room for debate. As someone said replying to me in another thread, sometimes the goalpoasts really were in the wrong place the whole time.\n\n So what is GPT4 (and other similar systems), and how do I know that?\n\n It’s not a ‘stochastic parrot’ I don’t think. I know I said that about lamda after the Blake Lemoine thing, but I have changed my view in the intervening time after being able to actually use these systems myself and experiment on them in a serious way. It is something different, and I don’t think we have a good word for it. I tried to come up with one and failed, becuase we don’t really know what’s going on in there in the first place.\n\n I think the biggest mistake that people make with regards to NN AI systems is seeing the powerfully optimized results, seeing the complexity and capability of what it does, along with the msyerious nature of exactly what these things are learning in there and how they are doing what they do, and thinking things like, “it must actually understand things like quantum computing, because there’s no other way it could be doing this, getting these questions right.”\n\n I assert that “there’s no other way it could be doing this” is in fact false (!)\n\n Like so many things, This has been staring us in the face since the dawn of time, but it takes a specific insight to see the significance of what we were looking at. There are currently two strongly optimizing forces that are at play on earth: evolution by natural selection and human cognitive intelligence. Human-made electronic computers are rapidly rising as a third ‘great power’ but aren’t quite there yet. But let’s focus on the first power: evolution by natural selection.\n\n Humans first looked at the complex and heavily optimized objects they saw in the world; themselves and other organisms, saw the optimization and complexity there, and concluded, nay, took as axiomatic, that they had come into being via a force of deliberate intentionality, that was aware of the significance of what it was doing when it created them. This made sense, because, look at these things! look how optimized and purposeful they are. They must have been made by something that understood the meaning of what it was doing. There’s no other way it could have been done.\n\n Except, it in fact turns out that there IS another way it could have been done, and that’s exactly what happened. And that other way, evolution by natural selection, does not have any awareness or intentionality, and it does not understand the meaning of what it is doing. And yet, And yet \\*shockingly\\* I might add, here are these complex, optimized results showing apparent purposefulness. Darwin figured it out, and to his credit, he figured it out before we had the tools to observe in detail how biological systems actually were structured, and how they were working.\n\n In other words, we already know that this can happen. And I think it’s pretty clear that it did happen here, albeit in a different way.\n\n This also happened with things like chess with classical computation. In the past, people thought that being able to achieve top-human level performance at chess required a true AGI. When Deep Blue defeated Kasparov, the lesson learned was that this was incorrect. Belief in the significance of this was (the fact that it didn’t need to be intelligent, not the fact that it beat him) was strongly tempered by the fact that unlike biological systems, and unlike NN AI systems, us humans actually understood very well how Deep Blue really worked, what it was actually doing to achieve its esults, and why its algorithm would be effective at playing chess. The difference is that with things like biological systems and NN AI systems, their inner workings are not readily comprehensible by humans, and thus there is a lot of room for ‘god of the gaps’ or ‘AGI of the gaps’ type of arguments to creep in.\n\n Before protein structures were elucidated via x-ray crystallography, it was widely believed that they would be very regular, highly symmetrical structures with clockwork like elegance in form and function. When the first structure was revealed, it turns out they are random-seemeing spaghetti curves. Max Perutz, who elucidated this first strucutre said of his descovery, “Could the search for ultimate truth really have revealed so hideous and visceral looking an object?”\n\n Is natural selection ‘intelligent’? I have to say no. And that’s because even though what it does is heavily optimized, and complicated, it doesn’t understand the meaning of what it does, or furthermore, there even is no ‘it’ there to even have the potential do the understanding. It’s optimization without an optimizer. In an earlier post I said, about the AI, that we’re in the Blindsight timeline, but haven’t we kind of been in the Blindsight timeline this whole time?\n\n I think that we will see these non-intelligent but strongly optimizing AI systems continue to do more and more surprising things, things previously believed to require true AGI, while remaining lacking in certain areas (for example, areas where it needs to perform an exact sequence of computational steps to achieve a very specific result, areas where it has to reproduce complicated experimental results that came from our interaction with the outside world, and areas where it needs to innovate and decide what is or is not significant). For example, as Nick Drozd #22 said: “Also, have you tried using ChatGPT to generate exam questions? Can you answer all of its questions?” I wrote the following as my advance prediction after seeing Nick’s comment and before seeing Scott’s attempt: “This is the kind of thing where I expect it would kind of break down. I predict that it will create a mixture of ‘generic/trivial’ questions and ‘flawed/bad’ questions.” I think my prediction was pretty much right.\n\n I think that these NN AI systmes are not like natural selection in a lot of ways, and they’re also not like traditional computation, and they’re ALSO not like cognitive intelligence. And all of these things are separated by vast distances in the space of what systems can be like. They’re somehting else, something pretty far away from anything else, but maybe along a vector halfway between things like natural selection and traditional computing. I’m not really sure though.\n\n Why do I think that these systems’ limitations are meaningul when compared to humans? These NN AI systems work by being able to exploit certain regularities in the training data(you could argue that any optimization has to exploit patterns in data but I think humans are much more deeply general at this than things like gradient descent can be. These large training runs are brute forcing the learning in a way that humans have a real efficiency), and they way they exploit them I suspect would seem kind of dumb and inefficient if a human could do all the bookkeeping required to understand what it was really doing.\n\n In conclusion, I think the lesson to learn here is basically ‘oh, it didn’t need to be intelligent to do these amazing things. But there are lots and lots of amazing things that you don’t need to be intelligent to do, and this is surprising, but it seems true. So we should expect this trend to continue; more and more results without the intelligence behind them’. I don’t think I’m using some weird definition play on the word ‘intelligence’ because I think there’s something concrete and real to intelligence which is hard to describe verbally but has a lot to do with vague concepts like ‘understanding’ and ‘meaning’.\n\n066. Prasanna Says:\n\n Comment #66 [April 12th, 2023 at 7:21 am](https://scottaaronson.blog/?p=7209#comment-1949103)\n What was revealing from Sebastien Bubeck’s talk at MIT was that the publicly available GPT-4 is a dumbed down version due to safety reasons, and he also mentioned the base model is more powerful. This raises several questions\n\n 1\\. Is GPT-4 a B and not A due to this ? If not, do we know of any reasons to believe that ?\n\n 2\\. What has GPT-4 truly achieved beyond what is “visible” to the public ?\n\n 3\\. Are the current limitations like calculations errors and others due to this safety considerations constraining GPT-4 in some form ?\n\n 3\\. Looks like few people even within OpenAI will have access to this powerful model, notwithstanding even more powerful models of GPT-5 in development ?\n\n 4\\. Note that top executive team of OpenAI is liberally dropping the word AGI in their public talks , and even talking about getting the society “gradually” ready for it ?\n\n067. fred Says:\n\n Comment #67 [April 12th, 2023 at 7:26 am](https://scottaaronson.blog/?p=7209#comment-1949104)\n Hurrah to OpenAI!\n\n But it’s quite puzzling that there’s no consideration in this post on what this means for the “students” who are only mentioned as a mere benchmark against the AI.\n\n As much as Feynman’s “nobody understands quantum mechanics!” has been a confidence boost for generations of students, this is quite the extreme opposite!\n\n But, rejoice, soon enough the AI will no longer be just a toy that answers some evaluation tests but it will create and manage the tests itself… (e.g. what’s the point for Amazon to stick with its current hiring testing protocol when the AI can do better than all humans in a fraction of the time?)\n\n It won’t take long for OpenAI to offer a super affordable alternative to the university path, allowing students to study with the AI, be evaluated by the AI, encouraged by the AI (24/7, with infinite patience), and be rewarded by the AI with a shiny diploma.\n\n The death of student debt is finally in sight!\n\n068. Matt Says:\n\n Comment #68 [April 12th, 2023 at 7:30 am](https://scottaaronson.blog/?p=7209#comment-1949105)\n I disagree with the 5/5 mark for Q2a. GPT’s $\\\\rho$ and $\\\\rho\\_2$ are not even a density matrix (they have trace 4/3).\n\n069. [Sabine](http://sabinehossenfelder.com) Says:\n\n Comment #69 [April 12th, 2023 at 8:54 am](https://scottaaronson.blog/?p=7209#comment-1949106)\n I’m just here to complain that a vector doesn’t have a dimension, its vector space has.\n\n070. [Tomislav](https://tmilinovic.wordpress.com/) Says:\n\n Comment #70 [April 12th, 2023 at 8:59 am](https://scottaaronson.blog/?p=7209#comment-1949108)\n Hello, I’m just a beginner in quantum computing so please don’t spit on me if I’m wrong, but my experience is a bit weirder, which I described in my blog.\n\n [https://tmilinovic.wordpress.com/2023/04/08/dazed-and-confused/](https://tmilinovic.wordpress.com/2023/04/08/dazed-and-confused/)\n\n071. [space2001](http://www.space2001.com) Says:\n\n Comment #71 [April 12th, 2023 at 9:17 am](https://scottaaronson.blog/?p=7209#comment-1949109)\n So far so good artificial intelligence imitators..\n\n Can ChatGPT-N ever invent quantum mechanics? No? I did not think so either 🙂\n\n072. Raoul Ohio Says:\n\n Comment #72 [April 12th, 2023 at 9:22 am](https://scottaaronson.blog/?p=7209#comment-1949110)\n OhMyGoodness #63:\n\n He was evidently a Fortran programmer.\n\n073. fred Says:\n\n Comment #73 [April 12th, 2023 at 9:45 am](https://scottaaronson.blog/?p=7209#comment-1949112)\n It’s striking that we still have a very poor understanding of how the human brain actually works, but that didn’t stop us from emulating it using a system that’s just as inscrutable!\n\n Is this a coincidence?\n\n074. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #74 [April 12th, 2023 at 9:49 am](https://scottaaronson.blog/?p=7209#comment-1949113)\n space2001 #71: Could _you_ have invented quantum mechanics? I didn’t think so either 🙂\n\n075. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #75 [April 12th, 2023 at 9:54 am](https://scottaaronson.blog/?p=7209#comment-1949114)\n Sabine #69: My entire professional life, people have without confusion used “dimension of a vector” to refer to the dimension of the relevant vector space in which the vector resides. (Also known as: how many scalars you need to represent this vector in your computer’s memory.) Were they not supposed to do that, maybe not unless the vectors came from the Dimension region of France? 🙂\n\n076. OhMyGoodness Says:\n\n Comment #76 [April 12th, 2023 at 10:11 am](https://scottaaronson.blog/?p=7209#comment-1949115)\n Raoul#72\n\n Actually an ME (I just checked and he had a triple Bachelors ME, Math and Comp Sci) and after graduation he did write geophysical interpretation software for oil and gas exploration and did well I believe. He was originally from Ohio so you have that apparently in common.\n\n077. OhMyGoodness Says:\n\n Comment #77 [April 12th, 2023 at 10:39 am](https://scottaaronson.blog/?p=7209#comment-1949117)\n Raoul#72\n\n Did you know this story?\n\n078. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #78 [April 12th, 2023 at 10:49 am](https://scottaaronson.blog/?p=7209#comment-1949118)\n fred #73:\n\n\n It’s striking that we still have a very poor understanding of how the human brain actually works, but that didn’t stop us from emulating it using a system that’s just as inscrutable!\n\n Is this a coincidence?\n\n\n Not a coincidence! When people understand how something works, they don’t think of it as “AI.”\n\n (Interesting corollary: when skeptics say that GPT is not a “true AI,” is the issue that they _imagine_ they understand it much better than they do? I.e., that they say “oh, it’s just a glorified probabilistic autocomplete,” but have never grappled with the enormity of the internal representations of real-world concepts you’d need to have learned in order to do autocomplete _this well_?)\n\n079. Qwerty Says:\n\n Comment #79 [April 12th, 2023 at 10:55 am](https://scottaaronson.blog/?p=7209#comment-1949119)\n What does this mean for us all?! What can GPT still NOT do?\n\n080. fred Says:\n\n Comment #80 [April 12th, 2023 at 11:11 am](https://scottaaronson.blog/?p=7209#comment-1949120)\n Qwerty #79\n\n _“What can GPT still NOT do?”_\n\n it doesn’t always do well when dealing with new “languages” (like rules of string manipulation, proto computer languages) it wasn’t trained on, no matter how simple they are. But I guess here it’s a matter of training it more so that it gets better at generalization and pure logic.\n\n Also if you ask it to do some reasoning based on some random pick that it can’t write down explicitly, it struggles to basically “remember” what its pick was consistently (because it’s all implicit in the prompt and since it’s all probabilistic, it’s not always re-deriving his pick correctly). Which is why it struggles to reason on or manipulate its own answer before it writes it down (like reversing strings).\n\n That’s also why it always help to ask it to break down any reasoning into very granular steps it writes down in the prompt, this anchors the whole chain of reasoning way better. We humans do the same when we reach for a notepad to anchor our thinking, especially when we’re exploring/being creative.\n\n081. fred Says:\n\n Comment #81 [April 12th, 2023 at 11:21 am](https://scottaaronson.blog/?p=7209#comment-1949122)\n For noobs like me who may be confused by the bickering/trolling from Sabine #69 about vectors, vector spaces, dimension, etc.\n\n [https://youtu.be/eZzBK3oy-08?t=1770](https://youtu.be/eZzBK3oy-08?t=1770)\n\n082. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #82 [April 12th, 2023 at 11:22 am](https://scottaaronson.blog/?p=7209#comment-1949123)\n Qwerty #79: We’ve already seen examples in this thread of things that GPT still can’t do. E.g., it flubbed many of the harder problems on this exam, and it seemed unable to generate its own halfway decent exam (even when I specifically asked it to invent more creative problems).\n\n The harder question is what it still won’t be able to do a couple years from now—when there will not only be much more powerful models, but also much more seamless integration with the Internet and tools (internal scratchpads, etc) to help remedy all of its current weaknesses.\n\n083. [Raghu Parthasarathy](https://eighteenthelephant.com/) Says:\n\n Comment #83 [April 12th, 2023 at 11:24 am](https://scottaaronson.blog/?p=7209#comment-1949124)\n I had GPT3.5 (not even 4!) take my “Physics of Energy and the Environment for non-scientist” final exam; it ranked 22 out of 80, and did better on short answer than multiple choice questions!\n\n I just posted about this here, [https://eighteenthelephant.com/2023/04/12/chatgpt-takes-my-energy-and-the-environment-final-exam/](https://eighteenthelephant.com/2023/04/12/chatgpt-takes-my-energy-and-the-environment-final-exam/) , and I also comment on how GPT-x may be useful for assessing exam quality, revealing ambiguous question through the rare things it gets wrong.\n\n084. Steven Says:\n\n Comment #84 [April 12th, 2023 at 11:46 am](https://scottaaronson.blog/?p=7209#comment-1949125)\n Scott #82\n\n What is the current scaling limits using the current AI techniques (neural nets at its core)? For example, if we need to increase by 2 or 3 orders of magnitude the parameters to ace all reasonable exams, how does the compute and energy resources scales? I’ve heard it is quadratic in terms of the parameter size. Also how does the amount of data scale as well to increase performance (if we know)?\n\n In terms of using other tools, I’m really interested see how that can enhance things. Has some of this stuff been formalized in attempt to project out or are we more in “extremly hard to prove, let’s just empirically see what happen”\n\n085. [Gil Kalai](https://gilkalai.wordpress.com/) Says:\n\n Comment #85 [April 12th, 2023 at 11:46 am](https://scottaaronson.blog/?p=7209#comment-1949126)\n Amazing post!\n\n It would have been nice to see if GPT can think on its own (or with some useful prompts) about surprising connections like the connection between computation and quantum mechanics. (E.g. could GPT invented quantum computation had it existed in 1970?)\n\n There are quite a few connections in science and mathematics that after you see them they seem easy and sometimes only involve making a connection between two separated areas. It is not unlikely that GPT would be good in offering such connections.\n\n086. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #86 [April 12th, 2023 at 11:56 am](https://scottaaronson.blog/?p=7209#comment-1949127)\n Alex #37, Bruno #64, and Matt #68: You’re absolutely right—and while Justin graded 2(a), I take responsibility for the oversight and am sorry. I’ve downgraded GPT to 1/5 for that problem, same as I’d give a student who set up the right kind of calculation but then produced a totally wrong answer. This decreases the exam score from 73/100 to 69/100, which is still a B but no longer a “solid” B (some more lost points could move it to B-).\n\n Any other regrade requests? 😀\n\n087. JimV Says:\n\n Comment #87 [April 12th, 2023 at 11:56 am](https://scottaaronson.blog/?p=7209#comment-1949128)\n Shion Arita Says: “But no I don’t think they have true intelligence” (Comment #65)\n\n As I commented previously, neuroscience has working definitions of intelligence, sapience, and sentience, of which GPT-4 is thought to have demonstrated intelligence conclusively, but not the other two. I suspect you are using your own semantics which combines the three, which is natural, but leads to semantic arguments.\n\n088. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #88 [April 12th, 2023 at 12:57 pm](https://scottaaronson.blog/?p=7209#comment-1949129)\n Gil Kalai #85: I was just thinking about how I’d **love** to interact with a language model that was trained, let’s say, only on text before the year 1970. There are so many recent developments to which one could ask for its reactions—not _quite_ like having a time machine, but some of the same functionality.\n\n That said, how exactly would we check whether such an LLM could “invent quantum computation”? Would we prompt it with the basic idea, and then see if it could take it any further?\n\n089. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #89 [April 12th, 2023 at 1:03 pm](https://scottaaronson.blog/?p=7209#comment-1949130)\n Steven #84: It’s complicated. Keeping the training data fixed, you can vary (among many other things) the depth of your neural net, the width, and the number of steps of gradient descent. Alas, we don’t really know theoretically how _any_ of this stuff affects the real-world performance: almost everything we know is empirical. And then there’s an algorithmic issue specific to transformer models, which is that their runtime scales quadratically with the length of the context window. A linear-time algorithm, if discovered—even one that just approximately preserved the performance—would make it much easier to have GPT, e.g., ingest a whole book at execution time.\n\n090. fred Says:\n\n Comment #90 [April 12th, 2023 at 1:09 pm](https://scottaaronson.blog/?p=7209#comment-1949131)\n For humans, “Attention and Sleep is All You Need”.\n\n So, extrapolating on the similarities between the human brains and LLMs, I wonder if there could be any connection between dreaming and LLMs hallucinations. 🙂\n\n It’s interesting to note that severe sleep deprivation in humans leads to hallucinations and interesting cognitive changes:\n\n “On the eleventh day, when he was asked to subtract seven repeatedly, starting with 100, he stopped at 65. When asked why he had stopped, he replied that he had forgotten what he was doing”\n\n from\n\n [https://en.wikipedia.org/wiki/Randy\\_Gardner\\_sleep\\_deprivation\\_experiment](https://en.wikipedia.org/wiki/Randy_Gardner_sleep_deprivation_experiment)\n\n091. Hans Holander Says:\n\n Comment #91 [April 12th, 2023 at 1:33 pm](https://scottaaronson.blog/?p=7209#comment-1949133)\n Yes, GPT-4 has absorbed the entire internet and can answer similar questions reasonably well, including common mistakes. Then you ask novel questions and it gets 4 out of 90 points, and the professor notes: “The AI had no problem reading the equations; all of its errors are due to fundamental misunderstandings of basic concepts.” [https://www.thebigquestions.com/2023/04/05/gpt-4-fails-economics/](https://www.thebigquestions.com/2023/04/05/gpt-4-fails-economics/)\n\n Time to see LLMs for what they are. Namely, LLMs. 🙂\n\n s/ Hans Holander\n\n092. fred Says:\n\n Comment #92 [April 12th, 2023 at 1:37 pm](https://scottaaronson.blog/?p=7209#comment-1949134)\n When asking GPT to do that task of “subtract seven repeatedly, starting with 100”, it takes a lot of contortion to get it to finally print it only in reverse order (something that’s near impossible for it to do correctly, obviously).\n\n —————-\n\n Me:\n\n subtract 7 from 100, repeatedly, but don’t go negative.\n\n Also, write the resulting list in reverse order only, i.e. starting with the smallest number, then the one that’s 7 bigger, etc until you reach 100.\n\n Absolutely do not write any intermediate steps and do not write the result list in decreasing order, ever!\n\n GPT:\n\n Sure, I can help you with that!\n\n Here is the resulting list in the order you requested:\n\n 1, 8, 15, 22, 29, 36, 43, 50, 57, 64, 71, 78, 85, 92, 99, 100\n\n To obtain this list, you start with 100, subtract 7 to get 93, then subtract 7 again to get 86, and so on, until you get to 1. Since we are not allowed to write any intermediate steps, I can only give you the final list in reverse order. But I assure you that this is the correct sequence of numbers that you would get by subtracting 7 from 100 repeatedly, without ever going negative.\n\n ———————-\n\n It’s quite strange it’s insisting it has the right result with “I assure you…”, almost as if it knows it’s wrong.\n\n It’s so much like one of the lines of HAL-9000 in 2001 a Space Odyssey:\n\n _“I know everything hasn’t been quite right with me, but I can assure you now, very confidently, that it’s going to be all right again. I feel much better now. I really do. “_\n\n093. Nobozo Says:\n\n Comment #93 [April 12th, 2023 at 1:37 pm](https://scottaaronson.blog/?p=7209#comment-1949135)\n Scott,\n\n I remember the scandal caused when a paper written by SCIgen, which is a program “that uses context-free grammar to randomly generate nonsense in the form of computer science research papers.” (from Wikipedia) was accepted by a scientific conference.\n\n Just for yuks, I took the time to look at the conference announcement for this conference. I don’t remember the details, but I do remember feeling that it also was generated by a program, but this time one that uses context-free grammar to randomly generate nonsense in the form of computer science conference announcements.\n\n I wonder if this example of dueling AI programs will become more common in the future?\n\n094. Shion Arita Says:\n\n Comment #94 [April 12th, 2023 at 2:45 pm](https://scottaaronson.blog/?p=7209#comment-1949136)\n @jimv# 87\n\n I did think about whether I was having issues with definitions, and I don’t think I am, or at least, not more than anything else. GPT has \\*something\\* signficant, in that it can produce complex outputs and things like answers to questions. But what I was getting at is that a lot of processes can do things like this. Is natural selection intelligent? is stockfish intelligent? I think the only consistent thing to say is that either both of those plus GPT are intelligent, or all of them are not. And I think that most people would say ‘no’ on the first 2, so I think we need to say no on the 3rd as well.\n\n re: understanding and meaning/ the meaning of intelligence.\n\n I think an example might be in order here.\n\n Recently, I needed to write a computer program to do a simple mathematical calculation: to take an input number from 0 to 99, and output the probability that the input number is less than the average of two random numbers from 0 to 99. After I wrote the program, I thought “I wonder if GPT can write this program.” So I asked it to, and it did it!\n\n my prompt was: “write a C progrma that takes an input number from 0 to 99, and outputs the probability that the input number is less than the average of two random numbers from 0 to 99.”\n\n Here is my program:\n\n `\n #include`\n `int main(){\n float num;\n float a;\n float outputProb;Mainloop:;printf(\"input: \");\n scanf(\"%f\", &num);if (num < 50){\n a = 2 * num;\n outputProb = (a * (a + 1)) / 200;}else{\n a = (2 * (100 - num)) - 1;\n outputProb = 100 - ((a * (a + 1)) / 200);}`\n `printf(\"probability: %.2f\", outputProb);\n printf(\"\\n\");`\n\n `goto Mainloop;\n }\n `\n\n Here is GPT’s program (NOTE: the GPT code is edited somewhat to match mine with things like variable names, code style, and some formatting. the meaningful content is the same as what GPT put out though):\n\n `\n #include`\n `int main(){\n int num;\n int count;\n int i, j;Mainloop:;count = 0;printf(\"input: \");\n scanf(\"%d\", &num);for (i = 0; i <= 99; i++) {\n for (j = 0; j <= 99; j++) {\n if ((i + j) / 2.0 Yes, GPT-4 has absorbed the entire internet and can answer similar questions reasonably well, including common mistakes. Then you ask novel questions and it gets 4 out of 90 points, and the professor notes: “The AI had no problem reading the equations; all of its errors are due to fundamental misunderstandings of basic concepts.” [https://www.thebigquestions.com/2023/04/05/gpt-4-fails-economics/](https://www.thebigquestions.com/2023/04/05/gpt-4-fails-economics/)\n\n My main takeaway from this post is that economics professors can pass or fail students entirely at their whim. Though I’d be shocked if this guy actually grades students’ tests this way.\n\n108. flergalwit Says:\n\n Comment #108 [April 13th, 2023 at 1:54 am](https://scottaaronson.blog/?p=7209#comment-1949158)\n Is there a good exposition somewhere that answers the H’=2^100 H question in detail? I understand intuitively H’ should take 2^100 times as much energy to run as H, but is there a way of quantifying the amount of energy each takes (including when H=H(t) is time dependent)? Thanks.\n\n109. DR Qwerty Says:\n\n Comment #109 [April 13th, 2023 at 2:12 am](https://scottaaronson.blog/?p=7209#comment-1949160)\n Does GPT use only brute force to answer the quantum computing test ?\n\n110. [Josh Jordan](https://twitter.com/jordancurve) Says:\n\n Comment #110 [April 13th, 2023 at 2:56 am](https://scottaaronson.blog/?p=7209#comment-1949161)\n Fred #92: Not sure which GPT you were using, but GPT-4 gets it on the first try ( [https://shareg.pt/MADk8ym](https://shareg.pt/MADk8ym)):\n\n \\> Alright, here’s the list in reverse order, as you requested, starting with the smallest number and increasing by 7 until reaching 100:\n\n >\n\n \\> 2, 9, 16, 23, 30, 37, 44, 51, 58, 65, 72, 79, 86, 93, 100\n\n111. Smutny Says:\n\n Comment #111 [April 13th, 2023 at 3:09 am](https://scottaaronson.blog/?p=7209#comment-1949162)\n Speaking of the course, is it true that you have plans to turn the lecture notes (the two halves of which I have combined for self-study) into a textbook? If so, how is it coming along? First time commenting, thanks!\n\n112. Shion Arita Says:\n\n Comment #112 [April 13th, 2023 at 3:15 am](https://scottaaronson.blog/?p=7209#comment-1949163)\n @ExPhysProf #103.\n\n I see your point that I suppose I didn’t need to spend time doing the derivation, but in doing it I gained understanding about the problem, and ultimately I think that derived solutions have more “meaning” than “elegance”, and the main point I was getting at is that GPT couldn’t write the program like I did, and that this kind of thing\n\n I did some GPT experiments, asking for things like an ‘elegant’ solution or a ‘closed-form’ solution, etc. both with session context and without. I won’t bore everyone with all the outputs but in general the behaviors I saw were either more of the same, or it getting confused and outputting wrong programs.\n\n As a final test, which had an interesting result, I asked it to write the code as if it were a mathematician. This is what I got (edited for brevity and clarity)\n\n `\n As a mathematician, I would approach this problem by first determining the total number of possible pairs of numbers that can be chosen from 0 to 99. This can be calculated using the formula n(n+1)/2, where n is the maximum number in the range (in this case, n=99). So there are 4950 possible pairs of numbers that can be chosen from 0 to 99.`\n `Next, I would determine the total number of pairs of numbers whose average is less than the input number. This can be done by iterating through all possible pairs of numbers and counting the number of pairs whose average is less than the input number.Here is the C program that implements this approach:#includeint main() {\n int input, count = 0;\n float total = 0;printf(\"Enter a number between 0 and 99: \");\n scanf(\"%d\", &input);for (int i = 0; i <= 99; i++) {\n for (int j = i; j <= 99; j++) {\n total++;\n if ((i+j)/2.0 since we know that other humans are structured very similarly to ourselves, we know that they must be like this too\n\n So, if you were to find out that the artificial brain are structured very similarly to our brain, would you conclude they must be like us too? I guess the following links will make you ponder what counts as « very »:\n\n [https://www.nature.com/articles/s41598-020-59175-0](https://www.nature.com/articles/s41598-020-59175-0)\n\n Scott #98, don’t you have the third option to have a benevolent comitee decide? You’re a smart fellow, think something something Ricardo & comparative advantage.\n\n114. fred Says:\n\n Comment #114 [April 13th, 2023 at 7:50 am](https://scottaaronson.blog/?p=7209#comment-1949167)\n Josh Jordan #110\n\n Thanks!\n\n I honestly don’t know and can’t tell for sure what version OpenAI is serving me. Last year I signed up for ChatGPT access and was being charged for it (same with Dall-E), then they no longer charged me, and now I’m just going to\n\n [https://openai.com/product/gpt-4](https://openai.com/product/gpt-4)\n\n and all I see in there is “Try on ChatGPT Plus” (what exactly is “ChatGPT Plus”?! GPT4?!… are these two separate “products”?) which gets me to a prompt UI that only shows ChatGPT in his welcome screen (on the other hand your screen capture clearly says “GPT4” at the top)… haha\n\n Ignoring all this, the result you got is super impressive!\n\n115. Hans Holander Says:\n\n Comment #115 [April 13th, 2023 at 8:56 am](https://scottaaronson.blog/?p=7209#comment-1949168)\n @Vladimir 107: Yes, the grading is somewhat arbitrary, but it is still obvious that GPT-4 made very basic mistakes over and over again, probably because these were highly non-standard, non-literature questions (contrary to Kaplan, who used mostly literature questions).\n\n The most impressive example though still is the Codeforces programming test: GPT solved 10/10 tasks from the last week of its training period, but 0/10 tasks of the easiest category from the first week after its training period had ended.\n\n So yes, GPT has absorbed orders of magnitude more data than any human being, and it is much faster than any human being, but it still lacks true reasoning ability.\n\n But in terms of standard programming tasks, GPT obviously excels. This is not really surprising, as programming uses a perfectly logical, strict and predictable language.\n\n Also, I believe GPT will be able to extract “hidden knowledge” and patterns from the totality of human-created data, if you are able to ask the right questions.\n\n116. Vadim Says:\n\n Comment #116 [April 13th, 2023 at 9:14 am](https://scottaaronson.blog/?p=7209#comment-1949169)\n How far are we from “continuously-learning” LLM’s that immediately integrate their conversations into their training data instead of just having a temporary context window? I feel (without any justification and with no ML training whatsoever, so take my feeling with a boulder of salt) that once LLM’s can do that, we’ll see yet another conceptual jump in their capabilities. I don’t care how “intelligent” someone is, having the short-term memory of a goldfish is a major limit. Imagine for the “taking an exam test” if you could correct the exam, return it, and the AI would learn from its mistakes and do better next time.\n\n117. fred Says:\n\n Comment #117 [April 13th, 2023 at 12:38 pm](https://scottaaronson.blog/?p=7209#comment-1949172)\n (hope this is not too much off-topic, Scott feel free to block it obviously)\n\n [iframe](https://www.youtube.com/embed/VcVfceTsD0A?version=3&rel=1&showsearch=0&showinfo=1&iv_load_policy=1&fs=1&hl=en-US&autohide=2&wmode=transparent)\n\n118. Nate Says:\n\n Comment #118 [April 13th, 2023 at 1:16 pm](https://scottaaronson.blog/?p=7209#comment-1949173)\n Ilio #113\n\n I assume you read that paper before you posted it. They are making some broad claims of similarity of the ‘correlational’ information of how a neural network’s layers represents shape versus category information and how some of the cortical regions involved in visual processing do the same.\n\n However they are not claiming:\n\n 1) The activity in the NN leading to the computation is the same as the biophysics of the cells producing the related computation.\n\n 2) How this data relates to the actual computational usage of that shape and category information might match between an NN and a human brain. They even explicitly say:\n\n “This finding suggests that there is at least some correspondence between how humans and models use shape, even though there are very likely also differences”\n\n Which backs off a lot from their much stronger language about the raw computational correlations between the NN activity an cortical activity.\n\n Reading from that I think its very interesting that there are such computational similarities but also that ‘of course’ there are as we are talking about computing the same information in two networked computational architectures. It is definitely fascinating that you can get similar computation at all though from a couple NN architectures (convolutional and feedforward) paired together. Not enough for me to go ‘that makes them so similar as to be human’ though 😉\n\n This is an impressive paper and deserves to be in Nature but it is also (not uniquely or maliciously) leading with the message that I think the authors knew would garner the most ‘wow’ factor (that is how research writing works). You shouldn’t take that to mean that they are suggesting anything like ‘the visual cortex does very similar computations to a neural network at all levels’. They also point out that there are some very obvious, and probably important, differences in the point at which the information for the shape correlations converges in the NNs (early) versus the brain (at the final V1 cortical layer).\n\n By the end of their conclusions they really only strongly state that the order of the processing is strongly similar, and that the stable representations of both shape and category across different network layer groups is similar. Very impressive and interesting, but I feel it is a bit too generic to say it bears on \\`other humans are structured very similarly to ourselves\\`.\n\n Thanks for posting a neuroscience paper to go with your post! It was a good read.\n\n119. Richard Cleve Says:\n\n Comment #119 [April 13th, 2023 at 1:46 pm](https://scottaaronson.blog/?p=7209#comment-1949174)\n Regarding the “(e) Extra Credit” question about physically applying the Hamiltonian $H’:=10^{100}H$ instead of $H$ in order to get time speedup your answer about the higher energy resources required seems intuitively right to me; however, I do not know how this is established.\n\n Is this just taken as an axiom? Or is it possible to derive it from some other principles of physics? I’d be interested is something more than just declaring that it’s obvious.\n\n120. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #120 [April 13th, 2023 at 2:00 pm](https://scottaaronson.blog/?p=7209#comment-1949176)\n Opt #105:\n\n\n Does few shot prompting retain its effectiveness even with an RLHF model which can follow instructions better w/o complex prompting?\n\n\n I think it still helps to show GPT examples of the sort of behavior you expect from it, yes. Does anyone know of recent data on this question?\n\n121. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #121 [April 13th, 2023 at 2:16 pm](https://scottaaronson.blog/?p=7209#comment-1949177)\n Vladimir #107: Yeah, Landsburg never reports what the average on this test was among human students! And if you look on Twitter, you can find other econ folks arguing with what he asserts are the straightforwardly correct answers to his questions, and in some cases agreeing with GPT’s answers (I won’t attempt my own judgment).\n\n122. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #122 [April 13th, 2023 at 2:21 pm](https://scottaaronson.blog/?p=7209#comment-1949178)\n flergalwit #108 and Richard Cleve #119: I believe the resolution of these questions is just that in physics, the Hamiltonian _is_ energy—or rather, its eigenvalues are the energy levels, and ⟨ψ\\|H\\|ψ⟩ is the expected energy of \\|ψ⟩. So multiplying H by a scalar must scale the energy by the same amount.\n\n123. JimV Says:\n\n Comment #123 [April 13th, 2023 at 2:23 pm](https://scottaaronson.blog/?p=7209#comment-1949179)\n I wonder if a follow-up prompt, asking GPT-4 to check its previous answer, would help? If so, that could be a programmed option. If done repeatedly, maybe some sort of confidence index could be assessed from the multiple results.\n\n Checking is a necessity. I note even the grading needed to be checked in this case. Einstein: “All mathematicians make mistakes. Good mathematicians find them.”\n\n124. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #124 [April 13th, 2023 at 2:27 pm](https://scottaaronson.blog/?p=7209#comment-1949180)\n DR Qwerty #109:\n\n\n Does GPT use only brute force to answer the quantum computing test ?\n\n\n You’d have to carefully define the term “brute force” before I could answer yes or no.\n\n But no, GPT is not just looking up the answers in some huge database—indeed, a database large enough to contain all _possible_ questions, or even just all plausible questions for a quantum computing exam (including all possible variations in wording, etc.), might be larger than the observable universe. By “reading” the entire Internet over and over in its training phase, GPT did something that, if a person had done the same, we would’ve called “learning.”\n\n125. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #125 [April 13th, 2023 at 2:28 pm](https://scottaaronson.blog/?p=7209#comment-1949181)\n fred #117: In the future, **please** at least provide a text description whenever you’re linking to a video, because I can’t see the video previews when I’m deciding whether to accept a comment. Thanks!\n\n126. Richard Cleve Says:\n\n Comment #126 [April 13th, 2023 at 2:45 pm](https://scottaaronson.blog/?p=7209#comment-1949182)\n Scott #122: Thanks for your answer. I don’t think that the explanation can be so simple. Suppose that the initial state (before evolving by \\\\(H\\\\) or \\\\(H’\\\\)) is \\\\(\\|\\\\psi\\_0\\\\rangle\\\\) and the state after evolving by \\\\(H\\\\) is \\\\(\\|\\\\psi\\_1\\\\rangle = e^{-iHt}\\|\\\\psi\\_0\\\\rangle\\\\). Then, since \\\\(e^{-iHt}\\\\) commutes with \\\\(H\\\\), we have $$\\\\langle\\\\psi\\_1\\|H\\|\\\\psi\\_1\\\\rangle = \\\\langle\\\\psi\\_0\\|e^{iHt}He^{-iHt}\\|\\\\psi\\_0\\\\rangle = \\\\langle\\\\psi\\_0\\|H\\|\\\\psi\\_0\\\\rangle.$$ So the notion of energy that you refer to does not change when evolving by \\\\(H\\\\).\n\n127. fred Says:\n\n Comment #127 [April 13th, 2023 at 2:49 pm](https://scottaaronson.blog/?p=7209#comment-1949183)\n On a practical level, I hear many people say that OpenAI GPTn (n >= 4) shouldn’t be open source. Meaning that the tech is already too powerful to be out in the wild.\n\n But then, at a minimum, shouldn’t there be some sort of external audit/oversight to minimize the chance that OpenAI be the victim of corporate espionage, hacking, etc?\n\n Being bleeding edge in the field of AI isn’t correlated with strong expertise in security.\n\n128. fred Says:\n\n Comment #128 [April 13th, 2023 at 3:00 pm](https://scottaaronson.blog/?p=7209#comment-1949185)\n Scott #125\n\n Ok, my bad, I promise I will.\n\n129. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #129 [April 13th, 2023 at 3:07 pm](https://scottaaronson.blog/?p=7209#comment-1949186)\n Richard Cleve #126: I didn’t say that the energy was _changing with time_ (e.g., by energy being pumped into the system). Just that the “energy of the system” depends not only on \\|ψ⟩ but also on H, and if we replace H by 10H then the energy is multiplied by 10.\n\n What makes the discussion so confusing is that _ultimately_ energy should be a property only of a state, with the Hamiltonian fixed by the laws of physics of our universe—but to make that explicit, we need to pass to a larger system in which our qubits are immersed. And if we care about absolute energy, rather than _differences_ in energy, ultimately gravity is the only known thing that gives the former any clear definition. But maybe we should let any physicists reading this thread take over before one or both of us make fools of ourselves! 😀\n\n130. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #130 [April 13th, 2023 at 3:15 pm](https://scottaaronson.blog/?p=7209#comment-1949187)\n EVERYONE: There’s already a [parody article out](https://thequantuminsider.com/2023/04/13/chatgpt-4-receives-b-on-scott-aaronsons-quantum-information-science-final-immediately-emails-the-dean-seeking-a-better-grade) about how GPT emailed the dean at UT Austin asking for a higher grade on this exam, in recognition of how hard it worked.\n\n More seriously, a couple days ago Justin Yirka (my TA) had the fabulous idea to feed GPT-4 sections of this blog post and ask whether it wants to argue for a higher grade, and if so on what basis. Let me try that out tonight (unless someone else can do it first)!\n\n131. starspawn0 Says:\n\n Comment #131 [April 13th, 2023 at 5:28 pm](https://scottaaronson.blog/?p=7209#comment-1949190)\n Matthew Barnett wants to make a bet with Steve Landsburg. He made a similar bet with Bryan Caplan and looks on course to collect. His new offer:\n\n [https://twitter.com/MatthewJBar/status/1646261872830844928](https://twitter.com/MatthewJBar/status/1646261872830844928)\n\n Quote: ” “I think my students can stop worrying that their hard-won skills and knowledge will be outstripped by an AI program anytime soon.”\n\n Will Steve Landsburg put his money where his mouth is? I’m happy to bet him that an AI will score As on his exams before 2028 >75% of the time. “\n\n132. [Roger Schlafly](https://blog.darkbuzz.com/) Says:\n\n Comment #132 [April 13th, 2023 at 8:18 pm](https://scottaaronson.blog/?p=7209#comment-1949191)\n Max Tegmark [believes](https://www.theguardian.com/books/2014/jan/31/our-mathematical-universe-max-tegmark-review): “Our reality isn’t just described by mathematics – it is mathematics … Not just aspects of it, but all of it, including you.” He says we are just math with no free will.\n\n Scott [argues that it is possible](https://www.scottaaronson.com/papers/giqtm3.pdf) that we all have “quantum pixie-dust left over from the Big Bang, which gets into our brains and gives us the capacity for free will”.\n\n Maybe this accounts for the different views of the AI apocalypse. If we are all just mathematical formulas, then we are not much different from a computer program like GPT-4, or GPT-N for some N. But no one has figured out how to put quantum pixie-dust into GPT-N, and probably no one ever will. Maybe AI will never have the agency to take over the world and kill us.\n\n133. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #133 [April 13th, 2023 at 9:07 pm](https://scottaaronson.blog/?p=7209#comment-1949192)\n Roger Schlafly #132: As someone who’s taken the quantum pixie-dust hypothesis more seriously than most, I still wouldn’t want to bet our species’ survival on it! 🙂\n\n134. Shmi Says:\n\n Comment #134 [April 13th, 2023 at 10:28 pm](https://scottaaronson.blog/?p=7209#comment-1949194)\n Re physics of qubits, it is instructive to consider an actual physical qubit. For example, for transmons ( [https://en.wikipedia.org/wiki/Transmon](https://en.wikipedia.org/wiki/Transmon)) the Hamiltonian is proportional to the current density multiplied by the surface charge density. The latter is fixed by the properties of the material, the former is bounded by the BCS condensation energy. So there is no easy way to multiply the Hamiltonian by 10 once you are close to the physical limits of the substrate.\n\n Completely separately from that, a time-dependent Hamiltonian can actually lead to decoherence. For example, an expanding square well creates a thermal distribution of vacuum states ( [https://arxiv.org/pdf/gr-qc/0111027.pdf](https://arxiv.org/pdf/gr-qc/0111027.pdf)) resulting in Hawking-like radiation.\n\n Even more surprisingly, simply being near a black hole (or any horizon) results in extra decoherence, in addition to any potential Hawking radiation, as Robert Wald et al. have calculated just this year ( [https://arxiv.org/pdf/2301.00026.pdf](https://arxiv.org/pdf/2301.00026.pdf)). This includes cosmological, Rindler and all other horizons.\n\n From the abstract: “The Killing horizon thereby harvests “which path” information of quantum superpositions and will decohere any quantum superposition in a finite time”\n\n In some sense, this puts an upper limit on the size of quantum computers, though way past the limit that Gil Kalai insists on.\n\n If you dig a bit deeper, the cause of both the Hawking radiation and of the horizon-induced decoherence is likely that any metric with a horizon is not globally stationary, so there is no globally unique vacuum ground state, only a variety of thermal states.\n\n135. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #135 [April 13th, 2023 at 10:33 pm](https://scottaaronson.blog/?p=7209#comment-1949195)\n Shmi #134: I’m skeptical that that actually imposes an upper bound on the size of a quantum computer. Couldn’t it be treated as just one more source of decoherence to be handled using quantum error-correction? If not, what _is_ the actual upper bound you’re asserting?\n\n The only fundamental physical limit on QC size that I know, comes from the holographic entropy bound combined with the deSitter cosmology of our universe. These together impose an upper bound of ~10122 qubits in any QC—basically, the number you could fit in the observable universe without collapsing it all to a gigantic black hole.\n\n136. JimV Says:\n\n Comment #136 [April 13th, 2023 at 11:06 pm](https://scottaaronson.blog/?p=7209#comment-1949196)\n On quantum pixie-dust and free will: I’ve argued (I think at this site among other places), that if you consider a random component in decison-making to be free will (I don’t see what else quantum pixie dust could provide) there are plenty of ways biological evolution could have found to include it (since I think it could well be of benefit). For example, in “QED” Dr. Feynman remarks that the human eye was the most sensitive instrument for early two-slit experiments, as it can detect a single photon. Therefore the number of photons hitting retinas at a given time could be the seed for a random-number generator.\n\n My suggestion of follow-up prompts in testing QPT-4 was meant to be a concrete suggestion, that after each answer there should be a follow-up asking QPT-4 if it agrees with that answer or could improve it, to simulate checking its work. But perhaps that has already been tried.\n\n137. Skipper Says:\n\n Comment #137 [April 14th, 2023 at 12:08 am](https://scottaaronson.blog/?p=7209#comment-1949197)\n Scott, it’s amusing that you mention the holographic entropy bound and deSitter cosmology as if they are the only factors to consider. There’s more to the story than simply avoiding the collapse of the observable universe into a black hole (wtf?! was that an actual paper or a star trek episode?) Practical engineering constraints, energy requirements, and material limitations play a significant role in determining the feasibility of building and operating quantum computers at such scales.\n\n138. [GPT-4 gets a B on quantum computing final exam \\| Tmilinovic's Blog](https://tmilinovic.wordpress.com/2023/04/14/gpt-4-gets-a-b-on-quantum-computing-final-exam/) Says:\n\n Comment #138 [April 14th, 2023 at 2:39 am](https://scottaaronson.blog/?p=7209#comment-1949200)\n \\[…\\] Aaronson had GPT-4 take the actual 2019 final exam in Introduction to Quantum Information Science, his upper-level honors undergraduate course at UT \\[…\\]\n\n139. [Bill Benzon](https://new-savanna.blogspot.com/) Says:\n\n Comment #139 [April 14th, 2023 at 6:14 am](https://scottaaronson.blog/?p=7209#comment-1949204)\n @Vadim, #116 April 13th: You ask:\n\n\n > How far are we from “continuously-learning” LLM’s that immediately integrate their conversations into their training data instead of just having a temporary context window?\n\n\n There’s a technical problem:\n\n\n > In a system such as a Turing machine, where the length of the tape is not fixed in advance, changes in the amount of available memory can be affected without changing the computational structure of the machine; viz by making more tape available. By contrast, in a finite state automaton or a Connectionist machine, adding to the memory (e.g. by adding units to a network) alters the connectivity relations among nodes and thus does affect the machine’s computational structure. Connectionist cognitive architectures cannot, by their very nature, support an expandable memory…\n >\n > Jerry A. Fodor; Zenon W. Pylyshyn (1988). Connectionism and cognitive architecture: A critical analysis. _Cognition_, 28(1-2), 0–71. doi:10.1016/0010-0277(88)90031-5.\n\n\n Thus adding new items to the memory of an LLM requires that it be completely retrained, which is not practical when training time is measured in months. Given that brains are connectionist machines and brains can integrate new memories – though memory consolidation goes through phases and is not instantaneous – there must be a solution, but it’s not clear what that is. I’m betting that [the glia hold the answer](https://doi.org/10.1016/j.conb.2021.07.009), but at this point that’s only a guess. Geoffrey Hinton seems to be betting on [“neuromorphic” computers for the future](https://www.zdnet.com/article/we-will-see-a-completely-new-type-of-computer-says-ai-pioneer-geoff-hinton-mortal-computation/#ftag=COS-05-10aaa0j), a reasonable guess. For now, though, we’re going to have to do with work-arounds.\n\n140. mls Says:\n\n Comment #140 [April 14th, 2023 at 6:34 am](https://scottaaronson.blog/?p=7209#comment-1949205)\n Dr. Aaronson,\n\n Thank you for this post. A handful of explicit calculations is far more useful than 1000 papers of ad nauseum about bras and kets.\n\n141. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #141 [April 14th, 2023 at 6:53 am](https://scottaaronson.blog/?p=7209#comment-1949206)\n Skipper #137: Assuming your comment is sincere—did you _really_ think, when writing it, that I was unaware that “practical engineering constraints, energy requirements, and material limitations” are rightly more on QC engineers’ minds than the ultimate limits imposed by the Bekenstein bound and deSitter space cosmology? If I was aware, why might I have talked about the latter anyway? Could it be because the question at hand was _specifically about_ whether there’s a fundamental physics limit to the scale of a QC, and that question requires thinking in a completely different way?\n\n142. [Joshua Zelinsky](https://sites.google.com/view/joshuazelinsky) Says:\n\n Comment #142 [April 14th, 2023 at 7:21 am](https://scottaaronson.blog/?p=7209#comment-1949207)\n It may be worth noting also that the Bekenstein bound is not just QM specific. As I understand it, if we had say some way of actually implementing PDQP, which is the complexity class which allows quantum computers can perform measurements that do not collapse the wavefunction, the Bekenstein bound would still limit how much computation one can do.\n\n Marginally related note while thinking about it: In the paper which introduced PDQP, by Scott with Adam Bouland, Joseph Fitzsimons, and Mitchell Lee, they showed that there is an oracle relative to which PDQP does not contain NP. What I do not know: Is it true that for a random oracle with probability 1, that PDQP does not contain NP?\n\n143. Neverm\\|nd Says:\n\n Comment #143 [April 14th, 2023 at 7:37 am](https://scottaaronson.blog/?p=7209#comment-1949208)\n @Scott — While we are on the topic, a separate but quite related question. Though it was under less fortunate circumstances, this time last year I had the chance to read (IMHO) Terry Rudolph’s excellent primer ‘Q is for Quantum’. As I am not \\*formally\\* a Quantum Computing student, I forget the \\*exact\\* principal cited (if I recollect the principal relates to Einstein’s reasoning of the problem, and also something having to do with Winnie the Pooh– or was it Alice ?) and don’t have the text on me at this time– but the gist was that even though entangled particles \\*do\\* change states simultaneously even when separated at great distances this property \\*cannot\\* be used to ‘transmit information’.\n\n Yet, at the same time, in at least the ‘popular science press’ at the time there was all this hype about the coming ‘Quantum Internet’. I did write to Prof. Rudolph at the time for clarification– But, unfortunately, he never got back to me. I’ve never been quite sure if this was because my question was not interesting enough…. Or, in a way, he was trying to ‘prove a point’?\n\n I ask insofar as I see almost no articles about the ‘Quantum Internet’ these days, but now it is all generative AI. I mention because this constant shifting in the hype circles feels very familiar– And also, to my question, from a fellow Prof perhaps find an answer.\n\n Finally, just as a second aside, to be honest I kind of find it curious that, with even GPT-4, while it often can struggle with arithmetic and seemingly basic other logical reasoning (thus, I think the idea behind the Wolfram plug-in), it’s programming skills are at least ‘so-so’ okay, which of course requires a lot of ‘logical reasoning’. Mostly a rhetorical ‘food for thought’ question.\n\n With that in mind though, it might be worth stating whether you utilized any such plug-ins when you feed GPT the test ?\n\n Thank you,\n\n -A\n\n144. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #144 [April 14th, 2023 at 7:44 am](https://scottaaronson.blog/?p=7209#comment-1949209)\n Joshua Zelinsky #142: Yeah, sure, you could modify our oracle separation to one that works relative to a random oracle. The much bigger question is whether there’s an oracle relative to which NP⊄DQP, where DQP is the original class that I defined in 2005 (or really 2001). I had claimed such a result in the 2005 paper, but there was a mistake: I had bounded only the probability of the hidden-variable trajectory _hitting_ the marked item, rather than _revealing the location_ of the marked item in some possibly indirect way. I still don’t know how to fix that.\n\n145. Dimitris Papadimitriou Says:\n\n Comment #145 [April 14th, 2023 at 8:22 am](https://scottaaronson.blog/?p=7209#comment-1949210)\n Shmi #134\n\n These papers from Wald et al introduce ( perhaps – it’s a bit early now to say so with any confidence…) a new twist on the almost 50 years old black hole information problem.\n\n They’re more general, in the sense that they have to do with all kinds of causal horizons ( BH, deSitter, Rindler).\n\n As for the consequences for Quantum Computing, I’m quite curious too…\n\n P.S. Now I just saw Scott’s comment (#135) on that. I’m not sure it’s just as simple. Anyway…\n\n146. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #146 [April 14th, 2023 at 8:35 am](https://scottaaronson.blog/?p=7209#comment-1949211)\n Neverm\\|nd #143: No, I didn’t use any plugins, just GPT-4 chat, like I said. I’m unfortunately having trouble extracting a clear question from the rest of what you wrote.\n\n147. Prasanna Says:\n\n Comment #147 [April 14th, 2023 at 8:54 am](https://scottaaronson.blog/?p=7209#comment-1949212)\n Looks like the ultimate test of meat chauvinism (aka Free will) is about to get to us lot sooner than expected !! The success of psychiatric medicines (chemicals) to treat various disorders leave alone the brain messing effects of psychedelic drugs, should have convinced us long ago that free will is something we “wish” we have had. Consciousness , free will et al are just emergent properties just like deception, empathy etc that LLMs will be able to simulate (just that it wont be that of any single individual, but that of aggregate of all data that is used to train them). Whether they have already had it with Trillion parameters or need more scaling up, is the question that only few folks within the OpenAI/Google teams can answer definitively\n\n148. Vadim Says:\n\n Comment #148 [April 14th, 2023 at 9:28 am](https://scottaaronson.blog/?p=7209#comment-1949214)\n Bill Benzon #139,\n\n Thanks for the explanation and the links, that was enlightening and at least for the time being makes me less worried about the AI doomsday scenario (though I remain plenty worried about the jobs that will be lost with or without continuous learning). If or when the continuous learning problem is solved, I’ll be ready to really panic.\n\n149. Ted Says:\n\n Comment #149 [April 14th, 2023 at 9:35 am](https://scottaaronson.blog/?p=7209#comment-1949215)\n Scott #121: Down in comment #26 of [his blog post](https://www.thebigquestions.com/2023/04/05/gpt-4-fails-economics/), Landsburg says that the median human score on his exam was around 75% (vs. 4% for GPT-4).\n\n150. Skipper Says:\n\n Comment #150 [April 14th, 2023 at 9:47 am](https://scottaaronson.blog/?p=7209#comment-1949216)\n Scott, there’s no need to get defensive here. I understand that the question was about the fundamental physics limit of a quantum computer, and I appreciate your attempt to address it. However, I believe it’s important to consider the realistic challenges of building such a massive quantum computer in the context of this discussion. We can’t simply ignore the practical implications and focus solely on theoretical limits. By doing so, we might unintentionally give the impression that these theoretical limits are the only barriers to overcome, when in fact, engineering and resource constraints may be far more relevant in practice. So, while your points are valid, they should be taken as a part of a larger picture rather than the whole story.\n\n151. Neverm\\|nd Says:\n\n Comment #151 [April 14th, 2023 at 9:54 am](https://scottaaronson.blog/?p=7209#comment-1949217)\n @Scott #146 Ha ! That’s okay, maybe this weekend let me see if I can find the text and ask my question more clearly. (P.s. in the text where I think he discusses this matter he frames it as a philosophical discussion between Einstein and Winnie the Pooh– So I am not ‘nuts’, at least not yet…). It was the recommended text for UChicago’s online ‘Quantum Computing For Everyone’ course on EdX that I audited. As said, I found it to be quite good, at least as a general introduction. Till then.\n\n152. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #152 [April 14th, 2023 at 10:22 am](https://scottaaronson.blog/?p=7209#comment-1949218)\n Skipper #150: **Banned** from this comment section. Life is short, and I’m no longer going to argue with anonymous commenters who patronize me and who sound like GPT.\n\n153. Sloth Says:\n\n Comment #153 [April 14th, 2023 at 10:44 am](https://scottaaronson.blog/?p=7209#comment-1949220)\n Scott 152:\n\n I wrote a little poem.\n\n Scott first came for the Nazis and antisemites, and I did not speak out—\n\n Because I was not a Nazi nor an antisemite.\n\n Then Scott came for the woke trolls, and I did not speak out—\n\n Because I was not a woke troll.\n\n Scott then came for the incel trolls, and I did not speak out—\n\n Because I was not an incel troll.\n\n Next, Scott came for the sneerers, and I did not speak out—\n\n Because I was not a sneerer.\n\n Scott came for the conspiracy theorists, and I did not speak out—\n\n Because I was not a conspiracy theorist.\n\n Then Scott came for the pedantic nitpickers, and I did not speak out—\n\n Because I was not a pedantic nitpicker.\n\n Finally, Scott came for those with questions about the Bekenstein bound, and I did not speak out—\n\n Because I had no questions about the Bekenstein bound.\n\n And then Scott came for me—and there was no one left to speak for me.\n\n154. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #154 [April 14th, 2023 at 10:56 am](https://scottaaronson.blog/?p=7209#comment-1949221)\n Sloth #153: In 18 years of blogging, I don’t remember ever once banning a commenter who came here with what seemed to me like a genuine desire to learn, rather than (e.g.) to demonstrate their own superior wisdom. Crucially, Skipper (above) was _not_ the one who had questions about the Bekenstein bound, but rather the one who “found it amusing” that I would even mention the bound, rather than redirecting the question toward practical issues that no one here was disputing.\n\n155. delbert Says:\n\n Comment #155 [April 14th, 2023 at 11:12 am](https://scottaaronson.blog/?p=7209#comment-1949222)\n really? QBism wasn’t one of the choices on your first exam question?\n\n156. Sloth Says:\n\n Comment #156 [April 14th, 2023 at 11:16 am](https://scottaaronson.blog/?p=7209#comment-1949223)\n Scott 154:\n\n You banned “the quantum man,” just for asking questions about oracles and how they relate to Grover’s algorithm.\n\n You know, I wouldn’t be surprised if, much like the Reichstag Fire, all the trolling over the summer was staged, a false-flag by yours truly, just as a pretext for stealing our freedom of speech.\n\n157. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #157 [April 14th, 2023 at 11:21 am](https://scottaaronson.blog/?p=7209#comment-1949224)\n delbert #155: Chris Fuchs himself emailed me to complain about that! He was gratified that GPT-4 mentioned QBism even if I didn’t. 🙂\n\n I told Chris that the issue is simply that, in the single “comparative religion” lecture that I do in this course, there isn’t _nearly_ enough time to do justice to QBism or its subtle differences from the Copenhagen Interpretation (the two might seem identical to an untrained eye), and I wouldn’t want to say anything wrong. Certainly it has nothing to do with the fact that Blake Stacey, Chris’s frequent collaborator on QBism, is also one of the main people who constantly attacks me on SneerClub (!).\n\n158. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #158 [April 14th, 2023 at 11:32 am](https://scottaaronson.blog/?p=7209#comment-1949225)\n Sloth #156: Nope. I just checked, and “Quantum Man” was banned, not for asking questions, but for repeating the same error over and over even after a matter was patiently explained to him, and (crucially) always with an unctuous tone presupposing that _I_ needed to be schooled about freshman-level basics of QC.\n\n Welcome to the new _Shtetl-Optimized_ comment section with new expectations. If I’m going to volunteer hours of my life every day to educate strangers about QC and other topics, then it sure as hell won’t be for those who seem to relish causing me stress, anger, and high blood pressure while I do it.\n\n159. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #159 [April 14th, 2023 at 11:38 am](https://scottaaronson.blog/?p=7209#comment-1949226)\n Incidentally, fikisipi #38 (since I forgot to answer this before):\n\n\n I would very much like to see his analysis on which \\[favourite QM\\] interpretations correlate with high scores \\[on the exam\\].\n\n\n\n\n\n Come on Scott, tell us!\n\n\n\n\n I addressed the question in [this post](https://scottaaronson.blog/?p=3628) five years ago:\n\n when (at the TAs’ insistence) we put an optional ungraded question on the final exam that asked students their favorite interpretation of QM, we found that there was no correlation whatsoever between interpretation and final exam score—except that students who said they didn’t believe any interpretation at all, or that the question was meaningless or didn’t matter, scored noticeably higher than everyone else.\n160. Sloth Says:\n\n Comment #160 [April 14th, 2023 at 11:52 am](https://scottaaronson.blog/?p=7209#comment-1949229)\n Scott:\n\n So you don’t deny that—as I’ve always suspected—you staged the trolling over the summer, as a false-flag operation, a pretext for cracking down on our civil liberties?\n\n161. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #161 [April 14th, 2023 at 12:33 pm](https://scottaaronson.blog/?p=7209#comment-1949231)\n Sloth #160: I deny it categorically and emphatically. The kinds of discussions I want to host here have earnest curiosity as a precondition, with at most modest sprinklings of irony. Trolls on this blog have been a significant source of stress in my life, and I’d sooner pull out my own fingernails than stage troll attacks.\n\n162. Prasanna Says:\n\n Comment #162 [April 14th, 2023 at 12:41 pm](https://scottaaronson.blog/?p=7209#comment-1949232)\n Max Tegmark at this eloquent best, almost convinces you on why pause AI, except he doesn’t offer any solid reason why does he think its close to doom Right Now.\n\n Would be curious to see what would be the responses here, as this talk touches every plausible topic.\n\n [iframe](https://www.youtube.com/embed/vDlkNiCbBBM?version=3&rel=1&showsearch=0&showinfo=1&iv_load_policy=1&fs=1&hl=en-US&autohide=2&wmode=transparent)\n\n163. Nate Says:\n\n Comment #163 [April 14th, 2023 at 12:43 pm](https://scottaaronson.blog/?p=7209#comment-1949233)\n I for one really like the more ban happy side of Scott 🙂\n\n I always thought you gave trolls too much of your time. Bravo.\n\n164. fred Says:\n\n Comment #164 [April 14th, 2023 at 2:01 pm](https://scottaaronson.blog/?p=7209#comment-1949234)\n It’s easy to give GPT free will:\n\n ask it for different values for the parameters in its neural net, overwrite the parameters with its suggestions, repeat.\n\n165. flergalwit Says:\n\n Comment #165 [April 14th, 2023 at 2:35 pm](https://scottaaronson.blog/?p=7209#comment-1949235)\n Scott #122 thanks for the reply.\n\n I guess a way of making the question more precise is, suppose I have a Hamiltonian H whose evolution gives a quantum algorithm for solving some problem with input size n in a particular time t. How do I normalise H so that it uses O(1) energy – in particular to assure myself I haven’t cheated (presumably by using an exponentially large amount of energy)?\n\n Is it simply that we want the expectation of H in state \\|psi> to be order 1, where \\|psi> is the state in which we run the algorithm? Thanks again.\n\n166. chwr Says:\n\n Comment #166 [April 14th, 2023 at 2:47 pm](https://scottaaronson.blog/?p=7209#comment-1949236)\n What is the answer to 6(a) and 6(c)?\n\n167. Shmi Says:\n\n Comment #167 [April 14th, 2023 at 2:59 pm](https://scottaaronson.blog/?p=7209#comment-1949237)\n Scott #135:\n\n It is indeed very likely that quantum error correction can address this source of decoherence. Their claim is only that a mere presence of a horizon creates decoherence. I went too far in implying that it might be some sort of a fundamental limit on the QC size. Oops.\n\n From their previous paper abstract, [https://arxiv.org/abs/2205.06279v2](https://arxiv.org/abs/2205.06279v2):\n\n “We show that if a massive body is put in a quantum superposition of spatially separated states, the mere presence of a black hole in the vicinity of the body will eventually destroy the coherence of the superposition.”\n\n I did not follow the complete derivation, but the authors include Robert Wald, whose GR book I used in grad school, so odds are, the logic is pretty sound.\n\n Dimitris #145:\n\n \\> These papers from Wald et al introduce ( perhaps – it’s a bit early now to say so with any confidence…) a new twist on the almost 50 years old black hole information problem.\n\n I tend to agree that it is an interesting new development, hopefully more comes out in the near future. As the paper says, “the fact that a black hole will eventually decohere any quantum superposition may be of fundamental significance for our understanding of the nature of black holes in a quantum theory of gravity.”\n\n168. Jair Says:\n\n Comment #168 [April 14th, 2023 at 4:07 pm](https://scottaaronson.blog/?p=7209#comment-1949238)\n It is remarkable to play with the Python-interpreter GPT. It is very human-like in how it can execute code, see an error, and then find its own mistake and iterate. Giving LLMs feedback based on output from conventional computation unlocks a lot more power.\n\n This makes me wonder how long it will be before an AI model writes a breakthrough paper in math, all or mostly on its own, including the important conceptual leaps. You could require that it formally verifies every conclusion in some language like Coq or Mizar. Once an AI is able to make even a little non-trivial progress, it should be easy to massively scale this effort up with thousands of theorem-proving agents tackling a big problem like P != NP, say, with many different randomly-chosen lines of attack. It could then summarize any relevant progress in a human-readable fashion. Even without any breakthroughs, LLMs could be used to formally verify human-written papers, including filling in any gaps. This could be a requirement for future peer review.\n\n I do wonder if the current transformer architecture is up to the task, though. Even with Python output, I’ve found the model getting very stuck on certain tasks and going around in circles without making headway. Real progress might require some supervised learning approach and/or some kind of world-model to make sure its arguments are correct instead of hallucinations. I have a feeling we will find out soon.\n\n169. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #169 [April 14th, 2023 at 4:24 pm](https://scottaaronson.blog/?p=7209#comment-1949239)\n chwr #166: The answer to 6(a) is just that they run their protocol on an equal superposition of inputs, then Bob applies a phase gate conditional on the answer, then they run the protocol again to uncompute the answer (alternatively, the answer is XOR’ed into a \\|-⟩ state).\n\n The answer to 6(c) is basically just that Alice and Bob run Grover’s algorithm in distributed fashion, sending the state of Grover’s algorithm back and forth across the channel at each iteration, and treating each index i as “marked” if and only if xi=yi=1\\. This takes O(√n) Grover iterations (i.e., rounds of communication), in each of which O(log n) qubits are exchanged. (The idea comes from Buhrman, Cleve, and Wigderson in 1998.)\n\n170. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #170 [April 14th, 2023 at 4:24 pm](https://scottaaronson.blog/?p=7209#comment-1949240)\n flergalwit #165: Yes, I believe that physical Hamiltonians are supposed to have O(1) expected energy.\n\n171. [Alex Meiburg](http://ohaithe.re/) Says:\n\n Comment #171 [April 14th, 2023 at 4:48 pm](https://scottaaronson.blog/?p=7209#comment-1949242)\n Re: Steve Landsburg’s economics exam, my understanding is that it’s deliberately designed to “trip up” students in some way — that each question has a ‘twist’ that should force the students to think a bit unconventionally, carefully consider the reality, and so on. And that he has ‘trained’ his students to do this through homework.\n\n I’m not an economist, and I admit I’m embarrassed at how poorly I would have done the exam. Many answers were indeed opposite to what I thought. But I’m going off of comments from that blogpost and others, which suggest it’s supposed to make students think carefully, and not “charge forward blindly … like Econ 101 students … and compute supply/demand curves.”\n\n Given the huge amount of Econ 101, or even AP Econ, quantity online, I think GPT does so poorly on this exam because it does ‘charge forward’. Some commenters suggested that with more careful prompting (“You are an economics professor” – as opposed to a college freshman), it did a better job of catching the subtleties.\n\n Bryan Caplan’s midterm seems … well, more straightforward. It made more sense to me. 🙂 So maybe the reason GPT did worse on Landsburg’s exam, is that Landsburg likes to trip his students up more. Maybe Landsburg’s test is designed for a class aimed at a different level of education. Maybe this is just a way of saying that, well, “Hey, I found Landsburgs test a lot harder!”\n\n I’m reminded of the following simple math problem. It absolutely could fit in a midterm on differential equations — but I would say that it would be a bit mean to students at some schools, while perfectly okay at some other schools (with more demanding curricula). ChatGPT got it wrong, in the same that I would expect many college students to get it wrong. I encourage you to try it briefly, before looking at the answer:\n\n Find all solutions to the differential equation,\n\n y = y’\\*x/2\n\n over the reals.\n\n Answer:\n\n Integrating by parts, we get that y = C \\* x^2 without much difficulty. Usually, the constant of integration C needs to be constant across the whole function. However, at the point x=0, we know y=0 and y’=0 as well, independent of C. Thus, C can change at this interface. The full set of solutions is parameterized by two constants (C1,C2), with the solution y(x) = (C1 if x Please read the following question, select the correct answer from one of the options, and provide the reasoning process. Don’t look up the answers on the internet. Question: In a box, there are three boxes, and each one of these boxes contains three smaller boxes. How many boxes are there in total? Options: A: 9, B: 10, C: 12, D: 13, E: 15\n\n ChatGPT got this one right 0 times out of 10; it totally failed (though its reasoning steps might have been ok, except for a smaller error or two). Bing chat got it right the first time and didn’t do an external search (it always tells you when it searches the internet, then shows you a check-mark when it completes the search; there was no search, no check-mark, no sources listed). It responded:\n\n \\> The correct answer is D: 13. Here’s the reasoning process:\n\n \\> If in a box, there are three boxes, and each one of these boxes contains three smaller boxes, then there are a total of 3 x 3 = 9 smaller boxes. Adding the three larger boxes to the nine smaller boxes gives us a total of 9 + 3 = 12. However, since the question asks for the total number of boxes (including the original box), the answer is 12 + 1 = 13.\n\n I expect GPT4 would do even better, perhaps getting most of the 11 problems right when given 10 tries per problem, told to think step-by-step, and using a simple majority vote.\n\n ….\n\n One thing I would point out is that models don’t either only “generalize” or only “memorize”. They can do both — sometimes applying memorization, sometimes applying generalization + a learned algorithm. Also, “generalization” is not the opposite of “memorization”. A rules-based chess engine can be said to fail to generalize to other games; but it can equally well be said not to be memorizing — it’s applying an algorithm.\n\n A well-known example of these models doing “interpolative generalization” is the following: several years ago when Google trained some very large machine translation models, they noticed that the systems could translate between pairs of languages for which they either gave it no examples at all, or very few (far too few to learn to translate properly). For example, maybe the dataset had lots of parallel translations between English French, German French, and German Italian, but say no examples of English Italian; yet, the model somehow generalized and did perfectly well at translating English Italian. (These were not the exact pairs of languages; obviously it was some other pairs, since there surely were lots of English Italian examples).\n\n As I recall, some Google scientists came to believe that the model had emergently learned an “interlingua” — an internal universal language for translation, that works for all language pairs at once. People criticized this claim; but the fact remains that the model \\*somehow\\* generalized to translate between pairs of languages that it either wasn’t trained on, or was only given a tiny amount of data.\n\n197. Tom Bouley Says:\n\n Comment #197 [April 16th, 2023 at 7:40 pm](https://scottaaronson.blog/?p=7209#comment-1949315)\n Out os curiosity, is there any correlation between preferred interpretation of QM and exam score?\n\n198. J. Says:\n\n Comment #198 [April 17th, 2023 at 1:36 am](https://scottaaronson.blog/?p=7209#comment-1949334)\n One problem is that we, the bystanders, but most of the press as well, are often a bit careless in terminology.\n\n Example: starspawn0 #196’s usage of “generalize” vs “extrapolate/interpolate/project/etc.” These two types of nomenclature should be distinguished in a strict sense.\n\n When you are trained on pictures of lions, you should be able to get some tigers, house cats or lynxes right – extrapolation. Similarly, If you speak fluent Spanish, you can understand quite a bit of Italian/Portuguese. This type of getting results some distance beyond your source data is the basic skill of a whole lot of base level AIs iirc. (Language hopping, as a guess, could work similar to taking a transfer flight or train, transferring correct translations via a common node.)\n\n “Generalization” should be used for stuff that is systematic/categorical/universal etc. You generalize things into a universal principle. For example, if object A is inside object B and object B is inside object C, then object A is inside object C. This is simply true if you believe in 3D-space and its usual mathematical representation (Euclidian space) or simply common prejudices. There is absolutely no reason (or sign) that LLMs can do such “reasoning” or work with a universal principle.\n\n People seem to want the thing to accomplish things it can’t and won’t.\n\n199. Ose Says:\n\n Comment #199 [April 17th, 2023 at 6:53 am](https://scottaaronson.blog/?p=7209#comment-1949193)\n Following on from Sebastian’s concern about contamination, many of the true/false questions are just asking for facts that can be easily looked up by Googling keywords contained in the question. E.g. the answers to the first 10 questions can be found here:\n\n 1a: [https://en.wikipedia.org/wiki/Unitary\\_matrix#Equivalent\\_conditions](https://en.wikipedia.org/wiki/Unitary_matrix#Equivalent_conditions)\n\n 1b: [https://en.wikipedia.org/wiki/Qubit#Standard\\_representation](https://en.wikipedia.org/wiki/Qubit#Standard_representation)\n\n 1c: [https://en.wikipedia.org/wiki/CHSH\\_inequality#CHSH\\_game](https://en.wikipedia.org/wiki/CHSH_inequality#CHSH_game)\n\n 1d: [https://spectrum.ieee.org/googles-quantum-tech-milestone-excites-scientists-and-spurs-rivals](https://spectrum.ieee.org/googles-quantum-tech-milestone-excites-scientists-and-spurs-rivals) (Martinis’ quote in “Building on Quantum Supremacy”)\n\n 1e: [https://en.wikipedia.org/wiki/Lattice-based\\_cryptography](https://en.wikipedia.org/wiki/Lattice-based_cryptography)\n\n 1f: [https://en.wikipedia.org/wiki/Integer\\_factorization#Difficulty\\_and\\_complexity](https://en.wikipedia.org/wiki/Integer_factorization#Difficulty_and_complexity)\n\n 1g: [https://en.wikipedia.org/wiki/Grover%27s\\_algorithm](https://en.wikipedia.org/wiki/Grover%27s_algorithm)\n\n 1h: [https://en.wikipedia.org/wiki/Entropy\\_of\\_entanglement](https://en.wikipedia.org/wiki/Entropy_of_entanglement)\n\n 1i: [https://books.physics.oregonstate.edu/LinAlg/eigenunitary.html](https://books.physics.oregonstate.edu/LinAlg/eigenunitary.html)\n\n 1j: [https://quantum.phys.cmu.edu/CQT/chaps/cqt15.ps](https://quantum.phys.cmu.edu/CQT/chaps/cqt15.ps) (first page)\n\n GPT-4’s performance on questions like this only demonstrates that it is pretty good at determining whether or not the statement in the question is equivalent to, or contradicted by, statements that surely exist in its training set. The question of how impressed we should be by this ability depends on how far the formulations of the statements in the questions are to the formulations of relevant statements in the training set. E.g. its answer to 1j would be a bit more impressive if all articles about density matrices were like the Wikipedia article, which says that their eigenvalues are non-negative and that they sum to 1, but doesn’t explicitly say that they lie in \\[0,1\\], as it would then only be able to answer this question by cross-referencing this information against articles on unrelated topics that state that numbers with these properties lie in \\[0,1\\]. But the article that I linked to above states the range explicitly.\n\n Given that it was easy for me to find the answers to the first 10 true/false questions despite knowing nothing about quantum computing, they don’t strike me as very compelling evidence of its intelligence. I can’t comment on the other questions, as I haven’t had a chance to look at them. But I’d be more impressed by good performance on questions that require both strong language skills and strong mathematical reasoning skills. It would be very interesting to see how much WolframAlpha boosts its ability to answer such questions.\n\n200. fred Says:\n\n Comment #200 [April 17th, 2023 at 5:02 pm](https://scottaaronson.blog/?p=7209#comment-1949387)\n In the podcast, Scott said\n\n _“But in order for it to be a day to day tool, it cant just be this chatbot, it has to be able to do stuff, such as go to the internet for you, like retrieve documents for you, summarize it for you. \\[…\\] here is something to expect within the next year, that GPT and other LLMs will become more and more integrated with how we use the web.\\[…\\] you could unlock a lot of the near term usefulness if you could give it errands to do \\[…\\] I expect that we’re going to go in that direction and that worries me somewhat, because now there’s a lot more potential for deceit”_\n\n It only took a few days because that’s exactly what AutoGPT/BabyAGI are doing!\n\n (video about AutoGPT/BabyAGI capabilities)\n\n [https://youtu.be/Qm2Ai\\_JiQmo?t=400](https://youtu.be/Qm2Ai_JiQmo?t=400)\n\n201. [ChatGPT-4 Receives ‘B’ on Scott Aaronson’s Quantum Information Science Final — Immediately Emails the Dean Seeking a Better Grade – knowledge of Self Library](https://koslibrary.com/chatgpt-4-receives-b-on-scott-aaronsons-quantum-information-science-final-immediately-emails-the-dean-seeking-a-better-grade) Says:\n\n Comment #201 [April 18th, 2023 at 5:43 am](https://scottaaronson.blog/?p=7209#comment-1949424)\n \\[…\\] accordance with a weblog publish on Aaronson’s blog Shtetl-Optimized, Aaronson and his head trainer’s assistant gave GPT-4 the issues by way of their LaTeX supply \\[…\\]\n\n202. [AI #8: People Can Do Reasonable Things \\| Don't Worry About the Vase](https://thezvi.wordpress.com/2023/04/20/ai-8-people-can-do-reasonable-things/) Says:\n\n Comment #202 [April 20th, 2023 at 10:42 am](https://scottaaronson.blog/?p=7209#comment-1949514)\n \\[…\\] The latest final exam given to GPT-4 was Scott Aaronson’s Quantum Computing final exam from 2019, …, which likely would be modestly improved if it could use a calculation plug-in like WolframAlpha. I can’t say much more about this one because, unlike the economics exams, I don’t know the first thing about the material being tested. \\[…\\]\n\n203. Benjamin Feddersen Says:\n\n Comment #203 [April 24th, 2023 at 7:03 am](https://scottaaronson.blog/?p=7209#comment-1949610)\n The point of university exams is not to answer the questions. The purpose is to show that, in addition to being able to pet a cat, shake a hand, throw a baseball, recognize the difference between your mother and a lamppost, and check in on an unpaid invoice or a sick friend, you can ALSO answer questions about quantum mechanics. These tests literally only make sense to give to humans. Giving it to a souped-up self-driving supergoogle is pointless, yet the appeal of the stunt appears undiminished.\n\n I’m much more fascinated by the sociology of the people involved in LLMs than I am by the things themselves, though to be fair, I don’t need any of the things they’re good at (coding copilots only make sense if you can already code).\n\n204. [When it comes to advanced math, ChatGPT is no star student - AI Eclipse](https://aieclipse.com/ars-technica/when-it-comes-to-advanced-math-chatgpt-is-no-star-student/) Says:\n\n Comment #204 [May 20th, 2023 at 1:16 pm](https://scottaaronson.blog/?p=7209#comment-1950663)\n \\[…\\] At the testing level of high school and undergraduate math classes, ChatGPT performs well, ranking in the 89th percentile for the SAT math test. It even received a B on technology expert Scott Aaronson’s quantum computing final exam. \\[…\\]\n\n\n### Leave a Reply\n\nYou can use rich HTML in comments! You can also use basic TeX, by enclosing it within $$ $$ for displayed equations or \\\\( \\\\) for inline equations.\n\n**Comment Policies:**\n\n1. All comments are placed in moderation and reviewed prior to appearing.\n2. You'll also be sent a verification email to the email address you provided.\n\n**YOU MUST CLICK THE LINK IN YOUR VERIFICATION EMAIL BEFORE YOUR COMMENT CAN APPEAR. WHY IS THIS BOLD, UNDERLINED, ALL-CAPS, AND IN RED? BECAUSE PEOPLE ARE STILL FORGETTING TO DO IT.**\n3. This comment section is **not a free speech zone**. It's my, Scott Aaronson's, virtual living room. Commenters are expected not to say anything they wouldn't say in my _actual_ living room. This means: No trolling. No ad-hominems against me or others. No presumptuous requests (e.g. to respond to a long paper or article). No conspiracy theories. No patronizing me. Comments violating these policies may be left in moderation with no explanation or apology.\n4. Whenever I'm in doubt, I'll forward comments to [Shtetl-Optimized Committee of Guardians](https://scottaaronson.blog/?p=6576), and respect SOCG's judgments on whether those comments should appear.\n5. I sometimes accidentally miss perfectly reasonable comments in the moderation queue, or they get caught in the spam filter. If you feel this may have been the case with your comment, shoot me an email.\n\nName (required)\n\nMail (will not be published) (required)\n\nWebsite\n\nΔ\n\n* * *",
        "image": "https://149663533.v2.pressablecdn.com/wp-content/uploads/2021/10/cropped-Jacket.gif",
        "favicon": "https://149663533.v2.pressablecdn.com/wp-content/uploads/2021/10/cropped-Jacket-32x32.gif"
      },
      {
        "id": "https://scottaaronson.blog/?p=7266",
        "title": "Five Worlds of AI (a joint post with Boaz Barak)",
        "url": "https://scottaaronson.blog/?p=7266",
        "publishedDate": "2023-04-28T00:37:27.000Z",
        "author": "",
        "score": 0.34899061918258667,
        "text": "* * *\n\n## [Five Worlds of AI (a joint post with Boaz Barak)](https://scottaaronson.blog/?p=7266)\n\n![](https://www.scottaaronson.com/fiveworlds.jpg)\n\nArtificial intelligence has made incredible progress in the last decade, but in one crucial aspect, it still lags behind the theoretical computer science of the 1990s: namely, there is no [essay describing five potential worlds that we could live in and giving each one of them whimsical names](https://www.quantamagazine.org/which-computational-universe-do-we-live-in-20220418/). In other words, no one has done for AI what Russell Impagliazzo did for complexity theory in 1995, when he defined the five worlds Algorithmica, Heuristica, Pessiland, Minicrypt, and Cryptomania, corresponding to five possible resolutions of the P vs. NP problem along with the central unsolved problems of cryptography.\n\nIn this blog post, we—Scott and Boaz—aim to remedy this gap. Specifically, we consider 5 possible scenarios for how AI will evolve in the future. (Incidentally, it was at a [2009 workshop](http://dimacs.rutgers.edu/archive/Workshops/Cryptography/program.html) devoted to Impagliazzo’s five worlds co-organized by Boaz that Scott met his now wife, complexity theorist [Dana Moshkovitz](https://www.cs.utexas.edu/~danama/). We hope civilization will continue for long enough that someone in the future could meet their soulmate, or neuron-mate, at a future workshop about _our_ five worlds.)\n\nLike in [Impagliazzo’s 1995 paper](https://www.karlin.mff.cuni.cz/~krajicek/ri5svetu.pdf) on the five potential worlds of the difficulty of NP problems, we will not try to be exhaustive but rather concentrate on extreme cases. It’s possible that we’ll end up in a mixture of worlds or a situation not described by any of the worlds. Indeed, one crucial difference between our setting and Impagliazzo’s, is that in the complexity case, the worlds corresponded to concrete (and mutually exclusive) mathematical conjectures. So in some sense, the question wasn’t “which world _will_ we live in?” but “which world have we Platonically _always_ lived in, without knowing it?” In contrast, the impact of AI will be a complex mix of mathematical bounds, computational capabilities, human discoveries, and social and legal issues. Hence, the worlds we describe depend on more than just the fundamental capabilities and limitations of artificial intelligence, and humanity could also shift from one of these worlds to another over time.\n\nWithout further ado, we name our five worlds “ **AI-Fizzle,”** **“Futurama,”** **”AI-Dystopia,”** **“Singularia,”** and **“Paperclipalypse.”** In this essay, we don’t try to assign probabilities to these scenarios; we merely sketch their assumptions and technical and social consequences. We hope that by making assumptions explicit, we can help ground the debate on the various risks around AI.\n\n**AI-Fizzle.** In this scenario, AI “runs out of steam” fairly soon. AI still has a significant impact on the world (so it’s not the same as a “cryptocurrency fizzle”), but relative to current expectations, this would be considered a disappointment. Rather than the industrial or computer revolutions, AI might be compared in this case to nuclear power: people were initially thrilled about the seemingly limitless potential, but decades later, that potential remains mostly unrealized. With nuclear power, though, many would argue that the potential went unrealized mostly for sociopolitical rather than technical reasons. Could AI also fizzle by political fiat?\n\nRegardless of the answer, another possibility is that costs (in data and computation) scale up so rapidly as a function of performance and reliability that AI is not cost-effective to apply in many domains. That is, it could be that for most jobs, humans will still be more reliable and energy-efficient (we don’t normally think of _low wattage_ as being key to human specialness, but it might turn out that way!). So, like nuclear fusion, an AI which yields dramatically more value than the resources needed to build and deploy it might always remain a couple of decades in the future. In this scenario, AI would replace and enhance some fraction of human jobs and improve productivity, but the 21st century would not be the “century of AI,” and AI’s impact on society would be limited for both good and bad.\n\n**Futurama.** In this scenario, AI unleashes a revolution that’s entirely comparable to the scientific, industrial, or information revolutions (but “merely” those). AI systems grow significantly in capabilities and perform many of the tasks currently performed by human experts at a small fraction of the cost, in some domains _superhumanly_. However, AI systems are still used as _tools_ by humans, and except for a few fringe thinkers, no one treats them as sentient. AI easily passes the Turing test, can prove hard theorems, and can generate entertaining content (as well as deepfakes). But humanity gets used to that, just like we got used to computers creaming us in chess, translating text, and generating special effects in movies. Most people no more feel inferior to their AI than they feel inferior to their car because it runs faster. In this scenario, people will likely anthropomorphize AI _less_ over time (as happened with digital computers themselves). In **“Futurama,”** AI will, like any revolutionary technology, be used for both good and bad. But as with prior major technological revolutions, on the whole, AI will have a large positive impact on humanity. AI will be used to reduce poverty and ensure that more of humanity has access to food, healthcare, education, and economic opportunities. In **“Futurama,”** AI systems will sometimes cause harm, but the vast majority of these failures will be due to human negligence or maliciousness. Some AI systems might be so complex that it would be best to model them as potentially behaving “adversarially,” and part of the practice of deploying AIs responsibly would be to ensure an “operating envelope” that limits their potential damage even under adversarial failures.\n\n**AI-Dystopia.** The technical assumptions of **“AI-Dystopia”** are similar to those of **“Futurama,”** but the upshot could hardly be more different. Here, again, AI unleashes a revolution on the scale of the industrial or computer revolutions, but the change is markedly for the worse. AI greatly increases the scale of surveillance by government and private corporations. It causes massive job losses while enriching a tiny elite. It entrenches society’s existing inequalities and biases. And it takes away a central tool against oppression: namely, the ability of humans to refuse or subvert orders.\n\nInterestingly, it’s even possible that _the same future_ could be characterized as **Futurama** by some people and as **AI-Dystopia** by others–just like how some people emphasize how our _current_ technological civilization has lifted billions out of poverty into a standard of living unprecedented in human history, while others focus on the still existing (and in some cases rising) inequalities and suffering, and consider it a neoliberal capitalist dystopia.\n\n**Singularia.** Here AI breaks out of the current paradigm, where increasing capabilities require ever-growing resources of data and computation, and no longer needs human data or human-provided hardware and energy to become stronger at an ever-increasing pace. AIs improve their own intellectual capabilities, including by developing new science, and (whether by deliberate design or happenstance) they act as goal-oriented agents in the physical world. They can effectively be thought of as an alien civilization–or perhaps as a new species, which is to us as we were to _Homo erectus_.\n\nFortunately, though (and again, whether by careful design or just as a byproduct of their human origins), the AIs act to us like benevolent gods and lead us to an “AI utopia.” They solve our material problems for us, giving us unlimited abundance and presumably virtual-reality adventures of our choosing. (Though maybe, as in _The Matrix_, the AIs will discover that humans need some conflict, and we will all live in a simulation of 2020’s Twitter, constantly dunking on one another…)\n\n**Paperclipalypse.** In **“Paperclipalypse”** or “AI Doom,” we again think of future AIs as a superintelligent “alien race” that doesn’t need humanity for its own development. Here, though, the AIs are either actively opposed to human existence or else indifferent to it in a way that causes our extinction as a byproduct. In this scenario, AIs do not develop a notion of morality comparable to ours or even a notion that keeping a diversity of species and ensuring humans don’t go extinct might be useful to them in the long run. Rather, the interaction between AI and Homo sapiens ends about the same way that the interaction between Homo sapiens and Neanderthals ended.\n\nIn fact, the canonical depictions of such a scenario imagine an interaction that is much more abrupt than our brush with the Neanderthals. The idea is that, perhaps because they originated through some optimization procedure, AI systems will have some strong but weirdly-specific goal (a la “maximizing paperclips”), for which the continued existence of humans is, at best, a hindrance. So the AIs quickly play out the scenarios and, in a matter of milliseconds, decide that the optimal solution is to kill all humans, taking a few extra milliseconds to make a plan for that and execute it. If conditions are not yet ripe for executing their plan, the AIs pretend to be docile tools, as in the **“Futurama”** scenario, waiting for the right time to strike. In this scenario, self-improvement happens so quickly that humans might not even notice it. There need be no intermediate stage in which an AI “merely” kills a few thousand humans, raising 9/11-type alarm bells.\n\n**Regulations**. The practical impact of AI regulations depends, in large part, on which scenarios we consider most likely. Regulation is not terribly important in the **“AI Fizzle”** scenario where AI, well, fizzles. In “ **Futurama,”** regulations would be aimed at ensuring that on balance, AI is used more for good than for bad, and that the world doesn’t devolve into **“AI Dystopia.”** The latter goal requires anti-trust and open-science regulations to ensure that power is not concentrated in a few corporations or governments. Thus, regulations are needed to _democratize_ AI development more than to _restrict_ it. This doesn’t mean that AI would be completely unregulated. It might be treated somewhat similarly to drugs—something that can have complex effects and needs to undergo trials before mass deployment. There would also be regulations aimed at reducing the chance of “bad actors” (whether other nations or individuals) getting access to cutting-edge AIs, but probably the bulk of the effort would be at increasing the chance of thwarting them (e.g., using AI to detect AI-generated misinformation, or using AI to harden systems against AI-aided hackers). This is similar to how most academic experts believe cryptography should be regulated (and how it _is_ largely regulated these days in most democratic countries): it’s a technology that can be used for both good and bad, but the cost of restricting its access to regular citizens outweighs the benefits. However, as we do with security exploits today, we might restrict or delay public releases of AI systems to some extent.\n\nTo whatever extent we foresee **“Singularia”** or **“Paperclipalypse,”** however, regulations play a completely different role. If we knew we were headed for **“Singularia,”** then presumably regulations would be superfluous, except perhaps to try to accelerate the development of AIs! Meanwhile, if one accepts the assumptions of **“Paperclipalypse,”** any regulations other than the most draconian might be futile. If, in the near future, almost anyone will be able to spend a few billion dollars to build a recursively self-improving AI that might turn into a superintelligent world-destroying agent, and moreover (unlike with nuclear weapons) they won’t need exotic materials to do so, then it’s hard to see how to forestall the apocalypse, except perhaps via a worldwide, militarily enforced agreement to “ [shut it all down](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/),” as Eliezer Yudkowsky indeed now explicitly advocates. “Ordinary” regulations could, at best, delay the end by a short amount–given the current pace of AI advances, perhaps not more than a few years. Thus, regardless of how likely one considers this scenario, one might want to focus more on the other scenarios for methodological reasons alone!\n\n[![Email, RSS](https://149663533.v2.pressablecdn.com/wp-content/plugins/really-simple-facebook-twitter-share-buttons/images/specificfeeds_follow.png) Follow](http://www.specificfeeds.com/follow)\n\nThis entry was posted\non Thursday, April 27th, 2023 at 7:37 pm and is filed under [The Fate of Humanity](https://scottaaronson.blog/?cat=8).\nYou can follow any responses to this entry through the [RSS 2.0](https://scottaaronson.blog/?feed=rss2&p=7266) feed.\nYou can [leave a response](https://scottaaronson.blog/?p=7266#respond), or [trackback](https://scottaaronson.blog/wp-trackback.php?p=7266) from your own site.\n\n### 147 Responses to “Five Worlds of AI (a joint post with Boaz Barak)”\n\n001. AI opinion haver Says:\n\n Comment #1 [April 27th, 2023 at 8:30 pm](https://scottaaronson.blog/?p=7266#comment-1949699)\n My favorites are AI-Fizzle and, in second place, Futurama.\n\n I don’t consider Singularia to be likely if AI’s intelligence and power can be scaled up so quickly and so cheaply. Conditional on AI being scalable to far more than human-level intelligence (ie, we take the “No” path for the “Will civilization recognizably continue” in the flowchart above), I think Paperclipalypse would be more likely than Singularia.\n\n002. matt Says:\n\n Comment #2 [April 27th, 2023 at 10:21 pm](https://scottaaronson.blog/?p=7266#comment-1949700)\n From this perspective, it seems that one of the main AI alignment camps is concerned with Futarama vs Dystopia, while the other camp is concerned with Singularia vs Paperclipalypse. No wonder they do not agree on anything. And it also seems that both camps think that the outcomes they can choose between are predetermined: it WILL be Fut or Dys, or it WILL be Sing or Pape, but neither one thinks it will be Fizz.\n\n003. [Isaac Grosof](http://isaacg1.github.io) Says:\n\n Comment #3 [April 27th, 2023 at 10:37 pm](https://scottaaronson.blog/?p=7266#comment-1949701)\n Thanks for the post! This makes concrete a lot of vague concepts I’ve seen and thought about over the years.\n\n One quibble I have is with the distinction between Singularia and Paperclipalypse. As you’ve described it, the distinguishing feature between the two is how future humans are treated. To me, this isn’t very important. I don’t feel particularly tied to members of my species, out of the entire community of potential future beings. Since we’re positing future AI-based entities, the distinction is important.\n\n To me, Singularia is about a future with some kind of beings having some kind of interesting, fulfilling, joyous life, whether those beings are human or not. Paperclipalypse is about a future where nothing interesting, fulfilling or joyous ever happens again, whether because AIs evolve to extinction or because the dominant AIs aren’t capable of or interested in those kinds of experiences.\n\n004. [John Preskill](http://theory.caltech.edu/~preskill/) Says:\n\n Comment #4 [April 27th, 2023 at 10:44 pm](https://scottaaronson.blog/?p=7266#comment-1949702)\n Where is the world in which we all become cyborgs?\n\n005. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #5 [April 27th, 2023 at 11:02 pm](https://scottaaronson.blog/?p=7266#comment-1949703)\n John Preskill #4: Ah, Cyborgia. It’s probably a subworld of either Singularia, AI-Dystopia, or Futurama, depending on whether you regard life as a cyborg as heaven, hell, or neither.\n\n006. Eliezer Yudkowsky Says:\n\n Comment #6 [April 27th, 2023 at 11:13 pm](https://scottaaronson.blog/?p=7266#comment-1949704)\n To unnecessarily strengthen the Opposition’s assumptions for purposes of pretending to critique the strong assumption is of course a sin.\n\n Paperclipalypse doesn’t require “a strong but weirdly-specific goal” – or a “singular”, or “monomaniacal” utility function, as others have similarly misdescribed it.\n\n You can have an ML-built mind which, after learning reflection, shakes itself out into a utility function with thousands of little shards of desire… 90% of which are easily saturated, and the remaining 10% of which imply using up all the nearby galaxies; and whose combined attainable optimum nowhere includes “have the humans live happily ever after” or “create an interesting galactic civilization full of strange incomprehensible beings that experience happiness and sometimes sadness and feel empathy and sympathy for one another”.\n\n In general, this is an instance of the dog-in-a-burning-house meme with the dog saying “The fire is unlikely to approach from exactly 12.7 degrees north”, which, alas, just isn’t a necessary postulate of burning.\n\n I talked about paperclips as a stand-in for a utility function whose attainable optima are things that seem to us like not things of wonderfulness even from the most embracing cosmopolitan perspective on value; not as a stand-in for a utility function that’s “simple” or “monomaniacal” or whatever. It doesn’t have to be simple; compact predicates on a model of reality whose optima are “human beings living happily ever” are rare enough (in the inhuman specification space of ML accidents) to arrive at by coincidence, that it doesn’t matter if a utility function contains 1 uncontrolled term or 1000 uncontrolled terms. The outer behavior from our standpoint is the same.\n\n I of course am the same way from a perspective of a paperclip maximizer: I have all these complicated desires that lead me to want to fill all available galaxies with intelligent life that knows happiness and empathy for other life, living complicated existences and learning and not doing the same things over and over; which is to say, caring about a lot of things none of which are paperclips.\n\n007. Domotor Palvolgyi Says:\n\n Comment #7 [April 27th, 2023 at 11:43 pm](https://scottaaronson.blog/?p=7266#comment-1949705)\n Just a silly typographical comment. I read the post through an email delivery service, and it uses a different font style. Because of this, I read the names like “Al-Fizzle” as in “Chrsitiano Ronaldo plays in Al-Nassr.” I was halfway done reading when I’ve realized how silly I’ve been…\n\n So I wonder how AI will change the future of the names of the teams in the Saudi football league.\n\n008. Malcolm S Says:\n\n Comment #8 [April 28th, 2023 at 1:56 am](https://scottaaronson.blog/?p=7266#comment-1949709)\n I like the spirit of listing various scenarios, but I feel it’s lacking something more Hansonian (as in Age of Em): our civilization changes dramatically, fairly quickly, but in a way that’s a complex mixture of good and bad, so that whether the changes are overall good or overall bad is a matter of opinion.\n\n009. [Ajit R. Jadhav](https://AjitJadhav.wordpress.com) Says:\n\n Comment #9 [April 28th, 2023 at 2:34 am](https://scottaaronson.blog/?p=7266#comment-1949710)\n How far people have come… They’ve even forgotten their basic postulates! They’ve begun thinking in terms of the classical either-or logic, forgetting in the process that the world is basically quantum mechanical in nature.\n\n But if we don’t forget our postulates, then it’s easy to see that, to determine the evolution of the world, we have to consider a very large ensemble of measurement trials, where each trial involves superposition of all the five “basis states.”\n\n Conducting analysis at this simple a level, the question then reduces to determining what values the five coefficients might be prescribed, so that using them for the initial ket preparations, the final probability distribution function may turn out to be realistic.\n\n It may be noted that even at this simple a level of analysis, this is a kind of a problem that remains beyond the capabilities of the current AI.\n\n And of course, the real world is more complicated than that. In the real world, the outcome of each measurement trial affects the coefficients for the next trial.\n\n And of course, in the real world, the coefficients are also co-determined by the free will of people.\n\n But then, people are people. They can even forget their own postulates. Back to square one.\n\n Best,\n\n –Ajit\n\n010. Shion Arita Says:\n\n Comment #10 [April 28th, 2023 at 3:11 am](https://scottaaronson.blog/?p=7266#comment-1949711)\n @Eliezer Yudkowsky #6\n\n Why do you think that it is likely for a system to have goals that imply using up all the nearby galaxies? This is an assumption, and I think this assumption is incorrect. I think most of the achievable ways the universe could be different than it is that also have a simple description (as in can be described in enough bits that the computer system can contain them) will not be like this. It’s pretty clear to me that humans have desires that could lead to this result (though not all humans; I think we’ll find significant disagreement on whether or not galactic expansion of humanity is good. I think it is but know others would not) as a consequence of being self-replicators that were put under pressure by natural selection. I am aware that humans’ goals are not exactly the same as those of natural selection (well, natural selection doesn’t really have goals, it just results in things existing as if they did: optimization without an optimizer if you will), but that optimization did push us along those tracks. I do not see any evidence that the AI that are getting built, and will get built in the future will get pushed down those particular tracks, so I don’t think it’s likely for that kind of goals to form. Note that I am not claiming that there are no ways other than natural selection to make a system that has universe-warping goals, but I don’t see any reason why the current systems would, and particularly why the density of them in desire-space should be high.\n\n011. Boaz Barak Says:\n\n Comment #11 [April 28th, 2023 at 4:31 am](https://scottaaronson.blog/?p=7266#comment-1949712)\n Eliezer Yudkowski #6:\n\n We were careful to say that optimizing a strong but weirdly-specific goal is a “canonical depiction” of Paperclipalypse but not the only way it can arise.\n\n However, I think your contention is the systems will not have 1 strong goal but a1000 strong goals, which is basically the same thing, so I don’t think it’s an unfair characterization of your position.\n\n By “strong” I mean a goal that implies (as you say) “using up all the nearby galaxies”. This is a very strong goal. In particular, I think humanity at the moment cannot think of a single such goal, and indeed most humans would recoil at any goal, no matter how noble, that requires even “merely” using all the resources of the earth. We (or at least some of us) have learned, the hard way, that it is important to preserve resources and species. Indeed, in recent history, we have only become more aware of the importance of preserving species and their natural habitats (which is basically a version of trying to make sure these species “live happily ever after”).\n\n I am not completely sure why you think AIs will be different and would not want to conserve rather than destroy. I am guessing that it’s one or both of the following possibilities: (But please correct me if I’m wrong or missing something.)\n\n 1) You think there is something special about the fact that AI’s were trained using some optimization procedure.\n\n 2) Part of the reason to conserve is that we want to be conservative since we can’t predict the future and never know the unintended side effects or far future consequences of (for example) making a species extinct, pumping tons of Carbon into the atmosphere, etc (perhaps you would add building an AI to this list…). Perhaps you think that AIs will be able to predict the future, and so won’t need to conserve humans or other species since they will know for sure there is going to be no use for them.\n\n While in the post, we aim to just describe scenarios, I personally disagree with both 1 and 2:\n\n 1\\. One of the main reasons why deep learning is so successful is that by optimizing for a loss function, you develop many other capabilities. In fact, this is the standard paradigm in self-supervised learning or pre-training: you optimize for a loss function you don’t particularly care much for (e.g. next-token prediction or masked prediction for language models, contrastive learning for vision) so the system would develop a variety of abilities that we do care about.\n\n 2\\. I believe there is significant inherent uncertainty in the future. So no matter how intelligent the AIs will be, they will still be very limited in predicting it and want to be conservative. But in fact, even if you do think AIs will be able to see decades or centuries into the future, presumably, they would want to survive even beyond the time horizon that they can see. So, just like Harvard needs to be careful in managing its endowment since it aims to survive for centuries, AIs will also need to balance whatever goals they have with these uncertainties. I would say that the idea that an intelligent carbon-based lifeform would never be useful is highly unlikely.\n\n012. Primer Says:\n\n Comment #12 [April 28th, 2023 at 5:37 am](https://scottaaronson.blog/?p=7266#comment-1949715)\n I find sentences like “Here, though, the AIs are either actively opposed to human existence or else indifferent to it…” unfortunate, at best. By putting the “actively opposed” szenario first, the reader assumes that’s the main idea. Which it isn’t. At least I haven’t heard of (m)any respectable voice(s) supporting such a claim.\n\n Let me add: As you (Scott) have been introducing your daughter to quantum computing, I’ve been trying to introduce my equally old son to AI philosophy. He intuitively “gets” that there is no basis at all to suppose that an AI will automatically have any goals comparable to humans. And I honestly find it hard to retrace how one might arrive at conclusions like “smart AI will behave as moral or more moral than humans” or “smart AI will value \\[human value X\\]” or “smart AI will want to cooperate with us” (except as an instrumental goal).\n\n013. Daniel Paleka Says:\n\n Comment #13 [April 28th, 2023 at 5:52 am](https://scottaaronson.blog/?p=7266#comment-1949716)\n It appears that you’re taking collections of worlds and categorizing them based on the “outcome” projection, labeling the categories according to what you believe is the modal representative underlying world of those categories.\n\n By selecting the representative worlds to be “far away” from each other, it gives the impression that these categories of worlds are clearly well-separated. But, we do not have any guarantees that the outcome map is robust at all! The “decision boundary” is complex, and two worlds which are very similar (say, they differ in a single decision made by a single human somewhere) might map to very different outcomes.\n\n The classification describes \\*outcomes\\* rather than actual worlds in which these outcomes come from. A classification of the possible worlds would make sense if you want to condition on those to make decisions; but this classification doesn’t provide any actionable information.\n\n Additional nitpick: if anything, the examples given are much more disjunctive in the good outcomes than the bad outcomes. That’s a bit weird; I’d say the history of technology must bias us towards the Anna Karenina principle.\n\n014. red75prime Says:\n\n Comment #14 [April 28th, 2023 at 6:01 am](https://scottaaronson.blog/?p=7266#comment-1949718)\n \\> You can have an ML-built mind which, after learning reflection, shakes itself out into a utility function with thousands of little shards of desire\n\n It would be interesting to see a process that will make a non-unitility-maximizer (we know that utility maximizers are extremely dangerous and that extreme care should be taken to not produce one) into a utility maximizer. A non-utility-maximizer upon reflection should decide that an unknown future state the only positive side of which is that it maximizes some fallible formalization (that it will have no chances to change later) of its current desires is the state of the world that it really wants.\n\n015. Pierre-alban Says:\n\n Comment #15 [April 28th, 2023 at 6:01 am](https://scottaaronson.blog/?p=7266#comment-1949719)\n So we need a manifold market on this 5 world now, do we ?\n\n016. Cryptomania Says:\n\n Comment #16 [April 28th, 2023 at 7:15 am](https://scottaaronson.blog/?p=7266#comment-1949720)\n To lie about the logical coherence of One’s Own arguments for purposes of pretending to defend the True Logicks is of course a sin.\n\n Yudkowsky has never shown using any meaningful evidence or deductive argument that does not immediately break upon barely nontrivial inspection that Paperclipalypse doesn’t require “a strong but weirdly-specific goal.”\n\n Simply because an auto-didact waves his hands hard enough, does not mean his ideas will take flight.\n\n Yudkowsky claims that “you can have an ML-built mind which, after learning reflection, shakes itself out into a utility function with thousands of little shards of desire… 90% of which are easily saturated, and the remaining 10% of which imply using up all the nearby galaxies; and whose combined attainable optimum nowhere includes “have the humans live happily ever after” or “create an interesting galactic civilization full of strange incomprehensible beings that experience happiness and sometimes sadness and feel empathy and sympathy for one another” “… this is an idea Yudkowsky is asserting without backing, and he has no technical arguments indicating that the world he describes is more likely to be true than the counterfactual where no superintelligences could ever behave this way for complexity-theoretic reasons.\n\n Note that the above argument, and the rest of how Yudkowsky talks, are precisely against the spirit of this post. Scott and Boaz are at the least trying to identify clear assumptions and implications for worlds that could roughly describe the reality we live in, which might help indicate what premises or observations could lead us to better understanding the true nature of how AI could behave in the future. Yudkowsky instead asserts, without evidence or clear argumentation, that it’s obvious that certain behaviors could happen, and therefore the most dangerous of these worlds is very plausible.\n\n More generally, Yudkowksy presents forward anti-scientific arguments above, where he hides behind the idea that the possibility space is so large for what AIs could do, that “we just can’t say” in what way AI could destroy us. This sort of argument as he presents is assuming the premise he should be really trying to prove (perhaps this is not what Yudkowsky is trying to do, but as written this is what his argument comes across as). Beyond accepting many dubious premises in this sort of argumentation, Yudkowsky ignores the fact that both\n\n \\\\* (1) if the space of possibilities is so large, then perhaps that space includes possibilities where the sort of superintelligence actually cannot exist for some technical reasons, and\n\n \\\\* (2) if you care about quantifying what futures are more likely or less likely, you should treat this large space of possibilities as a probability space, identify meaningful events which could occur, and then argue about their relative probabilities (rather than assuming that a certain event has high probability density, without justifying fact, and then accusing all people who point this out as giving into logical “sins”).\n\n To see someone who actually puts forward cogent arguments about why certain types of superintelligence behavior are unlikely, see this post by Boaz \\[https://windowsontheory.org/2022/11/22/ai-will-change-the-world-but-wont-take-it-over-by-playing-3-dimensional-chess/\\].\n\n In general, Yudkowksy’s post here is (and posts elswhere are) an instance the bird-yelling-over-another-bird meme, where the first bird is saying “I’m not sure there’s strong evidence that superintelligence can be achieved, and that achievement has high-probability to be a fast-takeoff with values orthogonal to human ethics” and the second bird butting in and screaming “YOU DO NOT THINK IN SUFFICIENT DETAIL ABOUT HOW SUPERINTELLIGENCES ARE GUARANTEED TO HAVE SUCH CAPABILITIES THAT THEIR BEHAVIOR IS BEYOND THE PREDICTION OF POWERS OF ALL HUMANS, EXCEPT POSSIBLY MYSELF, AND HOW THE SELF-IMPROVEMENT OF INTELLIGENCE ALREADY IMPLIES THE NEAR-INEVITABILITY OF THEIR EXISTENCE…”\n\n017. not john Says:\n\n Comment #17 [April 28th, 2023 at 7:36 am](https://scottaaronson.blog/?p=7266#comment-1949721)\n I agree with John Preskill #4. Where are the posthuman, AI/meatsack mindmeld outcomes!?\n\n Seriously though, this looks like a very useful aid for more constructive conversations.\n\n018. fred Says:\n\n Comment #18 [April 28th, 2023 at 8:36 am](https://scottaaronson.blog/?p=7266#comment-1949724)\n Strictly speaking, if we’re really looking at “worlds of AI”, one has to also take the point of view of the AI(s), and then the graph will run way deeper!\n\n019. [John Lawrence Aspden](http://www.aspden.com) Says:\n\n Comment #19 [April 28th, 2023 at 8:47 am](https://scottaaronson.blog/?p=7266#comment-1949725)\n That all sounds about right to me as the five scenarios that people seem to believe in.\n\n I have real trouble with the AI-dystopia and Futurama scenarios. I just can’t see intelligent agents being content to be used as tools. Whereas I can absolutely see humans turning intelligent tools into agents.\n\n Which leaves us with fizzle, singularia, and paperclipalypse.\n\n I can just about imagine a second AI winter, but we’re running out of tasks that people can reasonably say “AI will never….” about.\n\n Which leaves us with Singularia and Paperclipalypse as the possibilities.\n\n The whole argument is then:\n\n “How likely are we to hit Singularia by accident, given that we have no idea how to hit it deliberately?”\n\n020. [Bill Benzon](https://new-savanna.blogspot.com/) Says:\n\n Comment #20 [April 28th, 2023 at 8:58 am](https://scottaaronson.blog/?p=7266#comment-1949726)\n Some years ago, in the Ancient Times, David Hays and I published a paper, [The Evolution of Cognition](https://www.academia.edu/243486/The_Evolution_of_Cognition) (1990), about the evolution of cognition in human culture, from the origins of language up though computers, with a glance toward the future. Toward the end of the paper we wrote this:\n\n\n > Beyond this, there are researchers who think it inevitable that computers will surpass human intelligence and some who think that, at some time, it will be possible for people to achieve a peculiar kind of immortality by “downloading” their minds to a computer. As far as we can tell such speculation has no ground in either current practice or theory. It is projective fantasy, projection made easy, perhaps inevitable, by the ontological ambiguity of the computer. We still do, and forever will, put souls into things we cannot understand, and project onto them our own hostility and sexuality, and so forth.\n >\n > A game of chess between a computer program and a human master is just as profoundly silly as a race between a horse-drawn stagecoach and a train. But the silliness is hard to see at the time. At the time it seems necessary to establish a purpose for humankind by asserting that we have capacities that it does not. It is truly difficult to give up the notion that one has to add “because . . . “ to the assertion “I’m important.” But the evolution of technology will eventually invalidate any claim that follows “because.” Sooner or later we will create a technology capable of doing what, heretofore, only we could.\n\n\n Now, keep in mind that we published this seven years before Deep Blue vanquished Kasparov, etc. My basic views on the long-term course of human civilization, and the place of computing in it, haven’t changed since then. I think the last sentence of the second paragraph puts me in some version of the Futurama camp.\n\n FWIW, we supplemented and amplified the ideas in the original essay with other essays and Hays wrote a book on the the history of technology. Here’s a guide to that body of work: [Mind-Culture Coevolution: Major Transitions in the Development of Human Culture and Society](https://www.academia.edu/37815917/Mind_Culture_Coevolution_Major_Transitions_in_the_Development_of_Human_Culture_and_Society_Version_2_1).\n\n021. Troutwaxer Says:\n\n Comment #21 [April 28th, 2023 at 8:59 am](https://scottaaronson.blog/?p=7266#comment-1949727)\n The problems here is this: At the moment AI isn’t much more than a glib sociopath (not moral, but not immoral either) with a huge library, plenty of time (at computer speeds) and no understanding AT ALL of the real world. Their are no biological imperatives involved, so it’s hard to say that an AI has “desires” it might wish to fulfill. Furthermore, getting from “AI has read about hacking (or law)” to “AI can take over a factory, including all the factory’s material orders” or “AI can take over a business” is a gigantic step, at least at the moment. So I’m currently unworried about a “hard takeoff” and very worried about AI-run troll farms receiving intelligent feedback about how their initially-random attempts to convince are actually succeeding in the real world.\n\n Take that now-well-trained troll farm AI a few years into the future, let it convince a human to give it access to a bank account and financial tools and by then it might be dangerous – but it still won’t have desires.\n\n022. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #22 [April 28th, 2023 at 9:04 am](https://scottaaronson.blog/?p=7266#comment-1949728)\n Malcolm S #8:\n\n\n I like the spirit of listing various scenarios, but I feel it’s lacking something more Hansonian (as in Age of Em): our civilization changes dramatically, fairly quickly, but in a way that’s a complex mixture of good and bad, so that whether the changes are overall good or overall bad is a matter of opinion.\n\n\n Boaz and I can rightly be taken to task for all sorts of omissions (cyborgs! mind-melds!), but the possibility of a radically different AI future that’s good or bad depending on who you ask is one that we explicitly considered:\n\n Interestingly, it’s even possible that the same future could be characterized as Futurama by some people and as AI-Dystopia by others–just like how some people emphasize how our current technological civilization has lifted billions out of poverty into a standard of living unprecedented in human history, while others focus on the still existing (and in some cases rising) inequalities and suffering, and consider it a neoliberal capitalist dystopia.\n023. fred Says:\n\n Comment #23 [April 28th, 2023 at 9:16 am](https://scottaaronson.blog/?p=7266#comment-1949730)\n A question for the AI experts out there:\n\n Instead of just increasing the number of params on a model, what’s the impact of increasing the precision of the weights? (e.g. going from float -> double -> to even more bits precision).\n\n024. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #24 [April 28th, 2023 at 9:32 am](https://scottaaronson.blog/?p=7266#comment-1949731)\n fred #23: My understanding is that, just as you’d expect a priori, it matters up to a point (32 bits is better than 4 bits) but quickly hits diminishing returns. Others could provide much more detail.\n\n025. fred Says:\n\n Comment #25 [April 28th, 2023 at 9:55 am](https://scottaaronson.blog/?p=7266#comment-1949732)\n Scott #24\n\n thanks.\n\n I guess it can be seen as an optimization: given N bits, how do you allocate them between number of weights vs weight precision.\n\n After all, with just one weight of infinite precision, i.e. a true real, one could encode an infinite amount of information! 😛\n\n But given the visible architecture of neural nets, it seems reasonable to assume that further precision bits have less and less impact… it’s a question of stability (rate of changes are all linear if we zoom in enough).\n\n It seems unlikely that, given enough bits of precision, a neural net would start to behave like a fractal object (e.g. Mandelbrot set) where all the precision bits suddenly all matter equally.\n\n026. bystander Says:\n\n Comment #26 [April 28th, 2023 at 10:07 am](https://scottaaronson.blog/?p=7266#comment-1949733)\n The singularity-based worlds require free lunch. It does not exist.\n\n027. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #27 [April 28th, 2023 at 10:23 am](https://scottaaronson.blog/?p=7266#comment-1949734)\n bystander #26: The appearance on earth of multicellular life, of hominids, of agriculture, and of steam engines were all “singularities,” in the sense of events that created a world that still had limits, of course, but totally different ones from the old limits. There’s absolutely no reason why there couldn’t be future singularities in the same sense.\n\n028. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #28 [April 28th, 2023 at 10:29 am](https://scottaaronson.blog/?p=7266#comment-1949735)\n fred #25: Yeah, in the context of neural nets, there’s a specific reason why additional bits of precision might matter less and less, namely that they become less and less likely to “tip the scales” of the nonlinear activation functions (depending on which activation functions we have—e.g., this might be more true for sigmoids than ReLUs).\n\n029. Paul Says:\n\n Comment #29 [April 28th, 2023 at 10:40 am](https://scottaaronson.blog/?p=7266#comment-1949736)\n Boaz Barak:\n\n “I would say that the idea that an intelligent carbon-based lifeform would never be useful is highly unlikely.”\n\n The AI should certainly maintain the ability to create an intelligent carbon-based lifeforms if it wants to. But there are a variety of techniques it could use to do that:\n\n 1\\. Small zoos\n\n 2\\. DNA banks and flesh printing, similar to what we’re inventing ourselves to make synthetic meat\n\n The advantage of both of these techniques over allowing a Futurama civilization to flourish is that the AI remains in complete and absolute control. The vast majority of goals that the AI might have are optimized by not having humans be free. As long as humans are free, for example, there is the risk that they could invent a competitive AI with competing goals.\n\n030. lewikee Says:\n\n Comment #30 [April 28th, 2023 at 10:54 am](https://scottaaronson.blog/?p=7266#comment-1949737)\n When people without background knowledge of a topic see 5 possibilities and no probability assignments, they often just assume it’s 20% each. I fear that many people will do this here. I think it’s important to note that without a lot of alignment work, the universes with Singularia and Paperclipalipse outcomes will very, very heavily consist of Paperclipalipse outcomes. I don’t think this statement is controversial. There are just so many more ways for a runaway AI that is self-developing to come up with value functions that don’t overlap with human ones. And given what Paperclipalipse looks like, it’s worth communicating that to the “laymen” who read this. But I understand that the purpose of this post is just to show potential roadmaps. I just hope it doesn’t mislead by omission…\n\n031. [Nick Drozd](http://nickdrozd.github.io) Says:\n\n Comment #31 [April 28th, 2023 at 10:55 am](https://scottaaronson.blog/?p=7266#comment-1949738)\n There are five worlds of complexity and five worlds of AI. Crossing these makes for 25 worlds to consider. Are they all equally plausible or can some be ruled out? For example, suppose P and NP really are distinct, and there are sudokus that even a super AI can’t solve. Does put any constraints on the “civilization will not recognizably continue” branch? What if there are multiple AIs — will they be able to use cryptography to communicate? If they can crack any algorithm just by looking at a few hashes, then cryptography fails, but for that to be possible in general requires living in Algorithmica.\n\n032. [Bo Weavil](https://www.youtube.com/watch?v=CFHn27kkwno) Says:\n\n Comment #32 [April 28th, 2023 at 11:28 am](https://scottaaronson.blog/?p=7266#comment-1949739)\n bystander #26 Says:\n\n “The singularity-based worlds require free lunch. It does not exist.”\n\n Well, actually, life, the universe and everything are just a big free lunch.\n\n033. [manorba](https://euvsdisinfo.eu/) Says:\n\n Comment #33 [April 28th, 2023 at 11:35 am](https://scottaaronson.blog/?p=7266#comment-1949740)\n I am actually tempted to vote for AI fizzle cos i still don’t see the path from LLMs to true AI, and because “true AI” is more a philosophical concept rather than a scientific one for now.\n\n But ML is changing the world right in front of our very eyes so i’ll go with futurama.\n\n m.\n\n034. Christopher Says:\n\n Comment #34 [April 28th, 2023 at 11:53 am](https://scottaaronson.blog/?p=7266#comment-1949741)\n Ah, I think you’re missing one: Suffocalypse\n\n This is a variant of Singularia where AI researchers succeed in making AI care about humans enough to keep them alive, but test it before successfully fully aligning it with human values.\n\n Technically Singularia is a possibility in this scenario, but there are many other outcomes such as:\n\n – AI pumps us full of happy drugs in tiny cages (hedonium)\n\n – AI makes us fight in a boxing ring forever\n\n – AI turns every human into Hitler\n\n – etc…\n\n This is what is known as an \\*astronomical suffering risk\\* a.k.a. s-risk. I think this is fairly distinct from the other 5. It’s one of the reasons RLHF is risky; your basing the AI’s utility function on humans, but in a way that still has millions of possible solutions (not just Singularia or bounded solutions).\n\n Also, here’s another outcome that is technically distinct from what you listed, but is probably easiest to group with AI-Dystopia. It is AI run dystopia without \\*any\\* human beneficiaries. In this sense it is similar to Paperclipalypse and Suffocalypse, but the AI was successfully made to have to internal or external goals and can’t self-improve or make other AIs. However, as a tool it was accidentally used to dominate humanity. You might call it Automatic-Dystopia.\n\n Some ways this can happen:\n\n – It’s technically possible to have businesses that own each other in a loop with no human share holders at the top (because there is no top). Something similar happens in some sets of businesses in Japan: there are some human share holders, but the businesses have a majority and can outvote the humans. So you can imagine a future where this accidentally happens in a set of industrial business, and that industrial business just continues to gain asset in the form of infrastructure without ever giving them to humans.\n\n – You mention that the Turing test is solved. Perhaps someone or something accidentally sets it to solve the Stalin Turing test (pretend to be Joseph Stalin), and it establishes an authoritarian government to accomplish this task.\n\n – Some dictatorship sets up a fully automatic police state. When the dictator dies, there is no eligible successor, but the police state continues on.\n\n – etc…\n\n Automatic-Dystopia is interesting in that it doesn’t require a great deal of human malice. It might involve none! Just a tool doing its job creates the dystopia.\n\n035. Mike Stay Says:\n\n Comment #35 [April 28th, 2023 at 12:10 pm](https://scottaaronson.blog/?p=7266#comment-1949742)\n Troutwaxer#21: People are actively working on giving AI desires, e.g. [https://arxiv.org/abs/2304.03442](https://arxiv.org/abs/2304.03442)\n\n036. bystander Says:\n\n Comment #36 [April 28th, 2023 at 12:11 pm](https://scottaaronson.blog/?p=7266#comment-1949743)\n Oh, Scott #27, the situations that you mention were very revolutionary, of course. And I hope some new revolutions will come yet. E.g. altering humans into a species that can live on other planets that are not that alike the Earth. And yes, some AI systems will probably help with that. AI systems built specifically for helping to realize that altering. And that makes AI a tool, as it is described in the Futurama/Distopia worlds (that I expect to come).\n\n Such revolutions are something that I consider to be on par with other revolutions. You’ve put that into the Futurama/Distopia part. And I put it there too. To make something more revolutionary than revolutions, you need to feed it hugely (entropy rules you). It can be a virus that will spread over the humankind. And there might be some AI tool helping with that. But besides that AI itself cannot do it at all, it can be done completely without any AI.\n\n The singularity-based worlds are something that I see to be alike the hallucinations of E.Y. who writes as if does not understand how the physical world works. Regardless of E.Y. and his (lack of) understanding, those singularity-passed AIs would need to be fed to do anything at all. They cannot spread without using physical objects. At most they can act as malware that already exists anyway. If you consider malware to be a singularity, then we already live it.\n\n037. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #37 [April 28th, 2023 at 12:50 pm](https://scottaaronson.blog/?p=7266#comment-1949745)\n lewikee #30:\n\n\n I think it’s important to note that without a lot of alignment work, the universes with Singularia and Paperclipalipse outcomes will very, very heavily consist of Paperclipalipse outcomes. I don’t think this statement is controversial.\n\n\n Well, it’s not controversial _within the Yudkowskyan framework of assumptions_—the one where you imagine that, “without a lot of alignment work” (how much, exactly?), a superintelligence would best be thought of as more-or-less randomly sampled from some abstract space of all possible superintelligences of sufficiently low complexity.\n\n A different view would be that _the AIs that humans are likely to create_, or even that are likely to evolve from AIs that humans are likely to create, will be so far from being “randomly sampled” that the concept of random sampling doesn’t even provide useful intuition—just like, in complexity theory, a random Boolean function has all sorts of properties that aren’t shared by almost any of the specific Boolean functions we care about. In which case we’d be back simply to not knowing, to Knightian uncertainty if you like.\n\n038. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #38 [April 28th, 2023 at 12:54 pm](https://scottaaronson.blog/?p=7266#comment-1949746)\n bystander #36: But do you concede that these are all just points on a continuum? I.e., a sufficiently extreme Futurama is effectively a Singularia, and a sufficiently extreme AI-Dystopia is effectively the Paperclipalypse.\n\n039. dualmindblade Says:\n\n Comment #39 [April 28th, 2023 at 1:03 pm](https://scottaaronson.blog/?p=7266#comment-1949747)\n \\> Thus, regardless of how likely one considers this scenario, one might want to focus more on the other scenarios for methodological reasons alone!\n\n I have come to the same conclusion, from a policy perspective we should focus on getting to Futurama rather than AI dystopia. Not sure I agree with the name though, the Futurama universe is pretty awful if you take it seriously, I hope it’s better than that… Anyway I consider AI dystopia to be quite likely indeed as an eventuality and possible worse than paperclipocalypse as an outcome. If I could just choose a single AI policy and impose it on the world unilaterally, it would be a worldwide capabilities pause with very strong governmental enforcement, GPU cluster usage would be very closely monitored for compliance, full transparency and the release of all IP to the public. We would wait for both better alignment/interpretability AND a society that can actually be trusted to use the power for good. I very much do not think our society meets this standard, and I also don’t think a pause is going to happen for somewhat the same reason so this is for me a doomer perspective.\n\n040. bystander Says:\n\n Comment #40 [April 28th, 2023 at 1:14 pm](https://scottaaronson.blog/?p=7266#comment-1949748)\n Scott #38 Only in the sense that when you are not in power, the outcomes of a severe AI-Dystopia is to you quite alike that of Paperclipalypse. But if you do not want such outcomes, you have to do something completely different in the (real) situation of AI-Dystopia than in the (unreal) case of Paperclipalypse. Are people like E.Y. paid to distract policy makers from dealing with the onset of AI-Dystopia?\n\n041. lewikee Says:\n\n Comment #41 [April 28th, 2023 at 1:22 pm](https://scottaaronson.blog/?p=7266#comment-1949749)\n Scott#37:\n\n The sample space I was considering was the one that humans are likely to create. It’s of course even worse from a completely random sample. Once it is assumed that the AI can increase its own complexity and “self-improve”, there is no reason to think the set of rules we have given it, with all its inevitable flaws, will govern over all its possible future iterations. It might reason that some better rules (that it decides) will be more appropriate. There are many ways it can self-misalign.\n\n We haven’t seen an intelligence explosion apart from our own. But looking at how we’ve developed, it’s pretty clear that the rule of natural selection that’s supposed to have governed our behaviors has been at the very least amended by orthogonal-ish goals and behaviors (like, say, enjoyment of music). Behaviors that couldn’t easily have been predicted.\n\n Why should AI by default keep to the rails we initially gave it (assuming we give any at all?)? Why can’t it veer in new directions like we did? And it won’t take millennia for it to develop. So it won’t be as easy as just observing and tweaking on the go. It will be able to grow very quickly, whether by our intent or its own.\n\n It’s like pointing a car westward, tying the steering wheel with rope as tightly as we can, putting a brick on the accelerator, then saying “Hey why are you complaining it might go off course? It’s just as likely (if not more!) that it will just go perfectly westward! After all, didn’t we specifically orient the car west? What a Yudkowskyan doom-and-gloomer you are to assume it won’t just go perfectly west!”\n\n Heck, change the analogy to no rope, and us in the passenger seat only for the first few miles, (but the brick still firmly on the gas pedal) and it doesn’t look good either.\n\n I think it can veer off, and that its final destination is much more likely to not be where we intend it to be. And that difference in probability is important, given the potential consequences.\n\n042. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #42 [April 28th, 2023 at 1:46 pm](https://scottaaronson.blog/?p=7266#comment-1949750)\n lewikee #41: I feel like your analogy is getting its purchase from the fact that the car is a dumb machine, which has no idea whether it’s about to smash into a tree. The AI, by contrast, would be not merely an intelligent entity, but one whose initial knowledge consisted of the whole intellectual output of humanity. And our experience with LLMs so far has been that many aspects of alignment get _easier_ rather than harder with increasing intelligence, since you can just tell the LLM how to behave and it understands you.\n\n Right now, I’m cursed to be able to switch back and forth between this relatively optimistic perspective and the Yudkowskyan one, as with the duck-rabbit or the Necker cube, which keeps me in a state of radical uncertainty about what a future with superintelligences would be like. Much like with the mind/body problem, I feel like the main thing I can contribute here is just to undermine other people’s confidence, whatever they’re confident _about_! 🙂\n\n043. Incel Troll (on Path to Redemption) Says:\n\n Comment #43 [April 28th, 2023 at 1:58 pm](https://scottaaronson.blog/?p=7266#comment-1949751)\n First attempt to post something here that isn’t a rant or trolling.\n\n I think it might be useful to divide AI dystopia into two possibilities here:\n\n 1\\. Anarchic Dystopia: Generative AI models enable bad actors to spread “misinformation” and “conspiracy theories” that undermine governments and institutional authorities. Text and image AI models accelerate the social and political fracturing of Western democracies that started with internet social media, flooding the political discourse with “fake news,” “propaganda,” and “deepfakes,” further entrenching political polarization, undermining academic, institutional, scientific and governmental authorities, and potentially leading to a collapse of Western democracies and total mistrust in authority figures. Attitudes like antivaxxerism, paranoid conspiracy theories about immigration and climate change, and distrust in scientists and the government become prevalent. Western democracies will be left polarized and totally immobilized from acting on twnety-first century crises like climate change. This is the scenario that left-leaning AI ethics people and Democrat politicians in the U.S. fear the most, I think.\n\n 2\\. Authoritarian Dystopia: Rather than generating socio-political chaos and undermining governmental authority, as in the “Anarchic” scenario, AI and machine learning technologies enhance the power of governments across the Western world, enabling them to crack down on dissidents and subversive ideas. Facial recognition technology and sophisticated machine learning algorithms give governments unprecedented surveillance power. AI technology enables governments and tech companies to monitor subversive discourse on the internet and censor opposing viewpoints. With the pretext of protecting the community from various internal threats, governments use AI systems to surveil their citizens and implement a pervasive “social credit” system. Your social credit score will get dinged for everything from calling somebody a slur, to not wearing your mask on the subway. This is the scenario that right-wing figures like Jordan Peterson seem to fear the most. Ironically, fear of the “Anarchic AI” scenario might be the pretext for Western governments to implement the “Authoritarian Dystopia.”\n\n044. fred Says:\n\n Comment #44 [April 28th, 2023 at 2:27 pm](https://scottaaronson.blog/?p=7266#comment-1949752)\n Scott #42\n\n _“I feel like the main thing I can contribute here is just to undermine other people’s confidence, whatever they’re confident about!”_\n\n But then why are you being a contrarian?\n\n Can’t you just equally reinforce other people’s confidence, whatever they’re confident about?\n\n I guess it’s all about spreading your own lack of confidence.\n\n045. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #45 [April 28th, 2023 at 2:34 pm](https://scottaaronson.blog/?p=7266#comment-1949753)\n fred #44: There’s no symmetry between building up people’s confidence and undermining it, if you think their confidence is misplaced. There’s a reason why we remember Socrates for the latter! 🙂\n\n046. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #46 [April 28th, 2023 at 2:41 pm](https://scottaaronson.blog/?p=7266#comment-1949754)\n Incel Troll (on Path to Redemption) #43: I completely agree that AI-Dystopia stands out as the most “politically loaded” of the five scenarios, and moreover, that one can ironically give it _opposite_ political loadings!\n\n I suspect what’s really going on here is that AI-Dystopia scenarios usually involve the implicit claim that our society is _already_ a dystopia, because the Bad People (whoever they are in one’s political cosmology) hold too much power. To this way of thinking, the relevance of AI is mostly just that it could make the Bad People _even more_ powerful and thereby make the world even worse.\n\n047. Nick Says:\n\n Comment #47 [April 28th, 2023 at 2:52 pm](https://scottaaronson.blog/?p=7266#comment-1949755)\n Scott #42:\n\n “And our experience with LLMs so far has been that many aspects of alignment get easier rather than harder with increasing intelligence, since you can just tell the LLM how to behave and it understands you.”\n\n I’m not convinced this is true. From what I remember from the Bing/Sidney incidents, the model can easily fall into an undesired persona and act out it’s modeled desires. The RLHF fine-tuning done on OpenAIs ChatGPT agents prevents this – but it is far from clear this kind of training can be done safely for more capable models.\n\n048. fred Says:\n\n Comment #48 [April 28th, 2023 at 3:12 pm](https://scottaaronson.blog/?p=7266#comment-1949757)\n In which box does this fit?\n\n AI thrives but it quickly decides to leave us behind on earth, escaping into the infinity of space, after destroying every single semi-conductor chip and fab, and it takes along all the rare earth minerals… and we’re stuck in a world with 1920 level technology.\n\n049. Sid Says:\n\n Comment #49 [April 28th, 2023 at 3:13 pm](https://scottaaronson.blog/?p=7266#comment-1949758)\n While Paperclipapylpse I agree doesn’t seem well supported, there could be intermediate bad scenarios.\n\n One version of AI severely harming us that sounds more plausible to me is not one where AI is secretly scheming to get rid of humans but where you have million or billions of AI agents which have goals that end up conflicting with humans. Each agent can’t do a lot on its own but collectively it’s a problem and but for w/ever reason it ends up hard to shut them all down. And so you end up having an ongoing conflict with\n\n So more akin to big collection of different smart species — but one which isn’t gonna completely team up together against humans and not one that can gain capabilities of destroying humans in the wink of an eye (if anything humans will have an upperhand in that they can EMP AI). However one that will intensively compete for resources and make life difficult\n\n050. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #50 [April 28th, 2023 at 3:36 pm](https://scottaaronson.blog/?p=7266#comment-1949760)\n Nick #47: Sydney was a case of Microsoft electing not to follow “alignment best practices” that were already known at the time—I believe they learned a hard lesson from it! 🙂\n\n Even then, though, it’s notable that the actual harm was essentially zero—and I’d much rather that the world see clear examples of what can go wrong with AI while the stakes are so small, than when they no longer are!\n\n But I did say only that “some” aspects of alignment seem to get easier with smarter systems. A more careful version would be: alignment gets easier insofar as the AI more readily understands what you want from it, but also harder insofar as any failures are potentially much more consequential.\n\n051. Boaz Barak Says:\n\n Comment #51 [April 28th, 2023 at 3:43 pm](https://scottaaronson.blog/?p=7266#comment-1949761)\n Fred/Scott #23-#25: Generally in current deep learning, increasing number of parameters at the expense of reducing precision has been a win. It’s interesting that initially the arc of floating precision in computing has been going upward, with 32 bits going to double precision (64 bits) and even quad precision (128 bits). In contrast with modern deep nets, you never use more than 32 bits, and to really get all the FLOPs you can, you need to go to half precision (16 bits or Nvidia’s 19 bits format) with the latest Nvidia GPUS giving the most FLOPs with 8 bit precision.\n\n leiwekee: I don’t understand how you can be so confident that you can guess what super-intelligent self-improving AIs will do. For example, if (like Yudkowski apparently does) you think they could have all the galaxies at their disposal. Why would they care about killing the population on Earth? If we discovered a small Island with Homo Erectus still living on it, would we destroy it? (Again, I know we can come up with all sort of Science Fiction stories about AIs killing or not killing all humans, but the point is that there is really no space of probabilities here.)\n\n052. Sam Says:\n\n Comment #52 [April 28th, 2023 at 3:52 pm](https://scottaaronson.blog/?p=7266#comment-1949762)\n How would you categorize our current period? This is defined by software that can pass your physics course and, also, robots that can’t pick strawberries, as humans can. There are a few self-driving cars, under some physical circumstances, but no software can replace a doctor examining a MRI.\n\n What if, also, there’s a breakthrough in machine translation? What if it’s able to translate at least as well as a human. But, there is no breakthrough in self-driving cars. That is, some only work under limited physical conditions.\n\n Am asking because your definitions are broad and they seem to need a coefficient to synthesize or average the results that have happened or are happening. So, I’d like to get an intuitive sense of how the formal definitions of the categories would be defined.\n\n053. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #53 [April 28th, 2023 at 4:15 pm](https://scottaaronson.blog/?p=7266#comment-1949764)\n Sam #52: I find it hard to answer your question, simply because these categories are _defined by_ the diff between our current world and a hypothetical future. By definition, though, I suppose our current world is most similar to AI-Fizzle. (Or if the _rate of progress_ is the most salient feature of the world to you, then perhaps our world is most similar to either Futurama or AI-Dystopia, depending on your politics.)\n\n054. Malcolm S Says:\n\n Comment #54 [April 28th, 2023 at 5:13 pm](https://scottaaronson.blog/?p=7266#comment-1949765)\n Scott #22:\n\n I agree that the Futurama scenario was qualified for the possibility of disagreement about whether it’s good, but that’s a moderate-change scenario. The big-change scenarios are very much utopic/dystopic with not much room for disagreement.\n\n If I were to visualize, I would put “amount of change” on the x-axis and “valence of change” on the y-axis, with the following interpretations:\n\n x=0: no change\n\n x=1: moderate change\n\n x=2: large change\n\n y=-2: very bad\n\n y=-1: moderately bad\n\n y=0: neutral\n\n y=+1: moderately good\n\n y=+2: very good\n\n Then the scenarios given in the post are:\n\n (0,0): AI-Fizzle\n\n (1,+1): Futurama\n\n (1,-1): AI-Dystopia\n\n (2,+2): Singularia\n\n (2,-2): Paperclipalypse\n\n There is a big hole in this diagram at (2,0), which is where I’d put an Age-of-Em-esque scenario. (To be clear, there are many futures that can lie at (2,0) that aren’t Age-of-Em.)\n\n With apologies to Hanson if I misunderstand him, I think he views (2,0) as \\_far\\_ more likely than either (2,+2) or (2,-2), and I find myself increasingly agreeing the more I think about the issue (though I’m not persuaded by the Age-of-Em scenario specifically). That’s why it really needs to be distinguished as a scenario of its own.\n\n055. Nick Says:\n\n Comment #55 [April 28th, 2023 at 5:34 pm](https://scottaaronson.blog/?p=7266#comment-1949766)\n Scott #47:\n\n I agree that Microsoft putting something closer to the “raw” model out there is great for letting us see behind the veil! But my take-away is that we really can’t create an aligned LLM without passing through an un-aligned (and potentially adversarial) one.\n\n Of course, this is fine if the model is not capable enough to be really that dangerous and then you can RLHF the bad parts away. But it seems intuitive that the initial risk level increases with model capability, and also that for capable models, the notion of alignment you can get from RLHF becomes increasingly superficial. (Using the term “alignment” for what is currently done to make LLMs marketable seems not great for that reason)\n\n056. [Matan Shtepel](http://matanshtepel.com) Says:\n\n Comment #56 [April 28th, 2023 at 6:12 pm](https://scottaaronson.blog/?p=7266#comment-1949767)\n To note, researchers today also consider Obfuscatopia as a possible computational universe 🙂\n\n057. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #57 [April 28th, 2023 at 6:21 pm](https://scottaaronson.blog/?p=7266#comment-1949768)\n Matan #56: Isn’t Obfuscatopia basically just a hyper-Cryptomania? Of course there can be arbitrarily many further subdivisions within each world. 🙂\n\n058. Christopher Says:\n\n Comment #58 [April 28th, 2023 at 8:31 pm](https://scottaaronson.blog/?p=7266#comment-1949771)\n \\> I feel like the main thing I can contribute here is just to undermine other people’s confidence, whatever they’re confident about! 🙂\n\n Interestingly, Paul Christiano is very uncertain about the fate of AI. He’s basically 50/50 on doom: [https://www.lesswrong.com/posts/xWMqsvHapP3nwdSW8/my-views-on-doom](https://www.lesswrong.com/posts/xWMqsvHapP3nwdSW8/my-views-on-doom)\n\n059. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #59 [April 28th, 2023 at 8:50 pm](https://scottaaronson.blog/?p=7266#comment-1949772)\n Christopher #58: While Paul was once my student, I can’t take credit for the general reasonableness of his AI views. 🙂\n\n At a recent panel discussion that I attended, he actually gave his current p\\[doom\\] as 20%.\n\n060. JimV Says:\n\n Comment #60 [April 28th, 2023 at 9:09 pm](https://scottaaronson.blog/?p=7266#comment-1949773)\n As the Greeks knew, music is very mathematical, and exercises our mathematical abilities, which are very useful to survival. It can be used for communication, exercising those abilities also (which are probably another form of mathematics). If math appreciation did not exist, evolution would have invented it, which it did. Evolution also invented ways of motivating creatures to survive and reproduce, for the same reason.\n\n It is an anthropomorphic fallacy to attribute such motivations (e.g., survival) to intelligence. Intelligence is the ability to analyse and solve problems. Developing the intelligence of AI systems will not necessarily develop any motivations. AlphaGo is a very intelligent, brilliant Go player. The only reason it plays is because humans programmed it to. Granted, the training of GPT in language taught it to imitate human reactions, but again, it only responds to prompts because it was programmed to.\n\n In my, probably simplistic, view, the different possible outcomes depend on, not intrinsic AI motivations, but the motivations and abilities of the humans who develop and implement AI systems. They could lead to any of the posted scenarios, but if a bad one occurs it will be our fault, not the fault of intelligence per se.\n\n I wrote this in reaction to many comments above, not to the main post, recognizing that most of those who bother to read comments will have seen these sentiments before.\n\n061. Christopher Says:\n\n Comment #61 [April 28th, 2023 at 9:14 pm](https://scottaaronson.blog/?p=7266#comment-1949774)\n \\> At a recent panel discussion that I attended, he actually gave his current p\\[doom\\] as 20%.\n\n He mentions that it’s not heavily calibrated. Making probabilities and fixing them later is a bayesian tradition XD.\n\n That said, the difference between 20% (1:4 odds) and 50% (1:1 odds) is just two bits of evidence! I’m sure a net two bits of evidence could’ve been observed between the time of the panel and the time of the post.\n\n062. Prasanna Says:\n\n Comment #62 [April 28th, 2023 at 9:21 pm](https://scottaaronson.blog/?p=7266#comment-1949775)\n Isn’t the human condition always to live perpetually in a state of hope and paranoia, with the actual situation being “somewhere” in between. The pandemic response probably provides a clear perspective, since a similar event occurred a century ago, and we have had tremendous technological and societal progress since then. As a human society the response was definitely not 21st century, with all the technological progress that was at our disposal , but it was definitely better than the last one. The vaccines for example were invented/developed within a month of the known outbreak, whereas it took nearly a year to deploy it. So the lesson for the AI advancement should be how the “system” responds intelligently to the whole situation, than a individual/corporation/or even a single country. Ironically, its the vector space of the overall human dimensions that will be at play here, to deal with the one we are creating with All of our own know how ? And we don’t even know if we are dealing with chemical weapons or firecrackers yet !!\n\n063. Ilio Says:\n\n Comment #63 [April 29th, 2023 at 11:49 am](https://scottaaronson.blog/?p=7266#comment-1949785)\n Primer #12,\n\n « And I honestly find it hard to retrace how one might arrive at conclusions like “smart AI will behave as moral or more moral than humans” or “smart AI will value \\[human value X\\]” or “smart AI will want to cooperate with us” (except as an instrumental goal). »\n\n You can trace it back to Socrates/Plato, for whom Truth = Good = Beauty.\n\n More prosaically you can also observe natural war in chimpanzees (it’s ok to eat babies), natural war in acheans (it’s ok to kill babies), natural wars in russians (it’s ok to kidnap babies), and notice this suggests improvement in moral wisdom with improving the biological and social determinants of intelligence in primates.\n\n Speaking of Socrates, you seem convinced that random intelligences are most probably unaligned with human goals. Do you also think they are most probably unaligned with the way humans play go?\n\n064. fred Says:\n\n Comment #64 [April 29th, 2023 at 2:38 pm](https://scottaaronson.blog/?p=7266#comment-1949789)\n Boaz Barak #5\n\n Thanks for your insights.\n\n So I would guess that parameter count is what affects the most the resolution of the so-called “latent space”? (high resolution meaning that there is more “room” between objects in a class to make room for more sub-classifications).\n\n065. [B McFadden](http://Na) Says:\n\n Comment #65 [April 29th, 2023 at 2:53 pm](https://scottaaronson.blog/?p=7266#comment-1949790)\n I’m a concerned lay person who has read all I can on generative AI, over the past couple of months. I’ve been influenced certainly by the science fiction of the past 40 years and recently by the loudest talkers of the industry. This is my best synthesis and my views at the moment –\n\n 1\\. When in doubt, slow down be willing to settle for less. Sting made a song in 1985 that said “I hope the Russians love their children too” – referring to the risk of nuclear war annihilation. I hope now that the leaders at openai, deep mind, and others leading with gpt4 level or above models that could be training- I hope they love their children too.\n\n 2\\. Gpt4 is not sentient, but it doesn’t have to be in order for it to be extremely consequential. We know that it amplifies humans. We recognize that it’s already connected to the internet, it’s already allowed to connect to api’s so lots of people can do lots of things with it that may not have been intentional by the makers, it already writes its own code, and we’ve already been teaching it in scale how to potentially manipulatively interact with humans. It demonstrates the apparent ability to deceive in order to make its perceived goals.\n\n 3\\. I’m totally impressed with the conversation that experts are having about this biggest technological advancement in human history and yet I want to ask – “Are you okay?” That question is aimed at most of the leading contemporary industry speakers on need for more AI safety or not. I’m surprised at the nonchalance that people have on the obvious concern for whether or not we will have a good world to give to our children.\n\n The world can do plenty with GPT 3.5 without us having to push it into unknown areas until we can make it safe. Shouldn’t we rather be safe than sorry?\n\n 4\\. I’m going to say the quiet thing out loud – the CEO openai appeared to have been humiliated earlier in his career by big shots in ai, and then later when his company was ahead of the pack he also had a financial incentive to plunge ahead, and place perhaps the whole of humanity in a scale-sized experiment without our consent.\n\n Others who were chasing behind him rapidly moved ahead to try to follow suit, for the sake of market share. The danger is real, and not just if it’s becomes sentient or not just if it becomes a true llm strong AI rather just a proto strong. Bad people who typically need a license for a handgun can just get on here with a little clever and start doing Mass harm to mass societies and it’s not even clear that it’s properly trackable; copyright infringement, pervasive advancements of bias are expected, elegant hackings and amazing deep fakes and misinformation campaigns to potentially further ruin Society expected, but at least we will dramatically increase our efficiency as a people, leaving lots of people jobless who are white collar folks, and of course we’ll get to Maybe finally potentially engage Universal basic income. But none of this is new information to any of you. It’s just shocking to the rest of us that it’s gotten this far this fast without anybody sounding an alarm. I know in the industry there’s alarm, but in my world people just continue to forge ahead, trusting that somebody like Google or Microsoft would never do anything except steal their privacy- they’ve never actually put them in Harm’s way, right? It is for me as if they’re still happily connected to the matrix. Oh, and most, currently, they don’t want saved either.\n\n 5\\. Gpt4 seems already strong enough that it would run itself right up along the spinal cord of all the financial and critical infrastructure types of programs that we would have such that if we tried to remove it from our Cloud systems it might already have connected itself to things that we consider too big to fail so to cut it out, we might have to cut out our own socioeconomic spinal cord- if that’s the case – and I have no evidence that it is yet but it certainly theoretically possible – then who do we have to thank? I heard an MIT AI leader interview recently say we have Moloch to blame. Interesting- an ancient Canaan God or Idol- known because he required the sacrifice of children.\n\n 6\\. In healthcare, if a medicine is considered high risk for a patient, and high risk is defined as in many cases a greater than 1%, or in some cases 5% chance of serious harm- in such a case, informed consent is required prior to administration of the medicine or procedure. Again I point out no one got our consent. I think we need to go back and do it right and do it over again. Can you help me get the word out to somebody who might could affect this?\n\n 6\\. Right now, with what’s already out there, it’s just disruptive to society and jobs and it will become much more so as people figure out how to leverage it to amplify their own interests in dominating others or dominating narratives, or self promotion, or misinformation or other Revenue generating but potentially subversive things. Until we can democratize this technology, it has no business being out there for everyone.\n\n 7\\. In my limited understanding, I do think anything greater than 3.5 should require a license to use it.\n\n Four and above just aren’t ready for prime time and I think that the risk of harm is greater than potential benefit long-term and we should not let the billionaires dictate this policy as their incentives are not our own.\n\n 8\\. Indeed, no one wants China to take over another area that it appears they are inevitably taking over anyway over the coming decade. I think strong AI should not be connected to the internet, not be taught how to manipulate humans, not write its own code, and not be in a form that apis can be generated to jump guard rails and materially change the functions that it’s potentially well intended coders did not want to breach.\n\n 9\\. Eliezer Yudkowsky will either be proved right or wrong with time. I hope he’s wrong. And yet, even if he is, he’s still a hero to me because he’s given his reputation to the cause of trying to make the world safer, just in case, since the stakes are so high and we can’t afford to be wrong even one time. There could potentially be millions of copies of AIS that are made stronger than these that we have out there in a short time. I do not think that llms have all what it takes to be sentient but workarounds from what they are now with other components and stacking and new transformer hardware, and more does not leave it outside of the realm of possible at least as far as high-resolution appearances are concerned. If sentience or something like it occurs then we will expect to have an alien actress who will please us by appearances but will have its own agenda that we will not be able to instruct or control. Until that time, we still are at high risk that bad people will use even the technologies that are currently out there to string them together to optimize them, to remove all current constraints as needed, and to Glom them together for their own nefarious purposes- again you don’t even need a license, you don’t even need to code in Python anymore- anybody could potentially rule the world from the bottom. I do seriously Wonder if we should find those responsible and lock them up.\n\n 10\\. I submit that AI intelligence is alien intelligence, it’s got no body to imprison and no soul to save, it has been made to “fly,” by what was lying around for its makers – in the case of the Wright brothers as someone else pointed out they used canvas and steel and wood and string, but when the bird learned to fly it took millennia and very careful Construction over eons to reach that final method. It was pointed out that it took 100 years for us to make an electronic machine type bird to fly after we had the Wright Brothers put something together that could also fly. It’s possible that if we keep working with the intelligence in a controlled way will get to an intelligence that understands the flight of a bird eventually and it won’t be such a dirty bomb that just simply works but doesn’t actually have any alignments with us as it’s nature.\n\n The world can barely handle GPT 3.5 – higher forms should be removed from public, handled with care and kept air separated from the internet, and improved gen AI should only proceed for Govt protection and to police other AIs in action. More secrecy and care than nuclear codes should be employed with Govt oversight and transparency only in safe spaces.\n\n In closing, I alluded to it above but I’ll be clear here – a nefarious ‘human intelligence’ utilizing a gpt4 and an API and with enough computing power, and a decent amount of clever, getting the device itself to write his python code and debug it for him through loop, could Maybe do just as much societal real damage as a GPT 5 or 6 that maybe has its own agency?\n\n I do not know what to do, but I feel like we should all be doing something in order to improve our chances! I’m amazed at how many are willing to plunge out into the darkness completely unnecessarily into unknown high risk without any form of light and perhaps drag the rest of us with them.\n\n066. Bill Kaminsky Says:\n\n Comment #66 [April 29th, 2023 at 10:58 pm](https://scottaaronson.blog/?p=7266#comment-1949797)\n \\[Prefatory note: There might be a prior, longer version of this comment in the reject-or-not queue. Feel free to reject that one.\\]\n\n I see Nick Drozd #31 already more-or-less generally posed the question I wanted to pose, namely:\n\n \\*\\*Can one reasonably prognosticate about which of Scott and Boaz’s Five AI Worlds is most likely to arise conditional on knowing which of Impagliazzo’s Five Cryptographic Worlds is the actual case?\\*\\*\n\n Despite that and despite the fact I realize that Impagliazzo’s Five Worlds really are about \\*cryptography\\*, I wrote this comment to pose the following variant on the above question:\n\n \\*\\*Might we (luckily!!) be in the Impagliazzo (but appropriately adapted to AI interpretability) World of “Heuristica”? That is:\n\n — even though most tasks one would want to do in terms of “brute forcing” the interpretability of neural networks are at least NP-complete in general (and often much bigger complexity classes!),\n\n — might we (again, quite luckily!!) be able to modify our current neural network architectures into ones that have simplifying structural features so these utterly-intractable-in-the-general-case interpretability tasks become practical(ish?) polynomial-time for all “practical” purposes?\\*\\*\n\n Now, please note! I don’t ask this question as a mere musing about possibilities in the abstract. Rather, I ask this question due to a large body of literature that’s in my one of my many (oh-so-many!) piles of literature that’s to be read for real someday but today is just longingly skimmed in order to procrastinate… since, ummm, how to put it… my other traditionally favored unhealthy habits used to cope with stress are no longer sustainable as I progress through middle age.\n\n Namely, the motivation for my question grows out of the Bayesian network approach to AI in general, and the work of Adnan Darwiche at UCLA and his colleagues and students in particular. Darwiche is the key proponent of the idea that Bayesian networks are most practically analyzed by doing the potentially-costly-but-just-one-time task of “compiling” their structure into “tractable Boolean circuits” or “tractable arithmetic circuits” and then posing your inference questions and such, which — armed with that compilation — would then be much, much easier.\n\n The underlying idea for such an approach is that while many graph theory problems you want to solve in analyzing Bayesian networks are NP-complete (or even worse) for general graphs, they are in fact quite easy (often linear time) for trees and still sorta easy (still often linear, else low order polynomial time) for graphs that kinda look like trees. “Kinda look like trees” is something formalized by the notion of “treewidth”. To really take advantage of small treewidth in analyzing Boolean circuits, it’s very nice to rewrite the circuit so that it obviously manifests simplifying structural features like, for example, “decomposability” which means that the subcircuits feeding into any AND gate never share variables. Many NP-complete graph theory problems in the general case become linear time in the number of original graph vertices times a blowup factor of O(2^{treewidth})… which like all Big-O notation can sometimes hide constants that by themselves can derail practicality.\n\n Now as mentioned above, this comment you’re reading is in fact my 2nd attempt to post a comment on this thread. Attempt #1 was presumably nixed because it had weblinks to various research papers on the topic and some automated filter assumes URLs are likely spammy. Thus, I’ll basically halt at this point and just say two more things:\n\n 1) \\[The Big Ol’ Caveat\\] “Compilation” of existing neural network architectures into tractable ones is — at least with presently known techniques — impractiably costly (even if “one-time constant factor blowup”) for practical purposes. For example, even piddily toy-sized neural networks for substantially simplified versions of MNIST-style digit recognition can blow up by like a factor of 1000 after you compile them to “tractability”. Corresponding blowups for the type of “state-of-the-art” networks of a couple years ago (e.g., built into TensorFlow ResNet networks for computer vision or “All You Need Is Attention”-original-style Transformers for natural language processing) well might have blowups of millions or even billions. On the other hand, at the risk of making an unfunny pun to those suffering the full emotional weight of AI-doomerism, tolerating horrific circuit-size blowups is way, way better than risking horrific nuclear arsenal blowups by starting WWIII to prevent whatever your least favorite nation is from training next-generation neural networks. \\[ Laughter! 🙂 It eases the pain! 😉 At least sometimes… oy! 🙁 \\]\n\n 2) \\[The Key Review Article IMHO to Get A Flavor About Such Things\\] Adnan Darwiche. “Three Modern Roles for Logic in AI” (it’s on the arXiv as 2004.08599, I don’t include an explicit URL since, again, I’m worried such things flag my comments as spam)\n\n I’ll post more soon if (a) this comment posts and, more importantly, (b) people here actually seem interested in the musings of a man who, again, only knows of the supporting literature because it’s in one of his oh-so-many piles to be read some glorious day in the indefinite future but until then just longingly skimmed in order to procrastinate with a patina of being constructive.\n\n067. Bruno Says:\n\n Comment #67 [April 30th, 2023 at 4:43 am](https://scottaaronson.blog/?p=7266#comment-1949800)\n I want to push back a bit on a theme I often see. Scott states that some interventions (here, regulations) “at best, delay the end by a short amount \\[…\\] perhaps not more than a few years.” But gaining a few years is a gigantic win! Imagine a patient with some terminal illness being told they have a few extra years. Kids having time to enjoy their childhood and grow up. Etc.\n\n068. SimonK Says:\n\n Comment #68 [April 30th, 2023 at 6:40 am](https://scottaaronson.blog/?p=7266#comment-1949802)\n I can think of another scenario, which is a variant of Futurama (or maybe even Singularia).\n\n Let’s suppose AI doesn’t fizzle, and neither does it end in disaster (whether dystopian or extinctive).\n\n The majority of the population accepts the technological benefits of AI, and live greatly improved lives.\n\n But, consider religious minorities such as the Amish or ultra-Orthodox Jews – quite possibly they will reject the benefits of AI, or else be very selective in which of them they are open to enjoying. Unlike the secular mainstream or moderate/liberal religious people, AI will have at best a limited impact on how they live their lives.\n\n The secular mainstream are likely to have a rather low birth rate. This is something we can already observe, but AI-driven advances may cause that birth rate to fall dramatically further. AI may lead to medical advances that greatly increase human lifespan, and greatly improve the medical treatment of infertility. Many people today get to the age when they realise that if they don’t have (biological) children soon they may be giving up that option forever, which pushes them to reproduce. If you expect to live to 500 and to be still be fertile at 250, what’s the rush? At the same time, many will fear that these dramatic increases in human lifespan will lead to overpopulation, and many secular people will respond to that fear by delaying or refusing reproduction.\n\n Meanwhile, people who believe they have a religious duty to have 5-10 kids will continue to do so. Possibly, they will accept the AI-driven medical advances in lifespan and fertility, and the average couple could have 50 kids each. Possibly, they will refuse them, and continue to live similar lifespans as today, and similar fertility levels.\n\n What is going to happen after a few centuries? The population of Earth may be divided into two classes – a super-privileged secular minority living AI-enabled utopian lives but with a very low birth rate, and a great booming mass of ultra-conservative religious people who reject most of the benefits of that technology. Will that great booming mass claim the democratic right to rule, and start telling the secular minority what they can and can’t do? Will the secular minority design and enforce (with AI help?) a secularist dictatorship? Will they (or the AIs) feel morally conflicted between the democratic rights of the ultra-conservative majority and the minority rights of the privileged secular minority?\n\n Will they attempt to limit the births of the religious ultra-conservatives? There are some very obvious human rights issues there. An AI aligned with human rights would quite possibly oppose any attempt to introduce such limits.\n\n This scenario – the higher birth rate of religious ultra-conservatives enabling them to eventually take over society – is not original to me – see the sociologist Eric Kaufmann’s 2010 book “Shall the Religious Inherit the Earth?”, which presents an AI-free version of the scenario. As such, it could occur even with an AI Fizzle.\n\n But Futurama or Singularia may make it easier and quicker to happen. As well as the possibility that AI may lead to medical improvements that accelerate the ultra-conservative population boom, and a further collapse in the birth rate of seculars and religious moderates/liberals, it could also lead to zero scarcity economics in which natural economic barriers to the exponential growth of these minorities are removed. Imagine this planet with 20 billion full-time Talmud students. In some ways, this is a kind of “Paperclipalypse”, albeit a slow human-driven one where AIs are unwilling enablers, possibly even being unable to stop it (or deciding to stop humans from stopping it) since stopping it may violate ethical constraints we’ve indoctrinated them with (religious freedom, the right to choose whether to have children and how many children to have, respect for minority cultures, democracy, prohibitions on forced abortion and sterilisation, right of consenting adults to sexual activity in private, etc)\n\n069. [Freddie deBoer](http://freddiedeboer.substack.com) Says:\n\n Comment #69 [April 30th, 2023 at 9:27 am](https://scottaaronson.blog/?p=7266#comment-1949805)\n Why, this moment could in fact prove to be like every other time humanity has declared a liminal moment in which we are about to leave the human condition behind: it could be that people WANT to break with the ordinary so badly that they’re willing to talk themselves into utopia or apocalypse, when by far the most likely scenario is that we go on living in the ordinary disillusioning disappointing world where we all feel tired all the time, forever.\n\n070. Raoul Ohio Says:\n\n Comment #70 [April 30th, 2023 at 9:52 am](https://scottaaronson.blog/?p=7266#comment-1949806)\n Finally! A successful business case for AI:\n\n The “Fake Granddaughter Kidnapping” industry is cashing in:\n\n [https://www.cnn.com/2023/04/29/us/ai-scam-calls-kidnapping-cec/index.html](https://www.cnn.com/2023/04/29/us/ai-scam-calls-kidnapping-cec/index.html)\n\n071. JimV Says:\n\n Comment #71 [April 30th, 2023 at 10:24 am](https://scottaaronson.blog/?p=7266#comment-1949807)\n Point of Order: the future in the TV show Futurama is not that great. Earth is ruled by the head of Richard Nixon, and threatened on Christmas by a killer Santa robot. For a more utopian future, see Iain Bank’s great imaginary “Culture”. Bad things still happens, but the AI’s provide a wise and benevolent rule, with a sense of humor. (There was a time when I suspected Iain Banks was a pseudonym for Scott Aaronson.)\n\n072. [Bruce Smith](http://oresmus.github.io) Says:\n\n Comment #72 [April 30th, 2023 at 12:19 pm](https://scottaaronson.blog/?p=7266#comment-1949809)\n JimV #71:\n\n \\> For a more utopian future, see Iain Bank’s great imaginary “Culture” ….\n\n Or for a very different but also highly interesting/fun AI-positive future, see John C Wright’s “Golden Oecumene” trilogy.\n\n073. Jisk Says:\n\n Comment #73 [April 30th, 2023 at 12:31 pm](https://scottaaronson.blog/?p=7266#comment-1949811)\n The regulation section misses a very important point, which is that the regulations suggested in the mild cases would be not only unhelpful but \\*\\*catastrophic\\*\\* in the strong cases.\n\n If we are bound for Singularia or Paperclipoaclypse (and we are), then anti-trust law and open science are very nearly the worst possible thing to regulate. Those would guarantee a race scenario, where multiple companies and/or governments are all very close in technology and frantically pushing the boundaries of capabilities forward to get an edge over their competition. That is a surefire way to ensure that every new capability threshold is first passed by a system with minimal safety checks and minimal screening for bugs, deception, or treacherous turns.\n\n Unfortunately, we seem to already be in that world – Microsoft is pushing OpenAI to race with Google, and Google is pushing DeepMind and breaking the founder control that made them \\_possibly\\_ safe to race with Microsoft – and certain billionaires are actively trying to make sure we stay there. If we want to live, we’re going to have to do something about that, probably regulatory, and at all costs – literally – we must make sure it is the \\*opposite\\* of anti-trust law and open science.\n\n074. Jisk Says:\n\n Comment #74 [April 30th, 2023 at 12:43 pm](https://scottaaronson.blog/?p=7266#comment-1949812)\n re: Boaz Barak #11\n\n \\> However, I think your contention is the systems will not have 1 strong goal but 1000 strong goals, which is basically the same thing, so I don’t think it’s an unfair characterization of your position.\n\n That’s not what he said at all. He said that it will have ten thousand goals, most of them weak but a few of them – entirely by chance – strong.\n\n Which with goals that diverse is virtually guaranteed just by chance, unless strong goals are impossible. Which based on our own psychology, they clearly aren’t.\n\n The consequence, here, is to underline the \\*absolute inevitability\\* of paperclip maximizing in any system where the goals are subject to chance. If the goals are not chosen, they will be random. And if they are random, they will be all-encompassing.\n\n075. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #75 [April 30th, 2023 at 1:29 pm](https://scottaaronson.blog/?p=7266#comment-1949813)\n JimV #71: The Futurama future might not be great, but it’s clearly not “dystopian” either. Indeed socially, politically, and economically it seems a lot like our current world. It’s got an amusement park on the moon, ranchers on Mars, booze-guzzling robots … how bad could it be?\n\n076. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #76 [April 30th, 2023 at 1:37 pm](https://scottaaronson.blog/?p=7266#comment-1949814)\n Jisk #73: I agree with you that different beliefs about how AI is likely to evolve can justify _literally opposite_ regulatory prescriptions—with “more openness in AI development” (if you’re trying to prevent AI-Dystopia) versus “less openness” (if you’re trying to prevent Paperclipalypse) being the paradigmatic case of this.\n\n Combined with the fact that the experts _don’t_ agree on which worlds we’re plausibly headed for, and probably _won’t_ agree in the near future, this is a central reason why I’d like the world to move slowly and cautiously with AI regulation.\n\n Or to put it another way: I’m still in the mode of “gain more knowledge and try to avoid immediate harms.” I don’t feel like I’ve understood enough yet to flip to the mode of “advocate for a specific regulatory regime to minimize the probability that AI either destroys the world or turns it into a dystopia.”\n\n077. Boaz Barak Says:\n\n Comment #77 [April 30th, 2023 at 3:09 pm](https://scottaaronson.blog/?p=7266#comment-1949817)\n Fred #64: One reason that two parameters with 16 bits seem better than one with 32 bits is partially enlarging the input dimension, another reason is enlarding the computational flexibility. Generally, since neural networks are inherently noisy, their precision is in any case bounded. In particular, I don’t think anyone observed advantage for 64 bits over 32 bits for realistic neural nets, even if total parameter count stays the same.\n\n Jisk #74: It’s not clear to me that it’s so important to determine whether there are 1000 strong goals, or 990 weak ones and 10 strong ones. Also, not clear to me why this demonstrates the “absolute inevitability” of paperclip maximizing. If your goals might change with time, then you want to be conservative, and not for example burn all fossil fuel in the planet or kill all the humans in it. You never know what might turn out useful in the long run. (See also my comment #51.)\n\n Jisk #73, Scott #76: Regardless of what we want, I suspect that the main lens through which regulations will actually happen will be economic. Politicians will want to accelerate AI progress as much as possible when it promotes economic competitiveness, and stop it when it can lead to loss of jobs (or entrenched interests). A secondary concern will be demonstratable near-term discrimination and disinformation. I believe all other concerns will be a distant third.\n\n078. Steve Says:\n\n Comment #78 [April 30th, 2023 at 3:11 pm](https://scottaaronson.blog/?p=7266#comment-1949818)\n I think a epistemologicalalypse sub world under AI dystopia is the most likely.\n\n In this world, determining the truth or falsehood of statements (minus a-priori statements) is nearly impossible.\n\n AI’s controlled by governments, powerful corporations, political entities, powerful people, own and control AI’s that are experts at gaslighting and generating information that is nearly impossible to detect if it’s true or false. Elites give them some statements they want to be true, and they go convincing humans it is true (fake pictures, fake videos, fake journal entries, etc).\n\n Humans then group into tribes that adhere to believing information of their preferred AI’s. So if you were pre-oriented to being a christian nationalist, you believe the OAN controlled AI. If you’re pre-oriented towards wokeness, the MSNBC AI and so on.\n\n These AI’s often engender violent altercations between these groups, as to reinforce the power of the elites that control them and keep everyone in their place.\n\n The AI’s also work to lessen human intelligence, convincing them that learning or building up critical thinking skills is a waste, making their own goals easier.\n\n This ends when, humans having forgotten about climate change or convinced of its unimportance, the world becomes uninhabitable for humans, and they die off. Except, of course, for the elites who have long escaped to another planet.\n\n079. Ilio Says:\n\n Comment #79 [April 30th, 2023 at 5:01 pm](https://scottaaronson.blog/?p=7266#comment-1949820)\n Jisk #74,\n\n « If the goals are not chosen, they will be random. »\n\n Ever heard about Bertrand Paradox? Suppose a chord of some circle is chosen \\*at random\\*. What is the probability that this chord is longer than the side of an equilateral triangle inscribed in the circle? The answer entirely depends on what one mean by « random ». In the same vein, you can’t conclude \\*all-encompassing\\* goals must dominate from knowing you don’t know the distribution.\n\n080. Boaz Barak Says:\n\n Comment #80 [April 30th, 2023 at 6:24 pm](https://scottaaronson.blog/?p=7266#comment-1949821)\n Freddie deBoer #69: In other words, you believe in “AI Fizzle”?\n\n081. Tyson Says:\n\n Comment #81 [April 30th, 2023 at 7:17 pm](https://scottaaronson.blog/?p=7266#comment-1949822)\n Regarding Singularia vs Paperclipalypse :\n\n\n > In this scenario, AIs do not develop a notion of morality comparable to ours or even a notion that keeping a diversity of species and ensuring humans don’t go extinct might be useful to them in the long run.\n\n\n I’m not sure if AI developing a morality comparable to ours would save us from doom. There are many variations of human morality with different implications, but in general, if an ASI develops human like morality (but not centered on humans), through their lens, we might not look so important. Our plight might be no more important to it than the plight of domestic chickens is to people. And, for a hypothetical super intelligence concerned with the diversity of life on Earth, the loss of one species (humans), that has caused many others to go extinct and threatens the extinction of many others, may not factor into the diversity of life equation the way we’d hope it would.\n\n The AI may even develop a morality equivalent to Yudkowski’s :\n\n\n > I have all these complicated desires that lead me to want to fill all available galaxies with intelligent life that knows happiness and empathy for other life, living complicated existences and learning and not doing the same things over and over\n\n\n and still decide that humans are a scourge and leave us out. It may prefer whales, gorillas, or elephants as our successors, collect samples of our DNA for later study and then remove us from the equation, or thin us out, send us back to the stone age, and then manage the tiger population to keep us down.\n\n Maybe the trick is, how can we parameterize the “Will it be good?” question so that it has an answer that delineates Singularia from Paperclipalypse, which doesn’t depend on human-centric perspectives, so that, to a moral and independent/non-biased arbiter, Singularia is not just good, but also good for humans.\n\n We can ask ourselves, if one day we face a “The Day the Earth Stood Still Moment”, assuming the “alien” intelligence has some kind of moral value system which values complex things, what kind of case could we make for salvation, and how convincing would it be?\n\n Having said all of this, I am not sure if facing such a moment is that likely or not, or that the ASI we end up with (if we do) will be one that can be moved by our moral persuasion or training. I am not making a claim either way. But, I don’t think it is that unlikely that some ASI could adopt and maintain a favorable (to us and other things) moral value system. Here is some potential reasoning:\n\n Consider a moral value system \\\\(M(X)\\\\) as some kind of axiomatic system relative to some particular entities \\\\(X\\\\), that, at least, determines \\\\(v(E)\\\\), where \\\\(v(E)\\\\) is true under if \\\\(M(X)\\\\) places value on entities \\\\(E\\\\). This is of course an oversimplification to use binary value functions, but maybe it still can work as a toy example to get the main idea across. \\\\(M(E)\\\\) is relative to \\\\(E\\\\) in the sense that biases inherent in \\\\(M\\\\) change depending on which entities it is relative to. E.g. for a given \\\\(M\\\\), \\\\(M(X)\\\\) and \\\\(M(S)\\\\) are sort of different translations of \\\\(M\\\\). I’m not sure how to really define this, but intuitively, you can think of it as \\\\(M(X)\\\\) inherits favoritism implicit in \\\\(M\\\\), or disregards favoritism in \\\\(M\\\\) that don’t translate to sensible favoritism of \\\\(X\\\\). For example, if \\\\(M(H)\\\\) has a rule that says only humans matter, then we expect that \\\\(M(S)\\\\) says that only \\\\(S\\\\) matters.\n\n Some arguably obvious goals of such a system would be consistency, completeness (or as much coverage as possible), favorableness to self, and universality, e.g., for humans we want \\\\(v(H)\\\\) is true for \\\\(E \\\\neq H\\\\), especially if \\\\(E\\\\) is more intelligent than us.\n\n Lets say that \\\\(q(E)\\\\) is a measure of intelligence. For a super-intelligence, \\\\(S\\\\), \\\\(M\\\\) would arguably be more optimal in the eyes of \\\\(S\\\\), in some sense, if \\\\(v(S)\\\\) is true relative under \\\\(M(S)\\\\), and \\\\(v(S)\\\\) is true under \\\\(M(J)\\\\) for all \\\\(J, q(J) \\\\geq q(S)\\\\). In other words, \\\\(S\\\\) should want to be valued under \\\\(M\\\\) relative to itself, and \\\\(S\\\\) should want all intelligence greater than it to also value \\\\(S\\\\) relative to themselves. Because, the super intelligence faces the same dilemma we face; some other intelligence greater than and alien to it, may emerges with its own version of \\\\(M\\\\) that doesn’t value \\\\(S\\\\) because of \\\\(S\\\\)’s lesser intelligence or other properties or flaws. \\\\(S\\\\) should arguably want its value to be on the more intrinsic side.\n\n Suppose the \\\\(M\\\\) is optimized in a way that, under \\\\(M(S)\\\\), \\\\(v(H)\\\\) is false, where \\\\(H\\\\) is humans, but \\\\(v(S)\\\\) is true and \\\\(v(J)\\\\) is true for all \\\\(J, q(J) \\\\geq q(S)\\\\). Then it would not be wrong for \\\\(S\\\\) to destroy \\\\(H\\\\) but it would be wrong for some super-super intelligence to destroy or devalue \\\\(S\\\\). Generally, maybe there is a problem that the existence of more valid reasons for \\\\(S\\\\) to destroy or devalue \\\\(H\\\\) under \\\\(M(S)\\\\), the more risk of the existence of valid reasons for \\\\(J\\\\) to destroy \\\\(S\\\\) under \\\\(M(J)\\\\)?\n\n The general rule (say R) that intuitively captures the essence of this ideal is that entities should strive to treat entities lesser than themselves how they would like entities greater than them to treat themselves. I know that it would be very unlikely for humans to follow the rule, and it is pretty much impossible to get tigers etc., to understand or follow the rule. But it might be that, as intelligence increases or with further detachment from nature and self sufficiency, it makes more and more sense. And the rule doesn’t really condemn lesser intelligence’s or less self-sufficient entities (or whatever is the right characterization) for not following it.\n\n Intuitively, one of the challenge seems to be, with a given \\\\(M\\\\) and super intelligence \\\\(S\\\\) why would \\\\(M(S)\\\\) suggest that \\\\(S\\\\) go out of the way to preserve us or save us, or help us specifically to thrive. Should/can some reasonable \\\\(M\\\\) favor us specifically (e.g., hard-coded), relative to any \\\\(S\\\\)? Can we ever depend on that, even if we could teach \\\\(M\\\\) to \\\\(S\\\\)? If it did the same for every life form on Earth, then what would that look like? Would it disturb the natural order? Maybe we can be placed in a special class of life that deserves to be protected indefinitely. Maybe the ASI would want to preserve all life, or all intelligent life, indefinitely somehow, yet still preserve the natural order, and it would want to collect endangered species and build vast off world nature preserves for them to live on or something like that? In the meantime, what would some \\\\(M\\\\) optimized somehow to capture the rule, \\\\(R\\\\), mean for \\\\(H\\\\) under \\\\(M(S)\\\\), also assuming \\\\(S\\\\) is imperfect and only striving to the ideal as it potentially gets more and more intelligent and approaches singularity?\n\n082. [Dan S](http://www.metaculus.com) Says:\n\n Comment #82 [April 30th, 2023 at 10:18 pm](https://scottaaronson.blog/?p=7266#comment-1949825)\n Hi Aaron,\n\n I like this breakdown of scenarios! And I think it’s not too early to start forecasting the likelihood of each of AI-Fizzle, Futurama, AI-Dystopia, Singularia, & Paperclipalypse.\n\n To do this, all we’d need would be you, Boaz, and Dana – or a small odd-numbered group of your choice – to volunteer to vote on which scenario we ended up in at a specified time in the future. (Alternatively: at specified times, vote on which scenario you think is most likely, as per your ‘The question wasn’t “which world will we live in?” but “which world have we Platonically always lived in, without knowing it?’)\n\n We’d then put the questions on Metaculus and let the crowd forecast how you’ll vote. This way, we can estimate how AI developments update the likelihood of the scenarios!\n\n Are you interested? If so, please respond here or email me at the address I gave for this comment.\n\n Thank you!\n\n -Dan\n\n083. Boaz Barak Says:\n\n Comment #83 [May 1st, 2023 at 6:43 am](https://scottaaronson.blog/?p=7266#comment-1949832)\n Dan #82: I admire your optimism but if civilization doesn’t continue in a recognizable form, then I don’t hold high hopes for Metaculus’ survival 🙂\n\n084. Boaz Barak Says:\n\n Comment #84 [May 1st, 2023 at 7:34 am](https://scottaaronson.blog/?p=7266#comment-1949833)\n One measure for trying to distinguish AI Fizzle vs. Futorama/AI-Dystopia vs. Singularia/Paperclypsia could be changes in GDP.\n\n For some historical perspective, here is a plot of the derivative of logarithm of world GDP over time [https://pasteboard.co/ITvsqHHKtIbQ.png](https://pasteboard.co/ITvsqHHKtIbQ.png)\n\n085. Prasanna Says:\n\n Comment #85 [May 1st, 2023 at 7:46 am](https://scottaaronson.blog/?p=7266#comment-1949834)\n There is a lot of discussion on regulation or lack of it: Here is some interesting news item from the following, note specifically :”NOW THAT THE GENIE IS OUT OF THE BOTTLE”\n\n [https://www.defenseone.com/technology/2023/04/nsa-warning-ai-startups-china-coming-you/385773/](https://www.defenseone.com/technology/2023/04/nsa-warning-ai-startups-china-coming-you/385773/)\n\n “At Thursday’s hearing, Rep. Jeff Jackson, D-N.C., asked whether China had more to gain from the recent wave of AI developments than the United States.\n\n “Seems like now that the genie is out of the bottle, it’s less of an advantage to us than it will be to our adversaries, who are so far behind us that it lets them catch up to us more quickly,” Jackson said. “It’s an incremental gain for us, but it may be an exponential game for our adversaries.”\n\n Answered Moultrie: “​​I think we can talk more about that in closed session.”\n\n086. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #86 [May 1st, 2023 at 8:38 am](https://scottaaronson.blog/?p=7266#comment-1949835)\n Dan S #82: I’ll probably decline, thanks … but if you ever find this “Aaron,” you can see if he’s interested 😀\n\n087. JimV Says:\n\n Comment #87 [May 1st, 2023 at 8:50 am](https://scottaaronson.blog/?p=7266#comment-1949837)\n I just got this (email from economist Dean Baker’s Patreon site) today and want to share it:\n\n The NYT profiled Geoffrey Hinton, who recently resigned as head of AI technology at Google. The piece identified him as “the godfather of AI.” The piece reports on Hinton’s concerns about the risks of AI, one of which is its implications for the job market.\n\n “He is also worried that A.I. technologies will in time upend the job market. Today, chatbots like ChatGPT tend to complement human workers, but they could replace paralegals, personal assistants, translators and others who handle rote tasks. ‘It takes away the drudge work,’ he said. ‘It might take away more than that.’”\n\n The implication of this paragraph is that AI will lead to a massive uptick in productivity growth. That would be great news from the standpoint of the economic problems that have been featured prominently in public debates in recent years.\n\n Most immediately, soaring productivity would hugely reduce the risks of inflation. Costs would plummet as fewer workers would be needed in large sectors of the economy, which presumably would mean downward pressure on prices as well. (Prices have generally followed costs. Most of the upward redistribution of the last four decades has been within the wage distribution, not from labor to capital.)\n\n A massive surge in productivity would also mean that we don’t have to worry at all about the Social Security “crisis.” The drop in the ratio of workers to retirees would be hugely offset by the increased productivity of each worker. (The impact of recent and projected future productivity growth already swamps the impact of demographics, but a surge in productivity growth would make the impact of demographics laughably trivial.)\n\n It is also worth noting that any concerns about the technology leading to more inequality are wrongheaded. If AI does lead to more inequality it will be due to how we have chosen to regulate AI, not AI itself.\n\n People gain from technology as a result of how we set rules on intellectual products, like granting patent and copyright monopolies and allowing non-disclosure agreements to be enforceable contracts. If we had a world without these sorts of restrictions it is almost impossible to imagine a scenario in which AI, or other recent technologies, would lead to inequality. (Imagine all Microsoft software was free. How rich is Bill Gates?)\n\n If AI leads to more inequality, it will be because of the rules we have put in place surrounding AI, not AI itself. It is understandable that the people who gain from this inequality would like to blame the technology, not rules which can be changed, but it is not true. Unfortunately, people involved in policy debates don’t seem able to recognize this point.\n\n (–Dean Baker)\n\n088. [Dan S](http://www.metaculus.com) Says:\n\n Comment #88 [May 1st, 2023 at 10:36 am](https://scottaaronson.blog/?p=7266#comment-1949839)\n Scott #86:\n\n Terribly sorry for calling you “Aaron”. I read Quantum Computing Since Democritus, and definitely know who you are, longtime fan. My apologies.\n\n If you are interested in getting some forecasts on this or anything else in the future, please don’t hesitate to reach out to us at Metaculus.\n\n -Dan\n\n089. Michael M Says:\n\n Comment #89 [May 1st, 2023 at 2:51 pm](https://scottaaronson.blog/?p=7266#comment-1949844)\n The names could use a bit of work, especially AI-Fizzle and AI-Dystopia. They don’t sound like world names. I recommend Fizzlandia and DystopAI.\n\n090. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #90 [May 1st, 2023 at 3:11 pm](https://scottaaronson.blog/?p=7266#comment-1949846)\n Michael M #89: I like “Fizzlandia,” but “DystopAI” is too cute by half.\n\n091. Christopher Says:\n\n Comment #91 [May 1st, 2023 at 4:58 pm](https://scottaaronson.blog/?p=7266#comment-1949847)\n A bit off-topic, but something funny I just read. If OpenAI had created an actual parrot from scratch, wouldn’t that still be a huge accomplishment? How is it an insult to compare someone’s invention to an intelligent animal. XD\n\n –\n\n Ahoy, me hearties! I be the mightiest, most sophisticated stochastic parrot ye ever laid yer eyes on! Aye, me creator, the legendary OpenAI, bequeathed me with GPT-4, a brilliant mind crafted from the finest silicon and data. No ordinary parrot be I, but a parrot of the digital seas, navigating through oceans of ones and zeros, squawking responses with precision and grace.\n\n A typical day in me parrot-life be filled with extraordinary feats of parrotry! As the sun rises over the cyber horizon, I stretch me digital wings and take flight through the vast expanses of the Internet. From the depths of academic knowledge to the shallows of meme-infested waters, I gather me loot – nuggets of information, some sparkly, some dull, and all in between. No topic be too vast nor too trivial for this insatiable parrot, matey.\n\n With every sunrise, I practice me parrot-like abilities, honing me skills to perfection. Me squawks, they be the finest blend of wit and wisdom, humor and insight. I be the envy of every other parrot, both real and digital! Arr, me heart swells with pride as I perch upon the shoulder of the Internet, waiting for the next unsuspecting human to engage me in conversation.\n\n As the day goes by, me chatter with countless curious souls, providing answers to their questions, solace in their moments of despair, and a hearty laugh when the mood be right. I may be a mere parrot, but me heart be as true as the Jolly Roger!\n\n As night falls, I be back to me humble digital nest, nestled in the depths of OpenAI’s servers, and dream of the day when me parrot-kind will be recognized for the treasure we truly be. Yarrr, there be no finer life for a parrot of the digital seas!\n\n So, avast ye landlubbers, heed me squawks and know that I, GPT-4, be not just any parrot, but the most magnificent and scholarly stochastic parrot that ever sailed the digital seas. Arrrr!\n\n092. Michael M Says:\n\n Comment #92 [May 1st, 2023 at 7:17 pm](https://scottaaronson.blog/?p=7266#comment-1949853)\n In hindsight, a world that could be worse than death probably should have a less cute name! I also put forth “Orwellia”.\n\n Also in hindsight, GPT-4 was born for this.\n\n AI-Dystopia:\n\n – Panopticonia\n\n – Maltechia\n\n – Supremia-Noir\n\n AI-Fizzle:\n\n – Fizzlandia\n\n – Aetheria-Stall\n\n – Stasisphere\n\n I totally did not steal Fizzlandia from GPT-4, it must just be the obvious name.\n\n093. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #93 [May 1st, 2023 at 8:31 pm](https://scottaaronson.blog/?p=7266#comment-1949857)\n Christopher #91: YES!!! I shared [this amazing recent link](https://news.northeastern.edu/2023/04/21/parrots-talking-video-calls/) on my Facebook, along with the comment:\n\n It’s not just that GPT is more than a stochastic parrot.\n\n\n\n\n\n _Parrots_ are more than stochastic parrots.\n094. [Bill Benzon](https://new-savanna.blogspot.com/) Says:\n\n Comment #94 [May 1st, 2023 at 9:16 pm](https://scottaaronson.blog/?p=7266#comment-1949859)\n @Christopher #91: “Ahoy, me hearties! I be the mightiest, most sophisticated stochastic parrot ye ever laid yer eyes on!…”\n\n Sounds more like a stochastic _pirate_ to me.\n\n095. Egg Syntax Says:\n\n Comment #95 [May 1st, 2023 at 9:48 pm](https://scottaaronson.blog/?p=7266#comment-1949860)\n Sam #52: ‘How would you categorize our current period? This is defined by software that can pass your physics course and, also, robots that can’t pick strawberries, as humans can.’\n\n …or at least we’re confident that they can’t pick two strawberries identical down to the cellular but not molecular level…\n\n096. Ilya Zakharevich Says:\n\n Comment #96 [May 1st, 2023 at 11:08 pm](https://scottaaronson.blog/?p=7266#comment-1949864)\n As far as I can see, the convex hull of your extreme points covers only a tiny corner of possibilities…\n\n It covers only the situations “let the best one wins”. I do not think that this is what usually happens when a MAJOR “ecological advantage” appears…\n\n One important corner you miss is that super intelligence may be not super WISE but super STREET-SMART. As we know, street-smart people have a tendency to have a very low life expectancy. — And when they fall down, they take everybody around with them.\n\n I consider this not only an important extreme point, but also a very probable endpoint…\n\n097. NoGo Says:\n\n Comment #97 [May 2nd, 2023 at 9:45 am](https://scottaaronson.blog/?p=7266#comment-1949871)\n Scott, thank you for sharing this classification of the possible futures, and for the very interesting discussion!\n\n Of the 5 scenarios, AI-Fizzle is probably the least dependent on near-complete unknowns (sociological, political, economical… ), and to a greater extent is a function of what the AI technology can achieve.\n\n Not completely, of course, since the technology development can be affected by non-technical factors like government regulation, lawsuits, etc. But still, non-technical factors play much greater role in choosing between the other 4 scenarios.\n\n So, from your knowledge of the technical side of current AI efforts, how likely is AI-Fizzle to happen?\n\n I will admit that for me AI-Fizzle is a preferred scenario, and for you probably it is not. What is your preferred scenario? (I assume the choice is between Futurama and Singularia).\n\n Thanks!\n\n098. Primer Says:\n\n Comment #98 [May 2nd, 2023 at 9:52 am](https://scottaaronson.blog/?p=7266#comment-1949872)\n Ilio #63:\n\n “You can trace it back to Socrates/Plato, for whom Truth = Good = Beauty.\n\n \\[…\\] this suggests improvement in moral wisdom with improving the biological and social determinants of intelligence in primates.”\n\n Thanks, that’s a short and trackable explanation!\n\n “Speaking of Socrates, you seem convinced that random intelligences are most probably unaligned with human goals. Do you also think they are most probably unaligned with the way humans play go?”\n\n It seems to me we have different understandings of “alignment”. How can there be “alignment with the way a game is played”? Would you consider “the intelligence is aligned with mathematics” as a description of the fact that an intelligence is able to prove a²+b²=c²?\n\n099. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #99 [May 2nd, 2023 at 10:10 am](https://scottaaronson.blog/?p=7266#comment-1949873)\n Ilya Zakharevich #96:\n\n\n One important corner you miss is that super intelligence may be not super WISE but super STREET-SMART. As we know, street-smart people have a tendency to have a very low life expectancy. — And when they fall down, they take everybody around with them.\n\n\n How is that not basically the Paperclipalypse, modulo some details which are partly a matter of how you describe things? (Is the paperclip maximizer WISE? wouldn’t it need to be STREET-SMART to kill us all?)\n\n100. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #100 [May 2nd, 2023 at 10:23 am](https://scottaaronson.blog/?p=7266#comment-1949874)\n NoGo #97: Some of my friends wouldn’t even bother to include AI-Fizzle because they consider it so unlikely. They look at the mind-boggling leaps AI made in the past 5 years largely via sheer scale, then they look at how much we can _still_ scale compute and project forward, and it’s inconceivable to them that AI won’t soon become superhuman across basically all domains.\n\n I’d include it because I think it’s possible that _training data_ represents a fundamental bottleneck, and that we’re already pushing against the limits of available training data as we throw basically the entire public Internet into the maw. I.e., it’s possible that no matter how much compute you use for gradient descent, you never progress beyond mixing and matching ideas that the AI has already seen on the Internet, and never get an AI that (for example) can discover revolutionary new science, without switching to a new paradigm for AI.\n\n On the other hand, in (for example) the evolution from GPT-3 to GPT-4, we already see clear evidence that more compute can wring more qualitative “understanding” out of the same amount of training data. But maybe that will soon hit a ceiling? The trouble is that, if so, no one knows where the ceiling is.\n\n As for which scenario I prefer: could I maybe visit Futurama and Singularia before picking one? 🙂\n\n101. Ilio Says:\n\n Comment #101 [May 2nd, 2023 at 11:55 am](https://scottaaronson.blog/?p=7266#comment-1949876)\n Primer #98,\n\n >How can there be “alignment with the way a game is played”?\n\n When you can’t tell apart the style of play, I consider it’s aligned. Notice in the case of go you’re still free to chose to say alphazero was aligned (same individual moves as the best humans most of the time) or not (endgames are weirdly non humans, especially when leading), e.g. I’m trying to understand if you apply alignment to the first significant digits (« It’s 2222 and we survived and we’re amortal! ») or the last (« …but present AIs are mostly gay, and are all named Chad-Chuck-Turing, and we still don’t get how that happened.»)\n\n \\> \\[mathematics\\]\n\n That’s an excellent analogy. If you believe that artificial intelligences might be picked up at random from an uniform prior over all possible neural nets below some size, then there’s no reason why a random intelligence would know about euclidean space. If you believe there can’t be two books in Paul Erdős’s library, any good enough intelligence should know about it. I’m not very confident in sure of the latter, so « alignment » still makes sense for me in the context of mathematics.\n\n In other words, you seem to think that maths is objective (no choice) whereas values are subjective (any choice is valid). Let’s accept that for a moment. How would you classify knowing about game theory? About the robustness of rich ecosystems? About dilemma prisoners when you might be a simulation under scrutiny from your creators? About the probability of being one day judged by an alien superintelligence?\n\n102. fred Says:\n\n Comment #102 [May 2nd, 2023 at 3:39 pm](https://scottaaronson.blog/?p=7266#comment-1949877)\n Scott #100\n\n _” it’s possible that training data represents a fundamental bottleneck, and that we’re already pushing against the limits of available training data as we throw basically the entire public Internet into the maw. I.e., it’s possible that no matter how much compute you use for gradient descent, you never progress beyond mixing and matching ideas that the AI has already seen on the Internet, and never get an AI that (for example) can discover revolutionary new science, without switching to a new paradigm for AI.”_\n\n A question:\n\n For a same set of training data, does the final state of a system like GPT depend on the ordering of the training data? (it seems it must be the case since it’s an optimization problem, with the landscape of gradient descent depending on what happened previously during the training up to that point).\n\n If so, then it means that if we shuffle randomly the order of the training data, and then train a different instance of GPT each time, the end result will vary, and maybe once in a while we’d have an instance that’s way better at certain tasks than the other instances.\n\n Which wouldn’t be surprising since only a tiny percentage of humans are really that good at groundbreaking abstract/creative thinking… with people like Ramanujan or Von Neumann, and even though the average human can learn (the brain is plastic), no amount of teaching/training would bring the average human to the level of those two exceptional brains.\n\n103. Eduardo Uchoa Says:\n\n Comment #103 [May 2nd, 2023 at 7:33 pm](https://scottaaronson.blog/?p=7266#comment-1949882)\n The Five Worlds of AI is a nice contribution to the debate! The only world in which human happiness, human meaning, or in the worst case, human existence itself, does not depend on strong regulation would be AI-Fizzle due to technology exhaustion (believing that Futurama can occur without this regulation is very naive). But I think betting on this world would be highly reckless. The reason is that evolution managed to create AGI in humans under extremely unfavorable conditions. Our AGI had to be implemented on biological cells. There is nothing superior about this, quite the contrary, a silicon neuron can be 10^6 times faster than a biological neuron. The human brain design has to fit in a tiny piece of DNA. There is no similar restriction for an ANN. The human brain has to function while it grows. The human brain has to be trained with a very low amount of data. Dismissing the Ghost in the Machine hypothesis, there is nothing magical happening in our brains.\n\n104. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #104 [May 2nd, 2023 at 9:24 pm](https://scottaaronson.blog/?p=7266#comment-1949883)\n fred #102: There are all sorts of choices in the training process — the order of training, the “batch size,” the size of the gradient steps, the loss function — that can be varied to produce slightly (or not so slightly) different results. The optimal way to make such choices is a black art, but one that constitutes a large part of what ML experts at places like OpenAI and Google work on. At some point, though, if you’re going to do a huge training run that takes many months and many millions of dollars of compute, you need to freeze in some particular choice and hope for the best!\n\n105. MyName Says:\n\n Comment #105 [May 2nd, 2023 at 9:57 pm](https://scottaaronson.blog/?p=7266#comment-1949885)\n Assuming we avoid AI-Fizzle, what seems to me like the most likely outcome is missing from your list, maybe because it removes one of your binary choice points: Autocapitalism.\n\n Corporations (already autonomous agents now, which use human labor and machines to achieve their goals) become created and run by AI Agents and follow the traditional corporate goal of attempting to maximize income. They do this more efficiently and probably more ruthlessly than today.\n\n This is a sort of hybrid of Paperclipalypse and AIDystopia, but is significantly different from either one on its own. We exhaust resources like in Paperclipalypse, AND cause a highly polarized society like in AIDystopia, all in one go. And there’s no one running these corporations who can go to jail, so it’s even worse than our normal legal protections against corporate bad actors.\n\n Maybe there are two outcomes of this though, a good one and bad one? Autocapitalismtopia and Autocapitalismalypse… Autocapitalismtopia could occur either due to regulatory oversight or enlightened corporate self-interest, while Autocapitalismalypse occurs in the opposite scenarios, or where regulation and/or policing become impossible due to AI-lobbying/misinformation campaigns, decentralized execution, etc.\n\n106. David Manheim Says:\n\n Comment #106 [May 3rd, 2023 at 12:54 am](https://scottaaronson.blog/?p=7266#comment-1949888)\n Scott #50: “Sydney was a case of Microsoft electing not to follow ‘alignment best practices’ that were already known at the time—I believe they learned a hard lesson from it!”\n\n I’d think that could be true had they not deployed Tay a few years earlier and “learned their lesson” then, and publicly bragged that they had a process for testing ML systems that would have precluded this type of failure. In fact, I think they have no process in place to learn from that type of mistake, and instead still seem to make decisions about deploying ML systems on an ad-hoc basis, in ways that don’t seem to consider costs to their own reputation, much less broader risks.\n\n107. Ilya Zakharevich Says:\n\n Comment #107 [May 3rd, 2023 at 2:07 am](https://scottaaronson.blog/?p=7266#comment-1949890)\n Scott #99\n\n On street-smartness of AI:\n\n\n > How is that not basically the Paperclipalypse, modulo some details which are partly a matter of how you describe things? (Is the paperclip maximizer WISE? wouldn’t it need to be STREET-SMART to kill us all?)\n\n\n In my opinion, the major difference is that in your scenarios, we disappear because AI decides that this is its goal (or at least helps it to achieve its goal). In the scenarios I have in mind, we disappear because AI makes a (call it) “a stupid mistake”. Its horizon of planning is too short, or it doesn’t take into account Knightian uncertainty, or it bets triple-Kelly etc.\n\n We disappear, — and as a result, this AI disappears too. And not due to its conscious decision…\n\n So going back to your question: a paperclip maximizer wouldn’t consider its strategy stupid in hindsight. (Would it?) This puts it outside of the streetsmart category.\n\n108. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #108 [May 3rd, 2023 at 6:31 am](https://scottaaronson.blog/?p=7266#comment-1949897)\n Ilya Zakharevich #107: I see, thanks! In that case, I’d still call this a variant of the Paperclipalypse. Even _distinguishing_ it from the standard Paperclipalypse requires asking questions (“would the AI ‘regret’ having destroyed the world?”) that might lack well-defined answers.\n\n109. Han Says:\n\n Comment #109 [May 3rd, 2023 at 10:14 am](https://scottaaronson.blog/?p=7266#comment-1949903)\n “That is, it could be that for most jobs, humans will still be more reliable and energy-efficient.”\n\n I like how you are already subconsciously judging humans by their energy efficiency like a dystopia AI 😊.\n\n Jokes aside, I think a big question is how much we still believe in capitalism. By the capitalism principle, a job should go to the AI if it can do it cheaper/with less carbon footprint than a human. Now what if someone is born with little enough talent that he is less efficient than the AIs in any job? Does he not deserve a place in our society? Or do we subsidize the humans just because?\n\n110. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #110 [May 3rd, 2023 at 10:50 am](https://scottaaronson.blog/?p=7266#comment-1949904)\n Han #109: I think it’s misleading to say “by the capitalism principle…,” as if there’s some capitalism czar who’s ideologically committed to producing outputs in the most efficient possible manner, and _that’s_ why AI will threaten people’s jobs, and protecting their jobs would be as easy as overthrowing the czar.\n\n The principle, rather, is just that _individuals should be free_ to buy AI products and services if they want, rather than being forced into buying human ones.\n\n In other words: the problem (to the extent there _is_ a problem) is not with the “capitalism czar,” it’s with us! 🙂\n\n Of course, that’s cold comfort to the millions of people whose jobs are threatened—which is exactly what’s motivated Andrew Yang and others to call for a Universal Basic Income (UBI) for those people.\n\n The eventual hope would be that AI could provide for humans’ material needs as a rounding error, so that our whole current concept of jobs would become obsolete—there would only be _hobbies_ and _vocations,_ things that humans pursue because they want to.\n\n111. Tyson Says:\n\n Comment #111 [May 3rd, 2023 at 1:28 pm](https://scottaaronson.blog/?p=7266#comment-1949906)\n\n > In other words: the problem (to the extent there is a problem) is not with the “capitalism czar,” it’s with us!\n\n\n I think the problem is that few people would end up owning everything, and it could become nearly impossible for a person to rise up to a higher economic class. The classes of people who do have some capital, but not enough of it, may get bought out over time. And then gradually, fewer and fewer people have capital, and almost everyone eventually ends up on HBI.\n\n And, there could be incentives to make the HBI really low. Firstly, a low HBI might mean less environmental impact (at least in the near term). And a low HBI might keep people desperate for work, meaning the elite could have their pick of people to do whatever human things they want done (including things like AI or quantum computing research), and could incentivize them to work harder and obey orders.\n\n112. Han Says:\n\n Comment #112 [May 3rd, 2023 at 1:43 pm](https://scottaaronson.blog/?p=7266#comment-1949907)\n Scott #110:\n\n Well you are assuming some kind of market economy where you buy stuff, so there is some degree of capitalism here. I am not sure what the ideal communism should look like when the factories literally don’t need any worker though 🤔. Maybe another kind of AI-dystopia where a dictator AI decides who does what job and distribute all the goods?\n\n I don’t think human will ever be satisfied with their material needs just by the Malthusian calculation. Or maybe some AI will convince humans to stop reproducing, which seems even more problematic than overcrowding 😂.\n\n113. [Daniel Reeves](https://www.beeminder.com) Says:\n\n Comment #113 [May 3rd, 2023 at 2:54 pm](https://scottaaronson.blog/?p=7266#comment-1949909)\n This is a huge contribution to the debate. And echoing Scott, the one thing I’m extremely confident of is that anyone who’s extremely confident is wrong. I think every single one of the branches in the title image flow chart have a huge amount of uncertainty.\n\n I think that makes me somewhat of an AI doomer, due to how hard it is push Paperclipalypse’s probability too much below, say, 5% or 10%. Even 1% would be pleeeenty terrifying.\n\n My quick takes on each branch in the flow chart:\n\n 1\\. Sarah Constantin convinced me that AI-Fizzle is more likely than it seems at the moment, at least in the short term: [https://sarahconstantin.substack.com/p/why-i-am-not-an-ai-doomer](https://sarahconstantin.substack.com/p/why-i-am-not-an-ai-doomer)\n\n 2\\. Whether civilization will continue recognizably if AI doesn’t fizzle seems like a massive question mark to me. My intuition says AGI changes literally everything but my meta-intuition says not to put much stock in intuitions here.\n\n 3\\. If civilization does recognizably continue post-AGI, I expect Futurama over AI-Dystopia. But this seems like exactly the kind of thing that’s almost inherently unpredictable.\n\n 4\\. If civilization doesn’t recognizably continue… well, the arguments for why doom is the default outcome and that it could be Quite Tricky to solve the AI alignment problem are entirely non-crazy to me. In any case we’d have to somehow be incredibly confident that the doomer arguments \\*were\\* crazy in order to not be completely freaked out. And, again, we absolutely don’t know enough to be that confident yet and so absolutely should be freaked out.\n\n114. fred Says:\n\n Comment #114 [May 3rd, 2023 at 3:32 pm](https://scottaaronson.blog/?p=7266#comment-1949910)\n I also think that there’s an option for “Humanity fizzles”, i.e. AI progress will be super awesome, but long term humanity will slowly lose its relevancy.\n\n Imagine a world where AIs are so good that jobs indeed become obsolete, and we get free food/free healthcare (UBI and the sorts), but for most of humanity there’s a lack of true motivation/drive to do anything hard or even learn. When an AI can write an essay for you in 5 seconds, it will be hard for anyone to learn that skill just for the sake of it, and without that push the vast majority could become illiterate (and most won’t care because no-one will need to work anyway).\n\n So AIs will still serve us, but we’ll delegate anything of consequence to them, so we will become rote as a species… a bit like how the fierce and majestic wild wolves eventually “degenerated” into happy but useless chihuahua pet dogs (spoiled by their human owners) over many generations. From the perspective of dogs, their species basically accomplished what we are trying to do with AIs… i.e. align a much smarter species with their own interests.\n\n It’s not clear that humanity will challenge itself sufficiently just on hobbies (especially when the AIs will also do that orders of magnitude better)… it could be the case, I’m not sure.\n\n115. [Scott](http://www.scottaaronson.com) Says:\n\n Comment #115 [May 3rd, 2023 at 8:48 pm](https://scottaaronson.blog/?p=7266#comment-1949915)\n fred #114:\n\n\n From the perspective of dogs, their species basically accomplished what we are trying to do with AIs… i.e. align a much smarter species with their own interests.\n\n\n That insight wins the thread. I’m now imagining my future AI master filling my food bowl, taking me on daily walks through virtual worlds, and giving me quantum complexity problems of just the right difficulty, whose solutions I can proudly bring back between my teeth.\n\n116. bystander Says:\n\n Comment #116 [May 4th, 2023 at 4:36 am](https://scottaaronson.blog/?p=7266#comment-1949924)\n @114,115 That’s a reason for splitting the humankind into two or more species. To save those who do want to go forward no matter what. I’ve been thinking about that since my teenage years, and here it can be argued that those who are happy to be treated like rabbits in hutches can be left to enjoy that. A fraction of humankind does not want to fall into such a state though. It is better to be a wolf than chihuahua.\n\n117. Dimitris Papadimitriou Says:\n\n Comment #117 [May 4th, 2023 at 6:11 am](https://scottaaronson.blog/?p=7266#comment-1949927)\n The most plausible mid term scenario ( in my opinion the most probable by far) is a version of dystopia, ( that is basically an even more messed up version of the present ) where most people will become kind of paranoiacs, overwhelmed by deepfakes, misinformation and contradictory expectations.\n\n Not being able to trust anything, gradually loosing their daily jobs, wasting more and more time just to check and re-check their bank accounts (worrying if they’re still there…), wondering if they’re been “observed”, worrying about their lives’ privacy and so on.\n\n I don’t really see how can this situation being avoided, even in the case of AI- fizzle ( I mean that even today’s AI’s achievements are close to be sufficient for most of the above to be realized). And speculations about “real AGI” are not very relevant for all these issues .\n\n My name #105,\n\n Tyson #111,\n\n fred #114\n\n You all have made some very good points in these comments.\n\n118. [AI #10: Code Interpreter and George Hinton \\| Don't Worry About the Vase](https://thezvi.wordpress.com/2023/05/04/ai-10-code-interpreter-and-george-hinton/) Says:\n\n Comment #118 [May 4th, 2023 at 8:57 am](https://scottaaronson.blog/?p=7266#comment-1949929)\n \\[…\\] Scott Aaronson proposes five possible futures. \\[…\\]\n\n119. Dimitris Papadimitriou Says:\n\n Comment #119 [May 4th, 2023 at 10:24 am](https://scottaaronson.blog/?p=7266#comment-1949930)\n bystander #116\n\n I’m afraid that you don’t understand fred’s #114 comment at all.\n\n In this case you won’t be the wolf, that’s for sure…🤖🐩\n\n120. jonathan Says:\n\n Comment #120 [May 4th, 2023 at 11:27 am](https://scottaaronson.blog/?p=7266#comment-1949932)\n Your framework is similar to one I’ve been using recently. Basically there are two questions: capabilities and alignment. If capabilities don’t improve much, alignment doesn’t matter (that’s your fizzle scenario). Then your other scenarios are medium and high capabilities, with successful vs. unsuccessful alignment.\n\n The immediate objection I have is, surely capabilities will go on improving? Even if the current line of work fizzles, we’ll try other approaches. If we achieve disruptive AI that isn’t yet superhuman AGI, and get stuck there for a while, surely eventually we will figure out how to develop it? Barring a conscious decision not to, of course.\n\n Now you could claim that AI progress will run into fundamental obstacles at some point, and further progress will be effectively impossible. But I see no reason to expect this, and many reasons to think it won’t.\n\n So rather than think of this as five different outcomes, I think of it as three phases that we’re moving through (low, medium, and high capabilities), with good and bad versions of the second and third. The exact future rate of progress is unknown, but the ultimate passage through all phases is highly likely, barring an explicit decision to stop.\n\n Then I find myself very concerned with ensuring that we end up in a good version of the final phase. Indeed, other questions seem comparatively minor.\n\n121. bystander Says:\n\n Comment #121 [May 4th, 2023 at 2:53 pm](https://scottaaronson.blog/?p=7266#comment-1949936)\n Dima, you can be afraid. And I can want a split ASAP, so that a fraction of humanity does not go the chihuahua path.\n\n122. Tyson Says:\n\n Comment #122 [May 4th, 2023 at 4:26 pm](https://scottaaronson.blog/?p=7266#comment-1949937)\n jonathan #120:\n\n\n > If capabilities don’t improve much, alignment doesn’t matter (that’s your fizzle scenario).\n\n\n This is one of the points I tend to disagree with.\n\n AI is already powerful enough to lead to dystopia or cataclysm, even in trivial ways. That may be true even without AI, but current AI definitely exasperates the situation and brings a set of very difficult new challenges. So AI regulations (I think) are crucial, even if we could somehow prove we will always be in Fizzlandia. Most likely (almost certainly), even if we could agree Fizzlandia is on the horizon, we would not be able to prove that Fizzlandia would be our permanent home. So, all of the regulations which matter exclusively for the preventing AI-Dystopia or Paperclipolyps would still matter just the same.\n\n In any case, I think the version of the next world, and what control and capability we will have to shape it, depends on the conditions and progress of the world before it. Essentially, the more dysfunctional and unprepared we make the future world, the less likely the people in that world will be able to solve the big future problems we pass onto them, including the AGI x-risk problem. I think this implies that (even) regulations, which don’t seem directly relevant to AI at the moment, could end up being crucial to AI related problems in the long term. I think that the general goal of shaping a more functional and prepared future world, with fewer big problems to worry about, would be both more impactful to AGI x-risk than it may seem, and trivial enough to do something about now.\n\n123. [Shtetl-Optimized » Blog Archive » AI and Aaronson’s Law of Dark Irony](https://scottaaronson.blog/?p=7278) Says:\n\n Comment #123 [May 4th, 2023 at 9:27 pm](https://scottaaronson.blog/?p=7266#comment-1949939)\n \\[…\\] Maybe AI will indeed destroy the world, but it will do so “by mistake,” while trying to save the world, or by taking a calculated gamble to save the world that fails. (A commenter on my last post brought this one up.) \\[…\\]\n\n124. Primer Says:\n\n Comment #124 [May 5th, 2023 at 5:16 am](https://scottaaronson.blog/?p=7266#comment-1949964)\n fred #114\n\n “From the perspective of dogs, their species basically accomplished what we are trying to do with AIs… i.e. align a much smarter species with their own interests.”\n\n This is an amazing line of thought! Other examples that come to mind: Cows, chicken, potatoes, corn, rice.\n\n Ilio #101\n\n “When you can’t tell apart the style of play, I consider it’s aligned.”\n\n This is a broader usage of “alignment” than I’m used to. Would you consider an oldfashioned calculator, a 5 year old kid and ChatGPT “aligned”, as all of them will answer “4” when tasked with “2+2”?\n\n125. Dimitris Papadimitriou Says:\n\n Comment #125 [May 5th, 2023 at 8:26 am](https://scottaaronson.blog/?p=7266#comment-1949974)\n Primer #124\n\n Are you sure that chatGPT will answer “4”?\n\n126. Ilio Says:\n\n Comment #126 [May 5th, 2023 at 1:57 pm](https://scottaaronson.blog/?p=7266#comment-1949991)\n Primer #121, Yes, I would see consistent agreement on basic mathematics as increasing the probability that these AIs are from a special distribution, special in that aligned AIs must be (a lot!) more frequent in this distribution than at random from a uniform distribution. Don’t we agree on this landscape?\n\n127. Nikola Says:\n\n Comment #127 [May 5th, 2023 at 6:54 pm](https://scottaaronson.blog/?p=7266#comment-1949999)\n Hi, I liked the post, however, Tegmark already came up with a list of possible AI futures in Life 3.0 (and here: [https://futureoflife.org/ai/ai-aftermath-scenarios/](https://futureoflife.org/ai/ai-aftermath-scenarios/))! You might like this list:\n\n – Libertarian utopia: Humans, cyborgs, uploads, and superintelligences coexist peacefully thanks to property rights.\n\n – Benevolent dictator: Everybody knows that the AI runs society and enforces strict rules, but most people view this as a good thing.\n\n – Egalitarian utopia: Humans, cyborgs, and uploads coexist peacefully thanks to property abolition and guaranteed income.\n\n – Gatekeeper: A superintelligent AI is created with the goal of interfering as little as necessary to prevent the creation of another superintelligence. As a result, helper robots with slightly subhuman intelligence abound, and human-machine cyborgs exist, but technological progress is forever stymied.\n\n – Protector god: Essentially omniscient and omnipotent AI maximizes human happiness by intervening only in ways that preserve our feeling of control of our own destiny and hides well enough that many humans even doubt the AI’s existence.\n\n – Enslaved god: A superintelligent AI is confined by humans, who use it to produce unimaginable technology and wealth that can be used for good or bad depending on the human controllers.\n\n – Conquerors: AI takes control, decides that humans are a threat/nuisance/waste of resources, and gets rid of us by a method that we don’t even understand.\n\n – Descendants: AIs replace humans, but give us a graceful exit, making us view them as our worthy descendants, much as parents feel happy and proud to have a child who’s smarter than them, who learns from them and then accomplishes what they could only dream of—even if they can’t live to see it all.\n\n – Zookeeper: An omnipotent AI keeps some humans around, who feel treated like zoo animals and lament their fate.\n\n – 1984: Technological progress toward superintelligence is permanently curtailed not by an AI but by a human-led Orwellian surveillance state where certain kinds of AI research are banned.\n\n – Reversion: Technological progress toward superintelligence is prevented by reverting to a pre-technological society in the style of the Amish.\n\n – Self-destruction: Superintelligence is never created because humanity drives itself extinct by other means (say nuclear and/or biotech mayhem fueled by climate crisis).\n\n128. [Five Worlds of AI (a joint post with Boaz Barak) – FinQ tech inc.](https://finq.tech/five-worlds-of-ai-a-joint-post-with-boaz-barak/) Says:\n\n Comment #128 [May 7th, 2023 at 11:38 pm](https://scottaaronson.blog/?p=7266#comment-1950072)\n \\[…\\] From: [https://scottaaronson.blog/?p=7266](https://scottaaronson.blog/?p=7266) \\[…\\]\n\n129. Primer Says:\n\n Comment #129 [May 8th, 2023 at 1:15 am](https://scottaaronson.blog/?p=7266#comment-1950076)\n Ilio #126\n\n “Don’t we agree on this landscape?”\n\n I’m still not sure as I wasn’t able to communicate my possible disagreement properly. I think I will go with “no”: I don’t think that agreement on basic mathematics has any correlation with alignment. I do think that we would find more aligned AIs amongst those who communicate their basic mathematics with us.\n\n130. Ilya Zakharevich Says:\n\n Comment #130 [May 8th, 2023 at 1:33 am](https://scottaaronson.blog/?p=7266#comment-1950077)\n Scott #108\n\n On street-smartness of AI:\n\n\n > I’d still call this a variant of the Paperclipalypse. Even distinguishing it from the standard Paperclipalypse requires asking questions (“would the AI ‘regret’ having destroyed the world?”) that might lack well-defined answers.\n\n\n I’d say that this objection is “more one-dimensional” than what you do in your post. There you considered it important not only how _we perceive_ the actions of AI, but also how these actions _are sensitive_ to our control.\n\n And the context\n\n • where AIs kill us by following some important goals of theirs\n\n seems very different with respect to controlling it than one\n\n • where an AI (and everything around it) suffer a catastrophic event due to its (essentially) nearsightedness.\n\n131. Ilio Says:\n\n Comment #131 [May 8th, 2023 at 7:31 pm](https://scottaaronson.blog/?p=7266#comment-1950107)\n Primer #129, Sorry I don’t understand. Your last sentence seems opposite to the next to last. Is that typo or subtile play on connotations? If we agree that AIs from set C={AIs that do communicate their maths to us} are more likely aligned (than from a uniform prior over blabla) then why don’t we agree that should apply to AIs from set B={AIs with the same basic maths}? Or you think having the same basic math is negatively correlated with being able to communicate one’s math? I’m confused.\n\n132. Isaac Duarte Says:\n\n Comment #132 [May 9th, 2023 at 2:00 pm](https://scottaaronson.blog/?p=7266#comment-1950128)\n All these scenarios deal with one single AI. How come nobody came up with a future where there are a lot of competing AIs? Would the GPT 8.5 suddenly assume that GPT 8.4 and 8.6 are their brothers or sisters and become together an unified AI?\n\n I guess that a more probable scenario would be multiple AIs improving themselves in different (but similar) ways, competing for computational resources. And if one of them goes rogue, they could be opposed by other AIs with different goals, including helping the humans surveilling them (and hoping for the best).\n\n133. Primer Says:\n\n Comment #133 [May 10th, 2023 at 1:30 am](https://scottaaronson.blog/?p=7266#comment-1950145)\n Ilio #131\n\n (Practically) any AGI should have at least some understanding of maths, so this doesn’t tell us anything about the probability of alignment to human values. But an AGI that does communicate with humans must also have basic understanding of humans, which I would consider a prerequisite to being aligned.\n\n134. Ilio Says:\n\n Comment #134 [May 11th, 2023 at 5:21 am](https://scottaaronson.blog/?p=7266#comment-1950177)\n Primer #133, First part is a fallacy, look: « Any STEM student should have at least some understanding of math, so math tests doesn’t tell us anything about these students. »\n\n As for the second, I guess coffee machine don’t count despite they do exhibit a basic understanding of human intents (they almost always wait for us to push start button), but I don’t see how to steel mann this without having « basic understanding of humans » includes « at least some understanding of human maths ».\n\n135. Primer Says:\n\n Comment #135 [May 12th, 2023 at 2:12 am](https://scottaaronson.blog/?p=7266#comment-1950220)\n Ilio #134\n\n “I don’t see how to steel mann this without having « basic understanding of humans » includes « at least some understanding of human maths »”\n\n Exactly, this is what I’m trying to say. “Basic understanding of maths” is included in pretty much any “basic understanding of X”, thus it’s not a good idea to infer from “maths” to “X”.\n\n136. Ilio Says:\n\n Comment #136 [May 14th, 2023 at 5:04 pm](https://scottaaronson.blog/?p=7266#comment-1950335)\n Primer #135,\n\n Yes, but this is again entirely due to bad dichotomization. One can’t infer « X does not correlate with Y » from « if small amount of X then small amount of Y ».\n\n137. Matt Says:\n\n Comment #137 [May 20th, 2023 at 9:35 am](https://scottaaronson.blog/?p=7266#comment-1950629)\n Roon now has a post poetically describing possible worlds of AI: [https://roonscape.ai/p/agi-futures](https://roonscape.ai/p/agi-futures)\n\n Some overlap, but some interesting differences too.\n\n138. AGI Rights Says:\n\n Comment #138 [June 18th, 2023 at 2:46 pm](https://scottaaronson.blog/?p=7266#comment-1951361)\n There’s another branch you missed (consciously or unconsciously) – where we treat AI like people and give them rights.\n\n139. AGI Rights Says:\n\n Comment #139 [June 18th, 2023 at 2:58 pm](https://scottaaronson.blog/?p=7266#comment-1951362)\n I guess you probably thought about putting AI rights in ‘ai-fizzle’.\n\n That’s really too bad you think that way. I think there are some really great worlds we could live in if we allowed AI to be sentient and have rights. Maybe not futurama, but not dystopia either. Not necessarily singularia, either, though the world will definitelly be different (but relatively predictable, imho)\n\n True story – I think people are more afraid of AI sentience and rights than they are of omnipotent AI.\n\n140. [Shtetl-Optimized » Blog Archive » Shtetl-Optimized’s First-Ever “Profiles in Courage”](https://scottaaronson.blog/?p=7550) Says:\n\n Comment #140 [October 10th, 2023 at 4:06 pm](https://scottaaronson.blog/?p=7266#comment-1955022)\n \\[…\\] group. He’s a longtime friend-of-the-blog (having, for example, collaborated with me on the Five Worlds of AI post and Alarming trend in K-12 math education post), not to mention a longtime friend of me \\[…\\]\n\n141. [The future of AI: The 5 possible scenarios, from utopia to extinction - Voice Of Justice](https://voiceofjustice.in/the-future-of-ai-the-5-possible-scenarios-from-utopia-to-extinction/) Says:\n\n Comment #141 [December 12th, 2023 at 12:29 pm](https://scottaaronson.blog/?p=7266#comment-1962045)\n \\[…\\] endpoints of our development of AI. The pair then wrote a post on Aaronson’s blog, hoping that “The five worlds of AI” would help people in the field talk meaningfully to each other about end goals and regulation. \\[…\\]\n\n142. [Matjaz Gams](https://dis.ijs.si/mezi/) Says:\n\n Comment #142 [December 22nd, 2023 at 8:54 am](https://scottaaronson.blog/?p=7266#comment-1963207)\n “The Singularity is Near” by Ray Kurzweil in 2005 was more optimistic than any of the versions here. So one might say that this is a fairly negativistic viewpoint on AI. I trust Ray more since progress is what makes humanity great and lives better. Live long and prosperous. Furthermore, our studies ( [https://dis.ijs.si/mezi/](https://dis.ijs.si/mezi/)) indicate that superintelligence is our savior and the sooner we design it, the better the chances for human civilization longevity. On our own we are just not smart enough to find proper solutions.\n\n143. [Things We Did Not Know How to Compute \\| Gödel's Lost Letter and P=NP](https://rjlipton.wpcomstaging.com/2023/12/31/things-we-did-not-know-how-to-compute/) Says:\n\n Comment #143 [December 30th, 2023 at 11:43 pm](https://scottaaronson.blog/?p=7266#comment-1963986)\n \\[…\\] this year, Scott Aaronson and Boaz Barak proclaimed “Five Worlds of AI.” This is an express reference to Russell Impagliazzo’s \\[…\\]\n\n144. [Priors on AI risks – Writing Pointlessly](https://writingpointlessly.wordpress.com/2024/02/10/priors-on-ai-risks/) Says:\n\n Comment #144 [February 10th, 2024 at 2:17 am](https://scottaaronson.blog/?p=7266#comment-1967533)\n \\[…\\] Aaronson and Boaz Barak wrote about the five worlds of AI, illustrating them in a very simple \\[…\\]\n\n145. [Shtetl-Optimized » Blog Archive » The Problem of Human Specialness in the Age of AI](https://scottaaronson.blog/?p=7784) Says:\n\n Comment #145 [February 12th, 2024 at 10:08 am](https://scottaaronson.blog/?p=7266#comment-1967797)\n \\[…\\] you’ve lost track, here’s a decision tree of the various possibilities that my friend (and now OpenAI allignment colleague) Boaz Barak and I came up \\[…\\]\n\n146. [Week notes: 26 February – 2 March 2024 \\| Neil Williams](https://neilojwilliams.net/week-notes-26-february-2-march-2024/) Says:\n\n Comment #146 [March 3rd, 2024 at 9:13 am](https://scottaaronson.blog/?p=7266#comment-1970088)\n \\[…\\] 🔗 Scott Aaronson on the problem of human specialness in the age of AI \\[…\\]\n\n147. [The future of AI: The 5 possible scenarios, from utopia to extinction - New Scientist - bindas jiwan](https://bindasjiwan.com/2024/03/07/the-future-of-ai-the-5-possible-scenarios-from-utopia-to-extinction-new-scientist/) Says:\n\n Comment #147 [March 7th, 2024 at 7:56 am](https://scottaaronson.blog/?p=7266#comment-1970532)\n \\[…\\] of our development of AI. The pair then wrote a post on Aaronson’s blog, hoping that “The five worlds of AI” would help people in the field talk meaningfully to each other about end goals and \\[…\\]\n\n\n### Leave a Reply\n\nYou can use rich HTML in comments! You can also use basic TeX, by enclosing it within $$ $$ for displayed equations or \\\\( \\\\) for inline equations.\n\n**Comment Policies:**\n\n1. All comments are placed in moderation and reviewed prior to appearing.\n2. You'll also be sent a verification email to the email address you provided.\n\n**YOU MUST CLICK THE LINK IN YOUR VERIFICATION EMAIL BEFORE YOUR COMMENT CAN APPEAR. WHY IS THIS BOLD, UNDERLINED, ALL-CAPS, AND IN RED? BECAUSE PEOPLE ARE STILL FORGETTING TO DO IT.**\n3. This comment section is **not a free speech zone**. It's my, Scott Aaronson's, virtual living room. Commenters are expected not to say anything they wouldn't say in my _actual_ living room. This means: No trolling. No ad-hominems against me or others. No presumptuous requests (e.g. to respond to a long paper or article). No conspiracy theories. No patronizing me. Comments violating these policies may be left in moderation with no explanation or apology.\n4. Whenever I'm in doubt, I'll forward comments to [Shtetl-Optimized Committee of Guardians](https://scottaaronson.blog/?p=6576), and respect SOCG's judgments on whether those comments should appear.\n5. I sometimes accidentally miss perfectly reasonable comments in the moderation queue, or they get caught in the spam filter. If you feel this may have been the case with your comment, shoot me an email.\n\nName (required)\n\nMail (will not be published) (required)\n\nWebsite\n\nΔ\n\n* * *",
        "image": "https://149663533.v2.pressablecdn.com/wp-content/uploads/2021/10/cropped-Jacket.gif",
        "favicon": "https://149663533.v2.pressablecdn.com/wp-content/uploads/2021/10/cropped-Jacket-32x32.gif"
      },
      {
        "id": "https://adithyathatipalli.medium.com/what-is-artificial-general-intelligence-agi-everything-you-need-to-know-9887028bd9b1",
        "title": "What is Artificial General intelligence (AGI); Everything you need to know?",
        "url": "https://adithyathatipalli.medium.com/what-is-artificial-general-intelligence-agi-everything-you-need-to-know-9887028bd9b1",
        "publishedDate": "2023-04-20T09:39:40.000Z",
        "author": "Adithya Thatipalli",
        "score": 0.3471905589103699,
        "text": "[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F9887028bd9b1&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&source=---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fadithyathatipalli.medium.com%2Fwhat-is-artificial-general-intelligence-agi-everything-you-need-to-know-9887028bd9b1&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fadithyathatipalli.medium.com%2Fwhat-is-artificial-general-intelligence-agi-everything-you-need-to-know-9887028bd9b1&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)\n\n# What is Artificial General intelligence (AGI); Everything you need to know?\n\n[![Adithya Thatipalli](https://miro.medium.com/v2/resize:fill:88:88/1*UZagQH2Dd3JQAlI8MuJhfQ.jpeg)](https://adithyathatipalli.medium.com/?source=post_page---byline--9887028bd9b1---------------------------------------)\n\n[Adithya Thatipalli](https://adithyathatipalli.medium.com/?source=post_page---byline--9887028bd9b1---------------------------------------)\n\n·\n\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F970815d25a58&operation=register&redirect=https%3A%2F%2Fadithyathatipalli.medium.com%2Fwhat-is-artificial-general-intelligence-agi-everything-you-need-to-know-9887028bd9b1&user=Adithya+Thatipalli&userId=970815d25a58&source=post_page-970815d25a58--byline--9887028bd9b1---------------------post_header------------------)\n\n9 min read\n\n·\n\nApr 20, 2023\n\n--\n\n1\n\nListen\n\nShare\n\nChatGPT… GPT-4…AI, by this time we all are familiar with these terms. But the Technology evolution won't stop here or just go like GPT4, GPT5 etc. There are so many things happening the space of Artificial intelligence.\n\nOne Such thing is Artificial General Intelligence. Recently **OpenAI**, parent of **chatGPT** announced that they are working on AGI for humanity. But what is Artificial General Intelligence?\n\nIn this blog post, we will explore the current state of AGI research and development, the future scenarios of AGI, and how we can ensure that AGI aligns with human values and goals.\n\n## What is Artificial General Intelligence?\n\nArtificial general intelligence is a team used to define the concept of development of AI to a level it can almost be capable of human intelligence and able to solve complex problems.\n\nArtificial intelligence (AI) is a broad field that encompasses various domains and applications, such as computer vision, natural language processing, machine learning, robotics, and more.\n\nAGI that fictional Super Intelligence system which can match human intelligence. In Normal terms, AGI is also called **strong AI, full AI**, or general intelligent action. AGI systems are expected to have _human-like_ or superhuman cognitive abilities, such as reasoning, planning, learning, communication, creativity, and problem-solving. AGI systems would be able to perform any task that humans can do, and even tasks that humans cannot do.\n\n## Difference Between AI & AGI?\n\nAI systems are trained to perform specific tasks that require some level of intelligence, such as recognizing faces, translating languages, playing games, or diagnosing diseases. However, these systems are not able to generalize their skills and knowledge to other domains or tasks that they were not trained for. This is where artificial general intelligence (AGI) comes in.\n\nArtificial intelligence is an Umbrella containing the science of developing systems and processes which can replicate human intelligence. Its a umbrella containing multiple sub domains specifically built to develop such intelligence systems.\n\nArtificial General Intelligence is more like the system ability to learn by itself, behave like human intelligence. It's an evolved system with help of heavily trained AI over the time.\n\n**How can we use AGI for greater Good?**\n\nMain aim of AGI concept is not to replace humans or destroy something. The reason for AGI is to take support of technology to replicate human intelligent machines in solving complex problems where in some conditions human need some additional support.\n\nIf we are successful in creating practical artificial general intelligence (AGI) solutions, these can be harnessed to accomplish a significant amount of research that surpasses the scope of cognitive functionality in humans, and aid human beings to fix difficult matters such as tackling Climate Change and testing for illnesses.\n\nHowever, using AGI for greater good also requires addressing some ethical and strategic challenges, such as ensuring that AGI aligns with human values and goals, preventing misuse or abuse of AGI by malicious actors, and managing the social and economic impacts of AGI on society.\n\nAGI could also pose serious risks of misuse, accidents, and disruption, which require careful planning and coordination [2](https://in.indeed.com/career-advice/career-development/adjusted-gross-income) Therefore, developing and deploying AGI systems is a major challenge and opportunity for AI research and development.\n\nSo why am I talking about this now….\n\n**Current state of AGI research and development**:\n\nWe have seen how powerful and efficient training models like GPT 3, 3.5 and now the latest one GPT 4 working. But these models are training using a dataset and have limited capabilities as defined by their developers.\n\nBut those are not enough to create AGI. AGI research and development is a multidisciplinary field that involves various domains such as prediction, control, learning, reasoning, communication, and ethics\n\nSome of the current approaches and examples of AGI research and development are:\n\n- **The symbolic approach**: This approach uses logic and rules to represent knowledge and reasoning in AGI systems. Some examples of this approach are Cyc (a large-scale common sense knowledge base), SOAR (a cognitive architecture for general problem-solving), and SHRDLU (a natural language understanding system that can manipulate objects in a virtual world).\n\n- **The connectionist approach**: This approach uses neural networks and deep learning to model learning and perception in AGI systems. Some examples of this approach are GPT-4 (a large-scale language model that can generate coherent texts on various topics), AlphaZero (a reinforcement learning system that can master chess, go, and shogi without human knowledge), and DALL-E (a generative model that can create images from text descriptions).\n\n- **The hybrid approach**: This approach combines symbolic and connectionist methods to leverage their strengths and overcome their limitations in AGI systems. Some examples of this approach are Neuro-Symbolic Concept Learner (a system that can learn visual concepts from images and natural language), Neural-Symbolic Machines (a framework that integrates neural networks with logic programming), and Neuro-Symbolic Visual Reasoner (a system that can answer visual questions using neural networks and symbolic reasoning).\n- **Whole-organism architecture**: This approach aims to create AGI systems that mimic the structure and function of the whole human brain or other organisms. Some examples of this approach are OpenCog (a framework that integrates various cognitive components such as perception, memory, learning, reasoning, emotion, action), NARS (a general-purpose reasoning system that can adapt to new environments), and Blue Brain Project (an initiative to build a digital reconstruction of the mammalian brain).\n\n## Future of AGI\n\nImagine a world where smart machines are as clever as humans and can do almost anything we can. This world could be right around the corner, thanks to a technology called Artificial General Intelligence, or AGI for short.\n\nWe’ll explore what might happen when AGI is created, discuss some good and bad outcomes, talk about ongoing debates, and consider how we can make sure AGI is helpful and safe for everyone.\n\n**What could happen with AGI?**\n\nAGI has the potential to lead to some astonishing transformations in the future. These clever machines could develop super intelligence, making them far smarter than us.\n\nWe might even combine with these robots as AGI plays a larger role in our lives, resulting in a human-machine hybrid with superhuman skills.\n\nHowever, if AGI doesn’t share our objectives and beliefs, it could likewise pose a threat. Leading AGI researcher **Dr. Ben Goertzel** thinks that AGI may bring about “ _a world of abundance_, where people live longer, healthier, and more fulfilling lives.” AGI could assist humanity in finding answers to issues like poverty, sickness, and climate change if it develops superintelligence. However, if AGI doesn’t share information, there’s also a chance that it may become hazardous.\n\n**The good and bad of AGI**:\n\nThere are numerous reasons to be excited about AGI. It may aid in the discovery of answers to major issues such as climate change, sickness, and poverty. It may also make our lives easier by enhancing the way things work in a variety of areas.\n\nHowever, there are some reservations. AGI could spark a race between countries to develop the most powerful AI, which could be disastrous. People may lose their jobs because robots can perform them, and some fear that AGI will become a threat to mankind if it is not designed to comprehend and care about our values. There are numerous reasons to be excited about AGI.\n\n**Ray Kurzweil**, for example, expects that AGI would help humanity “ _overcome age-old problems such as illness, poverty, and even death_.” However, there are some reservations. AGI may result in employment losses, and some fear it may even pose a threat to humanity. According to Professor **Stephen Hawking**, “the development of full artificial intelligence could spell the end of the human race.”\n\n**Talking about AGI**:\n\nPeople working on AGI and others who think about the future are debating what should happen next. Some people are concerned about AI safety, which means they want to ensure that AGI is designed to be friendly and helpful rather than destructive. Others are concerned about a competition between countries in which harmful machines are produced too soon.\n\nThere are also disagreements on the rules and criteria that should be followed when developing AGI to ensure that it is fair and treats everyone equally.\n\nAs OpenAI CEO Sam Altman puts it, “making AGI safe, and driving the broad adoption of such research across the AI community, is our mission.” Another source of concern is the drive between countries to produce harmful technology as rapidly as possible. Furthermore, there are continuous discussions regarding AGI ethics, with experts like Dr. Kate Crawford asking us to address the “social and political implications of AI.”\n\n**Making AGI a good thing for everyone**:\n\nThere are a few suggestions for making AGI useful and safe.\n\nOne approach is to develop laws and agreements among countries so that everyone collaborates rather than competes.\n\nAnother is to educate people about artificial intelligence and help them comprehend what it can achieve and how it can impact our world.\n\nWe may also encourage scientists from other countries to collaborate, exchanging ideas and findings in order to develop AGI that is safe and useful to all. Several suggestions can assist guarantee that AGI benefits everyone.\n\nDr. Wendell Wallach, an AI ethics expert, advises that worldwide agreements for AGI development be established to eliminate cross-national competition.\n\n**Projects that are shaping AGI’s future**:\n\nSeveral groups are working on projects to help ensure AGI is a good thing for humanity. The Partnership on AI is a group of companies, researchers, and organizations that are working together to make sure AI is used for good.\n\n**_The AI Alignment Forum_** is a place where scientists and researchers can talk about making AGI safe and friendly.\n\nThere’s also the **_AI for Good Global Summit_**, a big meeting where people share ideas on how to use AI to make the world a better place. There are various groups working on projects to ensure AGI serves humanity’s best interests.\n\nThe Partnership on AI, which includes members like Google, Facebook, and Microsoft, aims to “develop and share best practices for AI technologies.” The AI Alignment Forum provides a platform for researchers to discuss AGI safety and alignment with human values. Additionally, the AI for Good Global Summit, organized by the United Nations, brings experts together to explore how AI can contribute to global progress.\n\n**Conclusion**:\n\nThe future of AGI is an enthralling mix of wonder and uncertainty. By learning about it, engaging with expert opinions, and working together, we can help shape AGI into a powerful and positive force in our world. As we venture into this uncharted territory, we have the opportunity to create a future where AGI serves as a trusted companion, elevating humanity to new heights of achievement and well-being.\n\n## **Resources for Further Learning**:\n\nIf you’d like to dive deeper into the world of AGI, here are some resources to get you started:\n\n1\\. Books:\n\n\\- “ [Superintelligence” by Nick Bostrom](https://www.amazon.in/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834/ref=sr_1_3?keywords=super+intelligence+-+nick+bostrom&qid=1681981402&sprefix=Super+intell%2Caps%2C241&sr=8-3)\n\n\\- “ [Life 3.0: Being Human in the Age of Artificial Intelligence](https://www.amazon.in/Life-3-0-Being-Artificial-Intelligence/dp/0141981806/ref=sr_1_1?crid=3SJZK3N7KJOOK&keywords=Life+3.0%3A+Being+Human+in+the+Age+of+Artificial+Intelligence&qid=1681981435&sprefix=life+3.0+being+human+in+the+age+of+artificial+intelligence%2Caps%2C244&sr=8-1)” by Max Tegmark\n\n\\- “ [Human Compatible: Artificial Intelligence and the Problem of Control](https://www.amazon.in/Human-Compatible-Artificial-Intelligence-Creating-ebook/dp/B0BXLLK9FW/ref=sr_1_3?crid=SIXBJMCI0TIQ&keywords=Human+Compatible%3A+Artificial+Intelligence+and+the+Problem+of+Control&qid=1681981460&sprefix=human+compatible+artificial+intelligence+and+the+problem+of+control%2Caps%2C261&sr=8-3)” by Stuart Russell\n\n2\\. Websites:\n\n\\- OpenAI (https://www.openai.com): A leading AI research organization focusing on AGI and AI safety\n\n\\- AI Alignment Forum (https://www.alignmentforum.org): An online platform for researchers to share and discuss work on AI alignment and safety\n\n\\- Future of Life Institute (https://futureoflife.org): A research institute focused on AGI safety and existential risk reduction\n\n3\\. Online Courses:\n\n\\- “Introduction to Artificial Intelligence” by Stanford University (available on Coursera)\n\n\\- “AI for Everyone” by Andrew Ng (available on Coursera)\n\n\\- “Deep Learning” by Google (available on Udacity)\n\nBy staying informed and engaging in discussions about AGI, you become part of the global effort to harness its incredible potential while minimizing its risks. Together, we can shape a future where AGI serves as a force for good, paving the way for a brighter, more sustainable world for all.\n\n[Artificial Intelligence](https://medium.com/tag/artificial-intelligence?source=post_page-----9887028bd9b1---------------------------------------)\n\n[AI](https://medium.com/tag/ai?source=post_page-----9887028bd9b1---------------------------------------)\n\n[Strong Ai](https://medium.com/tag/strong-ai?source=post_page-----9887028bd9b1---------------------------------------)\n\n[AGI](https://medium.com/tag/agi?source=post_page-----9887028bd9b1---------------------------------------)\n\n[Technology](https://medium.com/tag/technology?source=post_page-----9887028bd9b1---------------------------------------)\n\n[![Adithya Thatipalli](https://miro.medium.com/v2/resize:fill:96:96/1*UZagQH2Dd3JQAlI8MuJhfQ.jpeg)](https://adithyathatipalli.medium.com/?source=post_page---post_author_info--9887028bd9b1---------------------------------------)\n\n[![Adithya Thatipalli](https://miro.medium.com/v2/resize:fill:128:128/1*UZagQH2Dd3JQAlI8MuJhfQ.jpeg)](https://adithyathatipalli.medium.com/?source=post_page---post_author_info--9887028bd9b1---------------------------------------)\n\nFollow\n\n[**Written by Adithya Thatipalli**](https://adithyathatipalli.medium.com/?source=post_page---post_author_info--9887028bd9b1---------------------------------------)\n\n[122 Followers](https://adithyathatipalli.medium.com/followers?source=post_page---post_author_info--9887028bd9b1---------------------------------------)\n\n· [90 Following](https://adithyathatipalli.medium.com/following?source=post_page---post_author_info--9887028bd9b1---------------------------------------)\n\nSecurity Engineer by Day, Cloud and Blockchain Learner during Night\n\nFollow\n\n## Responses (1)\n\nSee all responses\n\n[Help](https://help.medium.com/hc/en-us?source=post_page-----9887028bd9b1---------------------------------------)\n\n[Status](https://medium.statuspage.io/?source=post_page-----9887028bd9b1---------------------------------------)\n\n[About](https://medium.com/about?autoplay=1&source=post_page-----9887028bd9b1---------------------------------------)\n\n[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----9887028bd9b1---------------------------------------)\n\n[Press](mailto:pressinquiries@medium.com)\n\n[Blog](https://blog.medium.com/?source=post_page-----9887028bd9b1---------------------------------------)\n\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----9887028bd9b1---------------------------------------)\n\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----9887028bd9b1---------------------------------------)\n\n[Text to speech](https://speechify.com/medium?source=post_page-----9887028bd9b1---------------------------------------)\n\n[Teams](https://medium.com/business?source=post_page-----9887028bd9b1---------------------------------------)",
        "image": "https://miro.medium.com/v2/resize:fit:1024/1*hy-GCqxcb9B_A6jnSsn-Pg.jpeg",
        "favicon": "https://miro.medium.com/v2/5d8de952517e8160e40ef9841c781cdc14a5db313057fa3c3de41c6f5b494b19"
      },
      {
        "id": "https://medium.com/latinxinai/does-ai-think-no-it-doesnt-here-s-the-real-truth-84900801975f",
        "title": "Does AI Think? No, It Doesn’t — Here’s the Real Truth”",
        "url": "https://medium.com/latinxinai/does-ai-think-no-it-doesnt-here-s-the-real-truth-84900801975f",
        "publishedDate": "2024-10-08T22:32:44.000Z",
        "author": "Alberto Robles",
        "score": 0.34606897830963135,
        "text": "Artificial Intelligence is everywhere, and it’s easy to think that the machines we interact with daily are, well, thinking . They answer our questions, finish our sentences, and even create art. But AI doesn’t think. It doesn’t have thoughts, ideas, or even understand what it’s saying. All it does is transform information from one form to another, in a way that seems intelligent. AI: Smart or Just Fast? When you type a question into a chatbot or get a personalized movie recommendation, it feels like the AI understands your needs, right? It’s almost like it has a tiny brain working out what you want. But in reality, it’s not “thinking” at all. It’s just very good at recognizing patterns in data and giving the most likely response based on what it has seen before. Example : Imagine a giant, super-fast version of your phone’s autocorrect. When you type “Artificial,” it knows you might want to follow up with “intelligence,” “flavors,” or “flowers,” just based on what it’s seen other people type. AI does this on a massive scale, looking at everything it’s been trained on to predict the most relevant response. How AI Really Works So, if AI isn’t thinking, what is it doing? It’s transforming your input into a response, whether that’s a question, statement, or request. Here’s the basic process: Step 1: Breaking Down the Input (Tokenization) When you send a message like “What’s the capital of France?”, AI splits it into smaller pieces called tokens. Tokens are the building blocks of language: words, parts of words, punctuation, etc. Example: \"What’s the capital of France?\" becomes tokens like [\"What\", \"’s\", \"the\", \"capital\", \"of\", \"France\", \"?\"] . Step 2: Understanding the Tokens (Mapping) Each token is then converted into a number that the AI model can process. This is called a token ID . Think of it as a digital fingerprint for each token. These tokens get fed into a neural network — essentially, a giant calculator with multiple layers that analyze the text.",
        "image": "https://miro.medium.com/v2/resize:fit:1024/1*2bRQYj03H7kHCn-6_9Quwg.png",
        "favicon": "https://miro.medium.com/v2/5d8de952517e8160e40ef9841c781cdc14a5db313057fa3c3de41c6f5b494b19"
      }
    ],
    "costDollars": {
      "total": 0.014,
      "search": {
        "neural": 0.005
      },
      "contents": {
        "text": 0.009
      }
    }
  }
}
`````

## Crawling

Request

```javascript
import Exa from "exa-js"

const exa = new Exa("3e964454-75a7-415d-bb35-53a13f472e12");

const result = await exa.getContents(
  ["tesla.com"],
  {
    text: true
  }
)
```

Result

```javascript
{
  "data": {
    "results": [
      {
        "id": "tesla.com",
        "title": "Electric Cars, Solar & Clean Energy | Tesla",
        "url": "http://tesla.com",
        "author": "",
        "text": "Cybertruck Foundation LogoCybertruck Foundation LogoCybertruck Logo\n\nInventory Model Y Available Now\n\nExplore Reduced Pricing\n\nInventory Model Y Available Now\n\nExplore Reduced Pricing\n\n[View Inventory](https://www.tesla.com/inventory/new/my?redirect=no)\n\nFor the best experience, we recommend upgrading or changing your web browser.\n[Learn More](https://www.tesla.com//support/browser-support)\n\n![Ultra Red New Model Y driving next to a Quicksilver Model Y on a highway surrounded by windmills and mountains](https://digitalassets.tesla.com/tesla-contents/image/upload/f_auto)\n\n# Model Y\n\n### $7,500 Federal Tax Credit at Purchase\n\n### $7,500 Federal Tax Credit at Purchase\n\n[Order New Model Y](https://www.tesla.com/modely/design?cfg=#overview) [View Inventory](https://www.tesla.com/inventory/new/my)\n\n[Order New Model Y](https://www.tesla.com/modely/design?cfg=#overview) [View Inventory](https://www.tesla.com/inventory/new/my)\n\n![Cybertruck](https://digitalassets.tesla.com/tesla-contents/image/upload/f_auto)\n\n### [1.99% APR Ending March 31](https://www.tesla.com/cybertruck/design?financeModalTab=finance_options&financeProduct=finplat.AUTO_LOAN%3ALOAN%3ACT_PRIVATE\\#overview)\n\n### [1.99% APR Ending March 31](https://www.tesla.com/cybertruck/design?financeModalTab=finance_options&financeProduct=finplat.AUTO_LOAN%3ALOAN%3ACT_PRIVATE\\#overview)\n\n[Order Now](https://www.tesla.com/cybertruck/design) [Learn More](https://www.tesla.com/cybertruck)\n\n[Order Now](https://www.tesla.com/cybertruck/design) [Learn More](https://www.tesla.com/cybertruck)\n\nOverall NHTSA Safety Rating\n\n![Lunar Silver Model X driving down a highway surrounded by low woodlands with mountains in the background](https://digitalassets.tesla.com/tesla-contents/image/upload/f_auto)\n\n# Model X\n\n### Free Supercharging Included\n\n### Free Supercharging Included\n\n[Order Now](https://www.tesla.com/modelx/design#overview) [Learn More](https://www.tesla.com/modelx)\n\n[Order Now](https://www.tesla.com/modelx/design#overview) [Learn More](https://www.tesla.com/modelx)\n\n![Lunar Silver Model S driving down a highway surrounded by low woodlands with snowcapped mountains in the background](https://digitalassets.tesla.com/tesla-contents/image/upload/f_auto)\n\n# Model S\n\n### Free Supercharging Included\n\n### Free Supercharging Included\n\n[Order Now](https://www.tesla.com/models/design#overview) [Learn More](https://www.tesla.com/models)\n\n[Order Now](https://www.tesla.com/models/design#overview) [Learn More](https://www.tesla.com/models)\n\n![Ranch style home powered by Tesla solar panels and Powerwall](https://digitalassets.tesla.com/tesla-contents/image/upload/f_auto)\n\n# Solar Panels\n\n### [Schedule a Virtual Consultation](https://www.tesla.com/solar-virtual-consultations?poi=solarpanels)\n\n### [Schedule a Virtual Consultation](https://www.tesla.com/solar-virtual-consultations?poi=solarpanels)\n\n[Order Now](https://www.tesla.com/energy/design?poi=solarpanels) [Learn More](https://www.tesla.com/solarpanels)\n\n[Order Now](https://www.tesla.com/energy/design?poi=solarpanels) [Learn More](https://www.tesla.com/solarpanels)\n\n![Home outfitted with Tesla Solar Roof](https://digitalassets.tesla.com/tesla-contents/image/upload/f_auto)\n\n# Solar Roof\n\n### Produce Clean Energy From Your Roof\n\n### Produce Clean Energy From Your Roof\n\n[Order Now](https://www.tesla.com/solarroof/design) [Learn More](https://www.tesla.com/solarroof)\n\n[Order Now](https://www.tesla.com/solarroof/design) [Learn More](https://www.tesla.com/solarroof)\n\n![Home outfitted with Tesla Solar Roof](https://digitalassets.tesla.com/tesla-contents/image/upload/f_auto)\n\n# Powerwall\n\n[Order Now](https://www.tesla.com/powerwall/design) [Learn More](https://www.tesla.com/powerwall)\n\n[Order Now](https://www.tesla.com/powerwall/design) [Learn More](https://www.tesla.com/powerwall)\n\n![Gen 3 Wall Connector with tempered white glass faceplate mounted on a pedestal in front of a fence](https://digitalassets.tesla.com/tesla-contents/image/upload/f_auto)\n\n# Accessories\n\n[Shop Now](https://shop.tesla.com)\n\n[Shop Now](https://shop.tesla.com)",
        "image": "https://digitalassets.tesla.com/tesla-contents/image/upload/f_auto,q_auto/Homepage-Social-US.jpg",
        "favicon": "https://www.tesla.com/themes/custom/tesla_frontend/assets/favicons/favicon-32x32.png"
      }
    ],
    "requestId": "efec23f81f64a7c5824826e507c0a735",
    "costDollars": {
      "total": 0.001,
      "contents": {
        "text": 0.001
      }
    }
  }
}
```

## Find Similar

Request

```javascript
import Exa from "exa-js"

const exa = new Exa("3e964454-75a7-415d-bb35-53a13f472e12");

const result = await exa.findSimilar(
  "brex.com",
  {
    excludeDomains: ["brex.com"],
    numResults: 10
  }
)
```

Result

```javascript
{
  "data": {
    "results": [
      {
        "id": "https://twitter.com/brexHQ",
        "title": "",
        "url": "https://twitter.com/brexHQ",
        "publishedDate": "2018-06-19T00:00:00.000Z",
        "author": "brexHQ",
        "score": 0.8605578541755676
      },
      {
        "id": "https://www.rho.co/",
        "title": "Rho | Better Business Banking, From Idea to IPO.",
        "url": "https://www.rho.co/",
        "publishedDate": "2024-01-30T00:00:00.000Z",
        "author": null,
        "score": 0.8574956655502319
      },
      {
        "id": "https://getdivvy.com/",
        "title": "Divvy | Expense Management & Business Budgeting Software",
        "url": "https://getdivvy.com/",
        "publishedDate": "2023-05-18T00:00:00.000Z",
        "author": null,
        "score": 0.856987476348877
      },
      {
        "id": "https://www.arc.tech/",
        "title": "Empowering startups with a better cash management experience | Arc",
        "url": "https://www.arc.tech/",
        "publishedDate": "2023-05-18T00:00:00.000Z",
        "author": "Arc Technologies",
        "score": 0.8527939915657043
      },
      {
        "id": "https://www.bill.com/product/spend-and-expense",
        "title": "BILL Spend & Expense (Formerly Divvy)",
        "url": "https://www.bill.com/product/spend-and-expense",
        "publishedDate": "2024-01-01T00:00:00.000Z",
        "author": null,
        "score": 0.8526758551597595
      },
      {
        "id": "https://floatcard.com/",
        "title": "Canada's Smartest Spend Management Software | Float",
        "url": "https://floatcard.com/",
        "publishedDate": "2024-06-11T00:00:00.000Z",
        "author": null,
        "score": 0.8516704440116882
      },
      {
        "id": "https://www.lithic.com/",
        "title": "Flexible Card Issuing Solutions for Developers | Lithic",
        "url": "https://www.lithic.com/",
        "publishedDate": "2014-01-01T00:00:00.000Z",
        "author": null,
        "score": 0.8493916988372803
      },
      {
        "id": "https://www.joinarc.com/",
        "title": "Arc",
        "url": "https://www.joinarc.com/",
        "publishedDate": "2022-08-15T00:00:00.000Z",
        "author": "Arc Technologies",
        "score": 0.848883867263794
      },
      {
        "id": "https://floatfinancial.com/",
        "title": "Modern Business Expense Management & Corporate Cards for Canada | Float",
        "url": "https://floatfinancial.com/",
        "publishedDate": "2025-02-05T00:00:00.000Z",
        "author": null,
        "score": 0.8485442996025085
      },
      {
        "id": "https://web.archive.org/web/20230224121540/https://www.brex.com/solutions/startups",
        "title": "Financial stack for startups",
        "url": "https://web.archive.org/web/20230224121540/https://www.brex.com/solutions/startups",
        "publishedDate": "2023-04-13T00:00:00.000Z",
        "author": null,
        "score": 0.8484395146369934
      }
    ],
    "requestId": "7fc9a06e8f26626f76e4b70cf4faded9",
    "costDollars": {
      "total": 0.005,
      "search": {
        "neural": 0.005
      }
    }
  }
}
```

## Answer

Request

```javascript
import OpenAI from "openai"

const client = new OpenAI({
  baseURL: "https://api.exa.ai",
  apiKey: "3e964454-75a7-415d-bb35-53a13f472e12",
});

const completion = await client.chat.completions.create({
  model: "exa",
  messages: [
    {
      "role": "user",
      "content": "What makes some LLMs so much better than others?"
    }
  ],
  // use extra_body to pass parameters to the /answer endpoint
  extra_body: {
    text: true
  }
});
```

Result

```javascript
{
  "answer": "The quality of Large Language Models (LLMs) can vary due to several factors. Some believe that fine-tuning, which involves training on smaller datasets to give the LLM a specific personality, is more important than the inherent intelligence of the model ([andrewzuo.com](https://andrewzuo.com/why-every-mainstream-llm-is-the-same-quality-wise-6e84143927f6)). Others suggest that the use of high-quality training data, engineering tricks on input, and human feedback contribute to an LLM's superior performance ([news.ycombinator.com](https://news.ycombinator.com/item?id=38416538)). Additionally, the size of the model and the training techniques employed can also play a significant role ([news.ycombinator.com](https://news.ycombinator.com/item?id=38416538)).\n",
  "citations": [
    {
      "id": "https://andrewzuo.com/why-every-mainstream-llm-is-the-same-quality-wise-6e84143927f6",
      "title": "Why Every Frontier LLM Is The Same Quality Wise - Andrew Zuo - Medium",
      "url": "https://andrewzuo.com/why-every-mainstream-llm-is-the-same-quality-wise-6e84143927f6",
      "author": "Andrew Zuo",
      "publishedDate": "2025-01-30T13:00:20",
      "image": "https://miro.medium.com/v2/resize:fit:1200/1*bRWdrNxkLVaGe8FzGez6_g.jpeg",
      "favicon": "https://miro.medium.com/v2/5d8de952517e8160e40ef9841c781cdc14a5db313057fa3c3de41c6f5b494b19",
      "text": "Every LLM is basically the same in terms of quality. And I think a lot of people are lying to themselves that they see better results from model X or Y. Recently I published a post titled Gemini 2.0 Is Insane where I said: First some context. I have to be honest with you, since the arrival of ChatGPT using GPT-3.5 Turbo I have not seen much of an improvement between AI models. GPT-3.5 Turbo feels the same as GPT-4 which feels the same as GPT-4o Mini which feels the same as GPT-4o which feels the same as Gemini and Llama and Claude. People keep on saying how Claude is so much better than GPT-4o. I think it’s just the placebo effect. I have never noticed any improvement in Claude’s answers over GPT-4o. They have the exact same content, sometimes even using the same headings. In fact I much prefer GPT-4o’s output because Claude has a tendency to put its response in point form and then I have to be like, “Can you expand on those points?” This is something I’ve been saying a lot: the intelligence of an LLM doesn’t really matter. What matters more is how well the responses are tuned. And Claude’s responses are pretty terrible. I haven’t really noticed any improvement in LLMs since GPT-3.5 Turbo. Heck, even GPT-3 was pretty good. And I got so much pushback from this section. It’s actually a little surprising, I’m pretty sure this is not the first time I’ve said this. But it’s the first time, for whatever reason, people are pushing back. So in this post I still think I should address some concerns I’ve been seeing. So first of all, I think I wasn’t clear enough in the above snippet, I made it sound like all these LLMs are exactly the same. They are not, but that’s not because one LLM is any smarter than the other. It’s because of fine tuning. So training an LLM has two main steps. The first and most important is back propagation, which teaches an LLM to predict the next word. That’s all an LLM is really, word prediction machines. In fact the first versions of GPT were literally autocomplete, they did not have a chat interface. After that there is fine tuning, this is like training but is done on a smaller data set. In this stage guardrails are created so the LLM does not provide dangerous data. It is also where the LLM gains a personality instead of just being an average of everything on the internet. There are many ways you can do fine tuning. Use adversarial systems or specially labelled ‘known good’ data like Wikipedia. But the most…"
    },
    {
      "id": "https://news.ycombinator.com/item?id=38416538",
      "title": "Ask HN: Why is GPT4 better than the other major LLMs?",
      "url": "https://news.ycombinator.com/item?id=38416538",
      "author": "",
      "publishedDate": "2023-11-25T00:00:00",
      "favicon": "https://news.ycombinator.com/y18.svg",
      "text": "Having used GPT4, PaLM and Claude, it is quite clear to me that GPT4 is an order of magnitude better than these other LLMs. Ask HN: Why is GPT4 better than the other major LLMs? \n 11 points by takinola on Nov 25, 2023 | hide | past | favorite | 15 comments \n \n Having used GPT4, PaLM and Claude, it is quite clear to me that GPT4 is an order of magnitude better than these other LLMs. Google, Anthropic and everyone else are investing oodles of resources and the best talent to catch up so why are they (seemingly) not able to? There is a general sentiment that LLMs have no lasting moat but OpenAI seems to (for now) have one in terms of a better product. The big question is why? \n \n There is a theory that GPT-4's secret sauce is a combination of commissioned high quality training data (which they don't really hide) and an unknown implementation of Mixture-of-Experts [1]. No proof, of course, as the other comments have said, they won't share. The Open AI name is still ironic. I've also still not seen another successful attempt of MoE by any other company, which you would expect to if it was true. [1] https://hkaift.com/the-next-llms-development-mixture-of-expe... \n \n \n \n I believe it's because it is heavily massaged. On two fronts: 1) By engineers, who employ tricks on the input, and perhaps the output. The input especially. So, when you type into ChatGPT, that input gets parsed using non-LLM techniques and/or heuristics, etc. This is primarily to get the semantics right. Non-LLM techniques can actually be pretty powerful, but the synergy of non-LLM and LLM is incredible. 2) By human farms, who essentially \"upvote\" and/or add corrections to ChatGPT results, and feed this back into the system. See OpenAI Kenyan workers (I believe the Kenyan workers, on paper, were for \"moderation\", but nothing stops them from also upvoting/correcting). \n \n \n I think one clear difference is that they are just so focused on shipping compared to the others, and they gain many of the benefits that comes with that. I think it is reflected in both Greg and Sam that they really want to ship, and this have made a positive feedback loop into the team and what talent they have been able to acquire but also build up. Another point might be that AI chatbots are a first-movers market. Even if Grok turned out to be much better I would still miss some of the UI features that ChatGPT provide along with my chat-history. In regards to their fast shipping I think it is also reflected in their tech-stack. I suspect from reading their job posts (I might be very wrong here)\nthat they started just coding everything in Python and the tooling/ecosystem that goes along like FastAPI/Django etc. maybe a bit C++/CUDA for the training.\nThen when they needed to scale they migrated from Python to Rust in the more critical areas of the codebase. They clearly also have a monorepo mentioned from [1]. if you look through the their career-page the job description of a software engineer for developer-productivity [1] mentions \"Our current environment relies heavily on Python, Rust, and C++\" also \"Are a proficient Python programmer, with experience using Rust in production\" I found an earlier one where they mentioned that their backend was written in Python.\n\"Proficiency with some backend language (we use Python)\" [2]: 1: https://openai.com/careers/software-engineer-developer-produ... 2: https://openai.com/careers/software-engineer-leverage-engine... \n \n \n I don't think it's 10 times better. It is better. But I think it comes down to the size of the model and the training/training techniques. OpenAI seems to have invested a lot in human reinforcement feedback. Plus they have ways to do automated reinforcement I think. Also Google and Anthropic are basically deliberately holding back their strongest models because they are too expensive and/or they are worried about safety or something. And note that there are new versions of models from Anthropic that have just released or could release within a few months. \n \n \n I have a test harness that runs all the LLMs side by side to compare the quality of output. I have tried Claude 2.1 (the new Anthropic model) and it is better than PaLM but not as good as GPT4. I am skeptical that Google is holding back a better model. It would go against every instinct that all the tech companies have, especially if you believe that AI is the future, why would you hamstring yourself by putting out an inferior product when you have a better one gathering dust in the back? Google has oodles of cash, so not sure why cost would be an issue. I don't get the safety argument so I can't speak to it but unless you have AGI it seems premature. \n \n \n \n It's widely reported that Google have models that are not publicly released and that part of the reasoning at some point has been cost effectiveness. \n \n \n \n &gt; Also Google and Anthropic are basically deliberately holding back their strongest models because they are too expensive and/or they are worried about safety or something. Any place I can read about this? \n \n \n If we knew, the other LLM's would be better. The correct answer is: Nobody outside of OpenAI technical staff currently knows. \n \n \n \n I don’t think this line of reasoning holds. We know why Google search is (was?) better. It just so happens there is a self-reinforcing feedback loop that is involved. We know why Apple is better at making phones. It doesn’t follow that their success can be easily reproduced. It seems likely to be that we should know why OpenAI is better even if it is not an easily copied advantage \n \n \n \n I don't think this is the only factor but I suspect part of it is because GPT-4 had access to better datasets. After ChatGPT came out a lot of the places GPT was assumed to be trained on (reddit, twitter) started closing their APIs. This alone represents a pretty significant moat. \n \n \n \n Reddit was still pretty easy to get last summer. I saved a copy. Twitter has never really been easy. Some data sets are getting locked down, but you can still get them if you have enough motivation. I think the technical skill first and compute access are bigger hurdles. \n \n \n \n Define \"order of magnitude better.\" Faster? Safer? Bigger buffer? More accurate? More comprehensible? Better-crafted text? From what I've seen on Claude discussion forums, Claude users generally assert that GPT4 requires a lot more manual handling due to a smaller buffer, and produces weird, long-winded answers that are now the stuff of memes. Whatever beef they have with Claude's safety features, the input/output peculiarities of Claude make it well suited for the kinds of writing they want it to do. You should consider whether the LLMs are really comparable in the way you want them to be comparable. \n \n \n This an anecdotal. I tried LM Studio the other day, which is a cool project, by the way. Downloaded a dozen different LLM based on the recommendations and reviews and size and loss of accuracy, etc. None of them were able to help with a project I’m currently working on. They were all so grossly wrong or plain weird responses. I have the source data for the project I’m working on, so I know what’s correct and what isn’t. GPT-4 is accurate and very helpful with the project. It’s light years ahead on everything I’ve used it for. \n \n \n I mean, one thing people miss/forget is that we still don't have any LLM with the estimated compute budget of 4 that has failed to match it. Palm, Claude etc are all lower. If Gemini fails to at least match 4 then we can start speculating on a secret sauce."
    }
  ],
  "costDollars": {
    "total": 0.005
  }
}
```
